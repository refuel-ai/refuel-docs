{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Refuel AI","text":"<ul> <li> <p> Cloud SDK</p> <p>Python library for interacting with Refuel's cloud APIs.</p> <p> Docs</p> </li> <li> <p> Autolabel</p> <p>Label, clean and enrich text datasets with LLMs.</p> <p> Docs</p> <p> Github</p> </li> </ul>"},{"location":"python-sdk/","title":"Refuel SDK Guide","text":"<p>Refuel SDK is a Python library to programmatically call Refuel's APIs. This guide describes how to install and use the Refuel SDK.</p> <p>The SDK is currently an beta release and under rapid development. Please expect some sharp edges. Feedback and bug reports are always welcome! Shoot us an email at <code>support@refuel.ai</code> or ping us on our shared Slack.</p>"},{"location":"python-sdk/#getting-started","title":"Getting Started","text":""},{"location":"python-sdk/#installation","title":"Installation","text":"<p>Install the SDK using a package installer such as pip:</p> <pre><code>pip install refuel\n</code></pre> <p>Installing and using the SDK requires Python 3.6+.</p>"},{"location":"python-sdk/#initializing-a-client-session","title":"Initializing a client session","text":"<p>Set the environment variable REFUEL_API_KEY. The SDK will read it during initialization, and use this value when sending requests:</p> <pre><code>import refuel\n# Assuming you've set `REFUEL_API_KEY` in your env,\n# init() will pick it up automatically\nrefuel_client = refuel.init()\n</code></pre> <p>Alternatively, you can supply the API key as a parameter during initializtion as shared below. In the cloud application, there is a top-level dropdown to select the project you\u2019re working on currently. And this selection powers all the pages downstream (datasets, labeling tasks etc). The SDK allows you to do this by setting the project during initialization as well:</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\n</code></pre> <p>Here\u2019s the complete list of initialization options currently supported:</p> Option Is Required Default Value Comments <code>api_key</code> Yes None Used to authenticate all requests to the API server <code>project</code> Yes None The name of the project you plan to use for the current session <code>timeout</code> No 60 Timeout in seconds <code>max_retries</code> No 3 Max number of retries for failed requests <code>max_workers</code> No Num CPUs (os.cpu_count()) Max number of concurrent requests to the API server"},{"location":"python-sdk/#projects","title":"Projects","text":"<p>These functions let you create a new project in your team\u2019s Refuel account, or get information about projects already defined.</p>"},{"location":"python-sdk/#get-projects","title":"Get Projects","text":"<p>The get_projects API will return a list of all projects that belong to your team:</p> <pre><code>projects = refuel_client.get_projects()\n</code></pre> <p>You can also retrieve information about a specific project, either by name or by UUID</p> <pre><code>project = refuel_client.get_project(project='&lt;PROJECT NAME&gt;')\n</code></pre>"},{"location":"python-sdk/#create-project","title":"Create Project","text":"<pre><code>refuel_client.create_project(\nproject='&lt;NEW PROJECT NAME&gt;',\ndescription='&lt;NEW PROJECT DESCRIPTION&gt;',\n)\n</code></pre>"},{"location":"python-sdk/#datasets","title":"Datasets","text":"<p>These functions let you upload/download a full dataset, or fetch rows and LLM labels within a dataset.</p>"},{"location":"python-sdk/#get-all-current-datasets","title":"Get all current datasets","text":"<p>The get_datasets function will return a list of all datasets in the project:</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\ndatasets = refuel_client.get_datasets()\n</code></pre>"},{"location":"python-sdk/#upload-dataset","title":"Upload Dataset","text":"<p>This function lets you upload a local CSV file as a new dataset to Refuel.</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\ndataset = refuel_client.upload_dataset(\nfile_path='&lt;PATH TO CSV FILE&gt;',\ndataset_name='&lt;NEW DATASET NAME&gt;',\nsource='local|uri',\nwait_for_completion='True|False'\n)\n</code></pre> <p>Some details about the function parameters:</p> Option Is Required Default Value Comments <code>file_path</code> Yes - Path to the data you wish to upload <code>dataset_name</code> Yes - Unique name of the dataset being uploaded <code>source</code> Yes local Place where the file resides <code>wait_for_completion</code> No False Whether to poll for the completion of the dataset ingestion <p>Note: When uploading dataset from <code>uri</code> source, the <code>file_path</code> should be publicly accessible (eg. S3 presigned url) for Refuel to process it.</p>"},{"location":"python-sdk/#download-dataset","title":"Download Dataset","text":"<p>This function create a snapshot of your dataset and generate presigned URL for secure download.</p> <pre><code>response = refuel_client.download_dataset(\ndataset='&lt;DATASET NAME&gt;',\nemail_address='&lt;YOUR EMAIL ADDRESS&gt;',\n)\n</code></pre> <p>This is an asynchronous workflow, and when the dataset is available for download, Refuel will send an email to the email address. Make sure the email address belongs to a valid user from your team. Depending on the size of the dataset, this export step can take a few minutes. Once the download link is created and emailed, it will be valid for 24 hours.</p>"},{"location":"python-sdk/#adding-items-rows-to-an-uploaded-dataset","title":"Adding items (rows) to an uploaded dataset","text":"<p>This function lets you append new rows to an existing dataset. Keep in mind that the dataset schema is decided during initial upload and is not updated here.</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\nnew_items_to_add = [\n{\"column1\": \"value1\", \"column2\": \"value2\", ...},\n{\"column1\": \"value3\", \"column2\": \"value4\", ...},\n...\n]\nrefuel_client.add_items(dataset='&lt;DATASET NAME&gt;', items=new_items_to_add)\n</code></pre>"},{"location":"python-sdk/#querying-items-rows-in-a-dataset","title":"Querying items (rows) in a dataset","text":"<p>In addition to downloading the entire dataset, you can also fetch a list of items (rows) from the dataset as follows:</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\nitems = refuel_client.get_items(\ndataset='&lt;DATASET NAME&gt;',\nmax_items=20,\noffset=0\n)\n</code></pre> <p>This function will return a pandas dataframe. Some details about the function parameters:</p> Option Is Required Default Value Comments <code>dataset</code> Yes - Name of the dataset you want to query and retrieve items (rows) from <code>max_items</code> No 20 Max number of rows you want to fetch <code>offset</code> No 0 If this is set to a positive number, say N, then the first N rows will be skipped and the API will return \u201cmax_items\u201d number of rows after skipping the first N rows."},{"location":"python-sdk/#querying-items-along-with-labels-from-a-labeling-task","title":"Querying items, along with labels from a labeling task","text":"<p>get_items() also allows you to provide an optional parameter - a labeling task. When provided, the function will also include the task results (LLM labels, confidence and manually confirmed labels, if any) for the returned items.</p> <pre><code>items = refuel_client.get_items(\ndataset='&lt;DATASET NAME&gt;',\ntask='&lt;LABELING TASK NAME&gt;',\nmax_items=100\n)\n</code></pre>"},{"location":"python-sdk/#applying-sort-ordering-when-querying-items","title":"Applying sort ordering when querying items","text":"<p>By default, the API will use Refuel\u2019s sort order (by decreasing order of diversity). You can use the <code>order_by</code> param to sort by any other columns in the dataset or by the label or confidence score from a labeling task.</p> <ol> <li>Sort by dataset column</li> </ol> <pre><code>items = refuel_client.get_items(\ndataset='&lt;DATASET NAME&gt;',\nmax_items=100,\norder_by=[{'field': '&lt;COLUMN NAME TO SORT BY&gt;', 'direction': '&lt;ASC or DESC&gt;'}],\n)\n</code></pre> <ol> <li>Sort by label or confidence score from a labeling task. Note that this requires a task name and a subtask name to be specified. <code>field</code> can be either 'label' or 'confidence'.</li> </ol> <pre><code>items = refuel_client.get_items(\ndataset='&lt;DATASET NAME&gt;',\ntask='&lt;LABELING TASK NAME&gt;',\nmax_items=100,\norder_by=[{'field': 'confidence', 'direction': '&lt;ASC or DESC&gt;', 'subtask': '&lt;SUBTASK NAME&gt;'}],\n)\n</code></pre> <p>You may have multiple dicts in the <code>order_by</code> list if you would like to sort by multiple columns (used in the case of ties). Some details about the keys for each dict in the <code>order_by</code> list:</p> Key Is Required Default Value Description Comments <code>field</code> Yes The name of the column in the dataset to sort by In addition to the columns in the dataset, the field can also be 'label' or 'confidence', if the task and subtask names are specified. <code>direction</code> No <code>ASC</code> The direction that you would like to sort the specified column by Should be <code>ASC</code> or <code>DESC</code> <code>subtask</code> No null The name of the subtask for which you would like to sort by label or confidence This should only be provided if the field is 'label' or 'confidence' and requires a task name to be specified in the function params."},{"location":"python-sdk/#applying-filters-when-querying-items","title":"Applying filters when querying items","text":"<p>In addition to sorting options, you can also define filters to only fetch items in the dataset that match a specific criteria. The SDK supports three types of filters:</p> <ol> <li>Metadata Filters:</li> </ol> <p>Filter based on the value of a specific column, for e.g. a filter such as \u201ccolumn = value\u201d is defined as a Python dictionary with three keys:</p> <ul> <li>field: This is the column on which you want to apply the filter</li> <li>operator: This is the comparison operator</li> <li>value: This is the value to compare to</li> </ul> <p>Here\u2019s an example:</p> <pre><code>items_filter = {\n'field': 'transaction_category',\n'operator': '=',\n'value': 'Food'\n}\nitems = refuel_client.get_dataset_items(\ndataset='&lt;DATASET NAME&gt;',\nmax_items=100,\nfilters = [items_filter]\n)\n</code></pre> <ol> <li>LLM output value/confidence filter:</li> </ol> <p>Filter items based on the LLM output value or confidence score from a specific task configured in Refuel.</p> <p>Here's a concrete example: Let's say you configured a classification task called <code>Sentiment Analysis</code> in your Refuel account, which has two subtasks (output fields):</p> <p>(i) <code>predicted_sentiment</code> - the predicted sentiment</p> <p>(ii) <code>explanation</code> - a one sentence explanation of why the LLM output the predicted sentiment as Positive or Negative for the item.</p> <p>Here are a few filters we can define for this task:</p> <ul> <li>\"predicted sentiment is Positive\":</li> </ul> <pre><code>{\n'field': 'llm_label',\n'subtask': 'predicted_sentiment'\n'operator': '=',\n'value': 'Positive'\n}\n</code></pre> <ul> <li>\"predicted sentiment confidence &gt;= 80%\":</li> </ul> <pre><code>{\n'field': 'confidence',\n'subtask': 'predicted_sentiment'\n'operator': '&gt;=',\n'value': '0.8'\n}\n</code></pre> <ul> <li>\"predicted sentiment does not agree with the ground truth label (available in a column called <code>ground_truth_sentiment</code>)\":</li> </ul> <pre><code>{\n'field': 'llm_label',\n'subtask': 'predicted_sentiment'\n'operator': '&lt;&gt;',\n'value': 'ground_truth_sentiment'\n}\n</code></pre> <ol> <li>Semantic search filter:</li> </ol> <p>Filter based on semantic similarity to a \u201cquery item\u201d in a dataset. Semantic search queries are mapped to approximate nearest neighbor searches in an embedding space. These filters are defined as a Python dictionary with two keys:</p> <ul> <li>operator: always set to \"SIMILAR\"</li> <li>value: This is query item, identified by the uuid in the <code>refuel_uuid</code> column.</li> </ul> <p>Here\u2019s an example of how to define a semantic search filter. The results are returned in decreasing order of similarity score:</p> <pre><code>items_filter = {\n'operator': 'SIMILAR',\n'value': '020b434f-e1d7-4b0e-bfc1-beded36c806a'\n}\nitems = refuel_client.get_dataset_items(\ndataset='&lt;DATASET NAME&gt;',\ntask='&lt;TASK NAME&gt;'\nmax_items=10,\nfilters = [items_filter]\n)\n</code></pre> <p>Here\u2019s the complete list of filter operators that are currently supported:</p> Operator Description <code>&gt;</code> Greater than <code>&gt;=</code> Greater than or equal to <code>=</code> Equals <code>&lt;&gt;</code> Not equal to <code>&lt;</code> Less than <code>&lt;=</code> Less than or equal to <code>IS NULL</code> True if field is undefined <code>IS NOT NULL</code> True if field is defined <code>LIKE</code> String matching: True if value is in field <code>ILIKE</code> String matching (case insensitive): True if value is in field <code>NOT LIKE</code> String does not match: True if value is not in field <code>NOT ILIKE</code> String does not match (case insensitive): True if value is not in field <code>SIMILAR</code> Semantic similarity filter operator"},{"location":"python-sdk/#labeling-tasks","title":"Labeling Tasks","text":"<p>These functions let you retrieve information about labeling tasks defined within a project, and start and cancel a task run.</p>"},{"location":"python-sdk/#define-a-new-task","title":"Define a new Task","text":"<p>You can create a new task programmatically within a given project using the <code>create_task</code> function:</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\nrefuel_client.create_task(\ntask='&lt;TASK NAME&gt;',\ndataset='&lt;DATASET NAME&gt;',\ncontext = 'You are an expert at analyzing sentiment of an online review about a business...',\nfields = [\n{\n'name': '...',\n'type': '...',\n'guidelines': '...',\n'labels': [...],\n'input_columns': [...],\n'fallback_value': '...'\n},\n...\n],\nmodel = 'GPT-4 Turbo'\n)\n</code></pre> <p>Some details about the various parameters you see in the function signature above:</p> Parameter Is Required Default Value Comments <code>task</code> Yes None Name of the new task you're creating <code>dataset</code> Yes None Dataset (in Refuel) for which you are defining this task <code>context</code> Yes None Context is a high level description of the problem domain and the dataset that the LLM will be working with. It typically starts with something like 'You are and expert at ...' <code>fields</code> Yes None This is a list of dictionaries. Each entry in this list defines an output field generated in the task. See below for details about the schema of each field <code>model</code> No team default LLM that will be used for this task. If not specified, we will use the default LLM set for your team, e.g. GPT-4 Turbo <p>Next, let's take a look at the schema of each entry in the <code>fields</code> list above:</p> Parameter Is Required Default Value Comments <code>name</code> Yes None Name of the output field, e.g. <code>llm_predicted_sentiment</code> <code>type</code> Yes None Type of output field. This is one of: [<code>classification</code>, <code>multilabel_classification</code>, <code>attribute_extraction</code>, <code>webpage_transform</code>, <code>web_search</code>] <code>guidelines</code> Yes None Output guidelines for the LLM for this field. Note that if the field type is a <code>web_search</code> type, the guidelines will be simply the query template <code>labels</code> Yes (for classification field types) None list of valid labels, this field is only required for classification type tasks <code>input_columns</code> Yes None Columns from the dataset to use as input when passing a \"row\" in the dataset to the LLM. <code>ground_truth_column</code> No None A column in the dataset that contains ground truth value for this field, if one exists. Note this is an optional parameter. <code>fallback_value</code> No None A fallback/default value that the LLM should generate for this field if a row cannot be processed successfully <p>Finally, here is the list of LLMs currently supported (use the model name as the parameter value):</p> Provider Name OpenAI GPT-4 Turbo OpenAI GPT-4o OpenAI GPT-4o mini OpenAI GPT-4 OpenAI GPT-3.5 Turbo Anthropic Claude 3.5 (Sonnet) Anthropic Claude 3 (Opus) Anthropic Claude 3 (Haiku) Google Gemini 1.5 (Pro) Mistral Mistral Small Mistral Mistral Large Refuel Refuel LLM-2 Refuel Refuel LLM-2-small"},{"location":"python-sdk/#get-tasks","title":"Get Tasks","text":"<p>You can retrieve a list of all tasks within a given project as follows</p> <pre><code>tasks = refuel_client.get_tasks()\n</code></pre>"},{"location":"python-sdk/#start-a-labeling-task-run","title":"Start a Labeling Task Run","text":"<p>You can begin running a labeling task on a dataset with the following:</p> <pre><code>response = refuel_client.start_task_run(\ntask='&lt;TASK NAME&gt;',\ndataset='&lt;DATASET NAME&gt;',\nnum_items=100\n)\n</code></pre> <p>This will kick off a bulk labeling run for the specified task and dataset, and label 100 items in the dataset. If <code>num_items</code> parameter is not specified, it will label the entire dataset.</p>"},{"location":"python-sdk/#cancel-an-ongoing-labeling-task-run","title":"Cancel an ongoing labeling task run","text":"<p>You can also cancel an ongoing labelling task with the same function as follows.</p> <pre><code>response = refuel_client.cancel_task_run(\ntask='&lt;TASK NAME&gt;',\ndataset='&lt;DATASET NAME&gt;'\n)\n</code></pre>"},{"location":"python-sdk/#get-task-run-statusprogress","title":"Get Task run status/progress","text":"<p>To check on the status of an ongoing labeling task run, you can use the following function</p> <pre><code>task_run = refuel_client.get_task_run(\ntask='&lt;TASK NAME&gt;',\ndataset='&lt;DATASET NAME&gt;'\n)\n</code></pre> <p>The <code>task_run</code> object has the following schema:</p> <pre><code>{\n'id': '...',\n'task_id': '...',\n'task_name': '...',\n'dataset_id': '...',\n'dataset_name': '...',\n'model_name': '...',   # This is the LLM used for the run\n'status': 'active',    # Status of the task run\n'metrics': [           # Metrics related to the task run\n{'name': 'num_labeled', 'value': ... },\n{'name': 'num_remaining', 'value': ...},\n{'name': 'time_elapsed_seconds', 'value': ...},\n{'name': 'time_remaining_seconds', 'value': ...},\n]\n}\n</code></pre> <p><code>status</code> enum shows the current task run status. It can be one of the following values:</p> <ul> <li><code>not_started</code>: this is the starting state before the batch run has been kicked off</li> <li><code>active</code>: a batch task run is ongoing</li> <li><code>paused</code>: the batch run was paused before the full dataset was labeled.</li> <li><code>failed</code>: the batch run failed due to a platform error. This should ideally never happen</li> <li><code>completed</code>: the batch run was completed successfully</li> </ul> <p><code>metrics</code> is a list containing all metrics for the current task run. Currently the platform supports the following metrics:</p> <ul> <li><code>num_labeled</code>: number of rows from the dataset that have been labeled</li> <li><code>num_remaining</code> number of rows from the dataset that are remaining</li> <li><code>time_elapsed_seconds</code>: time (in seconds) since the task run started. This is only populated when the task run is active (since this metric is not valid when there is no active run).</li> <li><code>time_remaining_seconds</code> estimated time (in seconds) remaining for the task run to complete. This is only populated when the task run is active (since this metric is not valid when there is no active run).</li> </ul>"},{"location":"python-sdk/#applications","title":"Applications","text":"<p>Refuel allows you to deploy a labeling task as an application. Applications allow you to label data synchronously on demand, primarily for online workloads.</p>"},{"location":"python-sdk/#deploy-labeling-application","title":"Deploy labeling application","text":"<p>To deploy an existing task as a labeling application, you can use the following function</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\nresponse = refuel_client.deploy_task(task='&lt;TASK NAME&gt;')\n</code></pre>"},{"location":"python-sdk/#get-all-labeling-application","title":"Get all labeling application","text":"<p>To get all labeling applications that are currently deployed, use the following function</p> <pre><code>applications = refuel_client.get_applications()\n</code></pre>"},{"location":"python-sdk/#label-using-a-deployed-application","title":"Label using a deployed application","text":"<p>You can use the deployed application for online predictions as follows:</p> <pre><code>inputs = [\n{'col_1': 'value_1', 'col_2': 'value_2' ...},\n{'col_1': 'value_1', 'col_2': 'value_2' ...},\n]\nresponse = refuel_client.label(application='&lt;APPLICATION NAME&gt;', inputs=inputs, explain=False)\n</code></pre> <p>Each element in <code>inputs</code> is a dictionary, with keys as names of the input columns defined in the task. For example, let's consider an application for sentiment classification called <code>my_sentiment_classifier</code>, with two input fields - <code>source</code> and <code>text</code>. You can use it as follows:</p> <pre><code>inputs = [\n{'source': 'yelp.com', 'text': 'I really liked the pizza at this restaurant.'}\n]\nresponse = refuel_client.label(application='my_sentiment_classifier', inputs=inputs)\n</code></pre> <p><code>response</code> has the following schema:</p> <ul> <li><code>refuel_output[i]</code> contains the output for <code>inputs[i]</code></li> <li><code>refuel_fields</code> is a list whose length is equal to the number of fields defined in the application. For example, let's say <code>my_sentiment_classifier</code> has just one field, <code>sentiment</code>. In this case the output will be:</li> </ul> <pre><code>{\n  'application_id': '...',\n  'application_name': 'my_sentiment_classifier',\n  'refuel_output': [\n    {'refuel_uuid': '...',\n     'refuel_api_timestamp': '...',\n     'refuel_fields':\n        {\n          'sentiment': {\n            'label': ['positive'],\n            'confidence': 0.9758\n          }\n        }\n    }]\n}\n</code></pre> <p>You can also set the optional <code>explain</code> parameter to <code>True</code> to get an explanation for why the provided label was returned. The explanation will be returned in the <code>explanation</code> field in the response, along with the <code>label</code> and <code>confidence</code>:</p> <pre><code>'sentiment': {\n    'label': ['positive'],\n    'confidence': 0.9758,\n    'explanation': 'The model predicted a positive sentiment because the text contains positive words like \"liked\" and \"good\".'\n  }\n</code></pre> <p>If you would only like to get an explanation for certain fields, you can optionally provide a list of field names in the <code>explain_fields</code> parameter for which you want an explanation returned. If <code>explain_fields</code> is provided, explanations will be returned regardless of whether <code>explain</code> is set to <code>True</code> or <code>False</code>. Here's an example of how to get explanations for the <code>sentiment</code> field in the <code>my_sentiment_classifier</code> application:</p> <pre><code>response = refuel_client.label(application='my_sentiment_classifier', inputs=inputs, explain=True, explain_fields=['sentiment'])\n</code></pre> <p>You can also set the optional <code>telemetry</code> parameter to <code>True</code> to get additional info such as the model, provider, and number of tokens used (prompt, output, and total) in the request. The telemetry data will be returned in the <code>usage</code> field in the response.</p> <pre><code>response = refuel_client.label(application='my_sentiment_classifier', inputs=inputs, telemetry=True)\n</code></pre>"},{"location":"python-sdk/#async-labeling","title":"Async Labeling","text":"<p>If you do not want to wait for the labeling to be completed, you can instead use the method <code>alabel</code> with the exact same parameters as with <code>label</code>. This will submit the inputs for labeling with Refuel and returns the refuel_uuid to get the labeled item back.</p> <pre><code>response = refuel_client.alabel(application='my_sentiment_classifier', inputs=inputs)\n</code></pre> <p>The output will be:</p> <pre><code>{\n  'application_id': '...',\n  'application_name': 'my_sentiment_classifier',\n  'refuel_output': [\n    {'refuel_uuid': '...',\n     'refuel_api_timestamp': '...',\n     'uri': '...'\n    }]\n}\n</code></pre> <p>You can eith use the <code>refuel_uuid</code> from the output to get the labeled item back using <code>get_labeled_item</code> method.</p> <pre><code>response = refuel_client.get_labeled_item(application='my_sentiment_classifier', refuel_uuid='dcbb0266-aeaa-4c0e-87b3-4341b5f3b7bc')\n</code></pre> <p>You can also directly call the returned uri to get the labeled item back.</p> <p>Async Labeling is also useful when the input data is large or the LLM generates large amount of data which can lead to timeouts on Refuel's Application.</p>"},{"location":"python-sdk/#share-feedback-for-application-outputs","title":"Share feedback for application outputs","text":"<p>The SDK allows users to log feedback for online predictions. When logging predictions, it is important to identify the input request for which you're logging feedback using <code>refuel_uuid</code> from the response above:</p> <pre><code>label = {'sentiment': ['negative']}\nrefuel_client.feedback(application='my_sentiment_classifier', refuel_uuid='...', label=label)\n</code></pre> <p>Any row with logged feedback will appear with the verified check mark (\"\u2713\") in the cloud application.</p>"},{"location":"python-sdk/#finetuning","title":"Finetuning","text":"<p>Refuel allows you to finetune a model based on all the human reviewed data and optionally data labeled by an LLM. Finetuned models allow you to reduce labeling cost and latency while achieving similar, and in some cases better, performance than LLMs like GPT-4.</p>"},{"location":"python-sdk/#starting-a-finetuning-run","title":"Starting a finetuning run","text":"<p>To start a model finetuning run, you can use the following function:</p> <pre><code>import refuel\noptions = {\n\"api_key\": \"&lt;YOUR API KEY&gt;\",\n\"project\": \"&lt;PROJECT NAME&gt;\",\n}\nrefuel_client = refuel.init(**options)\nhyperparameters = {\"num_epochs\": 1}\ndatasets = [\"dataset_0_id\", \"dataset_1_id\"]\nresponse = refuel_client.finetune_model(task_id='&lt;TASK ID&gt;', model='&lt;BASE MODEL&gt;', hyperparameters=hyperparameters, datasets=datasets)\n</code></pre> <p>Supported Base Models: ['refuel-llm-v2-large', 'refuel-llm-v2-small'] Supported hyperparameters: ['lora_r', 'lora_alpha', 'lora_dropout', 'weight_decay', 'learning_rate', 'cosine_min_lr_ratio']</p>"},{"location":"python-sdk/#get-all-finetuned-models","title":"Get all finetuned models","text":"<p>You can retrieve a list of all models within a given project as follows</p> <pre><code>models = refuel_client.get_finetuned_models(task_id='&lt;TASK ID&gt;')\n</code></pre>"},{"location":"python-sdk/#cancel-a-finetuning-run","title":"Cancel a finetuning run","text":"<p>You can also cancel an ongoing finetuning run as follows.</p> <pre><code>response = refuel_client.cancel_finetuning(model_id='&lt;FINETUNING MODEL ID&gt;')\n</code></pre>"},{"location":"autolabel/introduction/","title":"Introduction","text":"<p>Autolabel is a Python library to label, clean and enrich datasets with Large Language Models (LLMs).</p>"},{"location":"autolabel/introduction/#new-access-refuelllm-through-autolabel","title":"\ud83c\udf1f (New!) Access RefuelLLM through Autolabel","text":"<p>You can access RefuelLLM, our recently announced LLM purpose built for data labeling, through Autolabel (Read more about it in this blog post). Refuel LLM is a Llama-v2-13b base model, instruction tuned on over 2500 unique (5.24B tokens) labeling tasks spanning categories such as classification, entity resolution, matching, reading comprehension and information extraction. You can experiment with the model in the playground here.</p> <p></p> <p>You can request access to Refuel LLM here. Read the docs about using RefuelLLM in autolabel here.</p>"},{"location":"autolabel/introduction/#features","title":"Features","text":"<ul> <li>Autolabel data for NLP tasks such as classification, question-answering and named entity-recognition, entity matching and more.</li> <li>Seamlessly use commercial and open source LLMs from providers such as OpenAI, Anthropic, HuggingFace, Google and more.</li> <li>Leverage research-proven LLM techniques to boost label quality, such as few-shot learning and chain-of-thought prompting.</li> <li>Confidence estimation and explanations out of the box for every single output label</li> <li>Caching and state management to minimize costs and experimentation time</li> </ul>"},{"location":"autolabel/introduction/#getting-started","title":"Getting Started","text":"<p>You can get started with Autolabel by simpling bringing the dataset you want to label, picking your favorite LLM and writing a few lines of code.</p> <ul> <li>Installation and your first labeling task: Steps to install Autolabel and run sentiment analysis for movie reviews using OpenAI's <code>gpt-3.5-turbo</code>.</li> <li>Classification tutorial: A deeper dive into how Autolabel can be used to detect toxic comments at 95%+ accuracy.</li> <li>Command Line Interface: Learn how to use Autolabel's CLI to intuitively create configs from the command line.</li> <li>Here are more examples with sample notebooks that show how Autolabel can be used for different NLP tasks.</li> </ul>"},{"location":"autolabel/introduction/#resources","title":"Resources","text":"<ul> <li>Discord: Join our Discord community for conversations on LLMs, Autolabel and so much more!</li> <li>Github: Create an issue to report any bugs or give us a star on Github.</li> <li>Contribute: Share your feedback or add new features, and help us improve Autolabel!</li> </ul>"},{"location":"autolabel/concepts/concepts/","title":"Modules","text":"<p>On this page, we will talk about the different pages that exist in Autolabel. We will first discuss the overview of a module and then go into the different subheadings, expanding and giving some examples for each.</p>"},{"location":"autolabel/concepts/concepts/#prompts","title":"Prompts","text":"<p>Writing prompts is a crucial aspect of training language models for specific tasks. In this tutorial, we will explore the five essential parts of a prompt: the prefix prompt, task prompt, output prompt, seed examples, and current example. Understanding and constructing these components effectively can help guide the model's behavior and generate accurate and contextually appropriate responses. Let's delve into each part in detail.</p>"},{"location":"autolabel/concepts/concepts/#prefix-prompt","title":"Prefix Prompt","text":"<p>The prefix prompt is the initial line of the prompt, which sets the domain and provides task-independent information to the model. It helps the model understand the specific area or expertise it should embody while generating responses. For example, if the prefix prompt indicates a medical domain, the model will focus on generating responses that align with medical knowledge and terminology. Example: [Medical] In this prompt, the model should provide expert advice on diagnosing and treating common ailments.</p>"},{"location":"autolabel/concepts/concepts/#task-prompt","title":"Task Prompt","text":"<p>The task prompt explains the objective or task the model needs to accomplish. It describes the specific instructions or guidelines for completing the task. This section is crucial for clearly conveying the desired output from the model. Example: You are a medical expert. Given a patient's symptoms and medical history, provide a diagnosis and recommend appropriate treatment options.</p>"},{"location":"autolabel/concepts/concepts/#output-prompt","title":"Output Prompt","text":"<p>The output prompt informs the model about the expected answer format or structure. It defines the specific format in which the model should provide the answer. This step ensures consistency and enables easier processing of the model's responses. Example: Provide the diagnosis and treatment recommendations in JSON format, with the following keys: \"diagnosis\" and \"treatment.\" The value for each key should be a string representing the diagnosis and treatment, respectively.</p>"},{"location":"autolabel/concepts/concepts/#seed-examples","title":"Seed Examples","text":"<p>Seed examples play a vital role in training the model by providing real-world examples from the task distribution. These examples help the model grasp the nature of the task, understand the expected outputs, and align its behavior accordingly. It is crucial to provide meaningful and diverse seed examples to facilitate accurate responses. Example: Seed Examples:  </p> <p>Patient: Fever, sore throat, and fatigue. Medical History: None. Diagnosis: \"Common cold\" Treatment: \"Rest, plenty of fluids, and over-the-counter cold medication.\" Patient: Persistent cough, shortness of breath, and wheezing. Medical History: Asthma. Diagnosis: \"Asthma exacerbation\" Treatment: \"Inhaled bronchodilators and corticosteroids as prescribed.\"</p>"},{"location":"autolabel/concepts/concepts/#current-example","title":"Current Example","text":"<p>The current example is the specific instance for which you seek the model's response. It provides the exact answer or label you want the model to assign to this particular example. Example: Current Example: Patient: Severe headache, visual disturbances, and nausea. Medical History: None. Desired Diagnosis: \"Migraine\" Desired Treatment: \"Prescribed pain-relief medication and lifestyle modifications.\"  </p>"},{"location":"autolabel/concepts/concepts/#configs","title":"Configs","text":"<p>There are 3 modules required by every labeling run - 1. A task 2. An LLM 3. A dataset</p> <p>All 3 of these modules can be instantiated with configs. A config can be passed in as a dictionary or as the path to a json file. The config consists of different keys and the following section will list out each key along with the property of the module that it affects.</p>"},{"location":"autolabel/concepts/concepts/#config","title":"Config","text":"<p>The Config class is used to parse, validate, and store information about the labeling task being performed.</p> <p>               Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n\"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n# Top-level config keys\nTASK_NAME_KEY = \"task_name\"\nTASK_TYPE_KEY = \"task_type\"\nDATASET_CONFIG_KEY = \"dataset\"\nMODEL_CONFIG_KEY = \"model\"\nEMBEDDING_CONFIG_KEY = \"embedding\"\nPROMPT_CONFIG_KEY = \"prompt\"\nDATASET_GENERATION_CONFIG_KEY = \"dataset_generation\"\nCHUNKING_CONFIG_KEY = \"chunking\"\n# Dataset config keys (config[\"dataset\"][&lt;key&gt;])\nLABEL_COLUMN_KEY = \"label_column\"\nLABEL_SEPARATOR_KEY = \"label_separator\"\nEXPLANATION_COLUMN_KEY = \"explanation_column\"\nIMAGE_COLUMN_KEY = \"image_url_column\"\nTEXT_COLUMN_KEY = \"text_column\"\nINPUT_COLUMNS_KEY = \"input_columns\"\nDELIMITER_KEY = \"delimiter\"\nDISABLE_QUOTING = \"disable_quoting\"\n# Model config keys (config[\"model\"][&lt;key&gt;])\nPROVIDER_KEY = \"provider\"\nMODEL_NAME_KEY = \"name\"\nMODEL_PARAMS_KEY = \"params\"\nCOMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\nLOGIT_BIAS_KEY = \"logit_bias\"\nJSON_MODE = \"json_mode\"\n# Embedding config keys (config[\"embedding\"][&lt;key&gt;])\nEMBEDDING_PROVIDER_KEY = \"provider\"\nEMBEDDING_MODEL_NAME_KEY = \"model\"\n# Prompt config keys (config[\"prompt\"][&lt;key&gt;])\nTASK_GUIDELINE_KEY = \"task_guidelines\"\nVALID_LABELS_KEY = \"labels\"\nFEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\nFEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\nFEW_SHOT_NUM_KEY = \"few_shot_num\"\nVECTOR_STORE_PARAMS_KEY = \"vector_store_params\"\nEXAMPLE_TEMPLATE_KEY = \"example_template\"\nOUTPUT_GUIDELINE_KEY = \"output_guidelines\"\nOUTPUT_FORMAT_KEY = \"output_format\"\nCHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\nLABEL_SELECTION_KEY = \"label_selection\"\nLABEL_SELECTION_COUNT_KEY = \"label_selection_count\"\nLABEL_SELECTION_THRESHOLD = \"label_selection_threshold\"\nATTRIBUTES_KEY = \"attributes\"\nTRANSFORM_KEY = \"transforms\"\n# Dataset generation config keys (config[\"dataset_generation\"][&lt;key&gt;])\nDATASET_GENERATION_GUIDELINES_KEY = \"guidelines\"\nDATASET_GENERATION_NUM_ROWS_KEY = \"num_rows\"\n# Chunking config keys (config[\"chunking\"][&lt;key&gt;])\nCONFIDENCE_CHUNK_COLUMN_KEY = \"confidence_chunk_column\"\nCONFIDENCE_CHUNK_SIZE_KEY = \"confidence_chunk_size\"\nCONFIDENCE_MERGE_FUNCTION_KEY = \"confidence_merge_function\"\ndef __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\nsuper().__init__(config, validate=validate)\ndef _validate(self) -&gt; bool:\n\"\"\"Returns true if the config settings are valid\"\"\"\nfrom autolabel.configs.schema import schema\nvalidate(\ninstance=self.config,\nschema=schema,\n)\nreturn True\n@cached_property\ndef _dataset_config(self) -&gt; Dict:\n\"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\nreturn self.config.get(self.DATASET_CONFIG_KEY, {})\n@cached_property\ndef _model_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\nreturn self.config[self.MODEL_CONFIG_KEY]\n@cached_property\ndef _embedding_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for computing embeddings (e.g. provider name, model name)\"\"\"\nreturn self.config.get(self.EMBEDDING_CONFIG_KEY, {})\n@cached_property\ndef _prompt_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\nreturn self.config[self.PROMPT_CONFIG_KEY]\n@cached_property\ndef _dataset_generation_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt for synthetic dataset generation\"\"\"\nreturn self.config.get(self.DATASET_GENERATION_CONFIG_KEY, {})\n@cached_property\ndef _chunking_config(self) -&gt; Dict:\n\"\"\"Returns information about the chunking config\"\"\"\nreturn self.config.get(self.CHUNKING_CONFIG_KEY, {})\n# project and task definition config\ndef task_name(self) -&gt; str:\nreturn self.config[self.TASK_NAME_KEY]\ndef task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n# Dataset config\ndef label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\ndef label_separator(self) -&gt; str:\n\"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\nreturn self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\ndef text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\ndef input_columns(self) -&gt; List[str]:\n\"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\nreturn self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\ndef explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\ndef image_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an image url for the given item\"\"\"\nreturn self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\ndef delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\ndef disable_quoting(self) -&gt; bool:\n\"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\nreturn self._dataset_config.get(self.DISABLE_QUOTING, False)\n# Model config\ndef provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\ndef model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\ndef model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\ndef confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\ndef logit_bias(self) -&gt; float:\n\"\"\"Returns the logit bias for the labels specified in the config\"\"\"\nreturn self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n# Embedding config\ndef embedding_provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\ndef embedding_model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n# Prompt config\ndef task_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\ndef labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\nelse:\nreturn list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\ndef label_descriptions(self) -&gt; Dict[str, str]:\n\"\"\"Returns a dict of label descriptions\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn {}\nelse:\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, {})\ndef few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\ndef few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\ndef few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\ndef vector_store_params(self) -&gt; Dict:\n\"\"\"Returns any parameters to be passed to the vector store\"\"\"\nreturn self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\ndef example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\ndef output_format(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\ndef output_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\ndef chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\ndef label_selection(self) -&gt; bool:\n\"\"\"Returns true if label selection is enabled. Label selection is the process of\n        narrowing down the list of possible labels by similarity to a given input. Useful for\n        classification tasks with a large number of possible classes.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\ndef max_selected_labels(self) -&gt; int:\n\"\"\"Returns the number of labels to select in LabelSelector\"\"\"\nk = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\nif k &lt; 1:\nreturn len(self.labels_list())\nreturn k\ndef label_selection_threshold(self) -&gt; float:\n\"\"\"Returns the threshold for label selection in LabelSelector\n        If the similarity score ratio with the top Score is above this threshold,\n        the label is selected.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\ndef attributes(self) -&gt; List[Dict]:\n\"\"\"Returns a list of attributes to extract from the text.\"\"\"\nreturn self._prompt_config.get(self.ATTRIBUTES_KEY, [])\ndef transforms(self) -&gt; List[Dict]:\n\"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\nreturn self.config.get(self.TRANSFORM_KEY, [])\ndef dataset_generation_guidelines(self) -&gt; str:\n\"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n)\ndef dataset_generation_num_rows(self) -&gt; int:\n\"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_NUM_ROWS_KEY, 1\n)\ndef confidence_chunk_column(self) -&gt; str:\n\"\"\"Returns the column name to use for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\ndef confidence_chunk_size(self) -&gt; int:\n\"\"\"Returns the chunk size for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\ndef confidence_merge_function(self) -&gt; str:\n\"\"\"Returns the function to use when merging confidence scores\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\ndef json_mode(self) -&gt; bool:\n\"\"\"Returns true if the model should be used in json mode. Currently only used for OpenAI models.\"\"\"\nreturn self._model_config.get(self.JSON_MODE, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.attributes","title":"<code>attributes()</code>","text":"<p>Returns a list of attributes to extract from the text.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def attributes(self) -&gt; List[Dict]:\n\"\"\"Returns a list of attributes to extract from the text.\"\"\"\nreturn self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_column","title":"<code>confidence_chunk_column()</code>","text":"<p>Returns the column name to use for confidence chunking</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_column(self) -&gt; str:\n\"\"\"Returns the column name to use for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_size","title":"<code>confidence_chunk_size()</code>","text":"<p>Returns the chunk size for confidence chunking</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_size(self) -&gt; int:\n\"\"\"Returns the chunk size for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.confidence_merge_function","title":"<code>confidence_merge_function()</code>","text":"<p>Returns the function to use when merging confidence scores</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_merge_function(self) -&gt; str:\n\"\"\"Returns the function to use when merging confidence scores\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_guidelines","title":"<code>dataset_generation_guidelines()</code>","text":"<p>Returns a string containing guidelines for how to generate a synthetic dataset</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_guidelines(self) -&gt; str:\n\"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_num_rows","title":"<code>dataset_generation_num_rows()</code>","text":"<p>Returns the number of rows to generate for the synthetic dataset</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_num_rows(self) -&gt; int:\n\"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_NUM_ROWS_KEY, 1\n)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.disable_quoting","title":"<code>disable_quoting()</code>","text":"<p>Returns true if quoting is disabled. Defaults to false</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def disable_quoting(self) -&gt; bool:\n\"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\nreturn self._dataset_config.get(self.DISABLE_QUOTING, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.embedding_model_name","title":"<code>embedding_model_name()</code>","text":"<p>Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def embedding_model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.embedding_provider","title":"<code>embedding_provider()</code>","text":"<p>Returns the name of the entity that provides the model used for computing embeddings</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def embedding_provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.image_column","title":"<code>image_column()</code>","text":"<p>Returns the name of the column containing an image url for the given item</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def image_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an image url for the given item\"\"\"\nreturn self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.input_columns","title":"<code>input_columns()</code>","text":"<p>Returns the names of the input columns from the dataset that are used in the prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def input_columns(self) -&gt; List[str]:\n\"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\nreturn self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.json_mode","title":"<code>json_mode()</code>","text":"<p>Returns true if the model should be used in json mode. Currently only used for OpenAI models.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def json_mode(self) -&gt; bool:\n\"\"\"Returns true if the model should be used in json mode. Currently only used for OpenAI models.\"\"\"\nreturn self._model_config.get(self.JSON_MODE, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_descriptions","title":"<code>label_descriptions()</code>","text":"<p>Returns a dict of label descriptions</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_descriptions(self) -&gt; Dict[str, str]:\n\"\"\"Returns a dict of label descriptions\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn {}\nelse:\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, {})\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_selection","title":"<code>label_selection()</code>","text":"<p>Returns true if label selection is enabled. Label selection is the process of narrowing down the list of possible labels by similarity to a given input. Useful for classification tasks with a large number of possible classes.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_selection(self) -&gt; bool:\n\"\"\"Returns true if label selection is enabled. Label selection is the process of\n    narrowing down the list of possible labels by similarity to a given input. Useful for\n    classification tasks with a large number of possible classes.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_selection_threshold","title":"<code>label_selection_threshold()</code>","text":"<p>Returns the threshold for label selection in LabelSelector If the similarity score ratio with the top Score is above this threshold, the label is selected.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_selection_threshold(self) -&gt; float:\n\"\"\"Returns the threshold for label selection in LabelSelector\n    If the similarity score ratio with the top Score is above this threshold,\n    the label is selected.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.label_separator","title":"<code>label_separator()</code>","text":"<p>Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_separator(self) -&gt; str:\n\"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\nreturn self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\nelse:\nreturn list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.logit_bias","title":"<code>logit_bias()</code>","text":"<p>Returns the logit bias for the labels specified in the config</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def logit_bias(self) -&gt; float:\n\"\"\"Returns the logit bias for the labels specified in the config\"\"\"\nreturn self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.max_selected_labels","title":"<code>max_selected_labels()</code>","text":"<p>Returns the number of labels to select in LabelSelector</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def max_selected_labels(self) -&gt; int:\n\"\"\"Returns the number of labels to select in LabelSelector\"\"\"\nk = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\nif k &lt; 1:\nreturn len(self.labels_list())\nreturn k\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.transforms","title":"<code>transforms()</code>","text":"<p>Returns a list of transforms to apply to the data before sending to the model.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def transforms(self) -&gt; List[Dict]:\n\"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\nreturn self.config.get(self.TRANSFORM_KEY, [])\n</code></pre>"},{"location":"autolabel/concepts/concepts/#src.autolabel.configs.config.AutolabelConfig.vector_store_params","title":"<code>vector_store_params()</code>","text":"<p>Returns any parameters to be passed to the vector store</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def vector_store_params(self) -&gt; Dict:\n\"\"\"Returns any parameters to be passed to the vector store\"\"\"\nreturn self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n</code></pre>"},{"location":"autolabel/concepts/concepts/#tasks","title":"Tasks","text":""},{"location":"autolabel/concepts/concepts/#classification","title":"Classification","text":""},{"location":"autolabel/concepts/concepts/#question-answering","title":"Question Answering","text":""},{"location":"autolabel/concepts/concepts/#entity-matching","title":"Entity matching","text":""},{"location":"autolabel/concepts/concepts/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"autolabel/guide/accuracy/chain-of-thought/","title":"Chain of Thought","text":"Chain of Thought Prompting (Wei et al) <p>LLMs find it hard to perform well on complex reasoning tasks. We can unlock the reasoning abilities of LLMs using chain of thought prompting. This involves asking the LLM to produce the reasoning before producing the answer (roughly analogous to \"show me your work\").</p> <p>Chain of thought makes LLMs more effective at reasoning tasks like mathematical word problems, commonsense reasoning questions and complex medical questions. It also provides a window into the thought process of the LLM, though some research points the link between the generated explanation and the final answer may be weak.</p>"},{"location":"autolabel/guide/accuracy/chain-of-thought/#using-chain-of-thought-in-autolabel","title":"Using Chain Of Thought in Autolabel","text":"<p>Enabling chain-of-thought prompting for your task is straightforward with Autolabel. It works best when provided with a few seed examples with explanations. Thus enabling chain of thought requires a few things:</p> <ol> <li>Setting <code>chain_of_thought</code> flag in the labeling config.</li> <li>Providing explanations or generating explanations for your seed examples automatically by using an LLM</li> <li>Setting the <code>explanation_column</code> in the labeling config.</li> <li>Altering the task guidelines and <code>example_template</code> to tell the model to generate an explanation before generating the final answer.</li> </ol> <p>We will go through using chain of thought on a dataset where it shows improvement, like the SQuAD question answering dataset.</p> <p>Let's see a datapoint before there is any explanation added to it.</p> context question answer Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 <p>Now we can manually write the explanation for this or a couple of seed examples easily. But this will be tiresome for &gt; 10 examples. LLMs come to the rescue yet again! We can just define the config and ask the agent to generate explanations as well!</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"explanation_column\": \"explanation\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. Use the context to answer the question - the answer is a continuous span of words from the context.\\n\",\n\"output_guidelines\": \"Your answer will consist of an explanation, followed by the correct answer. The last line of the response should always be is JSON format with one key: {\\\"label\\\": \\\"the correct answer\\\"}.\\n If the question cannot be answered using the context and the context alone without any outside knowledge, the question is unanswerable. If the question is unanswerable, return the answer as {\\\"label\\\": \\\"unanswerable\\\"}\\n\",\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 3,\n\"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: Let's think step by step.\\n{explanation}\\n{answer}\",\n\"chain_of_thought\": True\n}\n}\n</code></pre> <p>Notice the changes that we have made to the config compared to the config without Chain-of-Thought here:</p> <ul> <li><code>chain_of_thought</code> flag - this tells labeling agent to expect an explanation for the answer, in the seed dataset as well as LLM generated responses.</li> <li><code>explanation_column</code> - this is the column where the explanation for the seed examples will reside.</li> <li><code>example_template</code> - Notice that the template contains contains the explanation column as well. This tells the config where the explanation should be put when using the seed examples. We use the <code>Let's think step by step</code> prompt to initiate the chain of thought in the model.</li> <li><code>output_guidelines</code> - We are explicitly prompting the LLM to first output an explanation, and then the final answer.</li> </ul> <p>Now, in order to generate explanations for the seed examples, in case they were not manually generated is,</p> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nagent.generate_explanations(\"path_to_seed_examples.csv\")\n</code></pre> <p>Once these explanations are generated, the dataset looks like</p> context question answer explanation Private schools generally prefer to be called independent schools, because of their freedom to operate outside of government and local government control. Some of these are also known as public schools. Preparatory schools in the UK prepare pupils aged up to 13 years old to enter public schools. The name 'public school' is based on the fact that the schools were open to pupils from anywhere, and not merely to those from a certain locality, and of any religion or occupation. According to The Good Schools Guide approximately 9 per cent of children being educated in the UK are doing so at fee-paying schools at GSCE level and 13 per cent at A-level.[citation needed] Many independent schools are single-sex (though this is becoming less common). Fees range from under \u00a33,000 to \u00a321,000 and above per year for day pupils, rising to \u00a327,000+ per year for boarders. For details in Scotland, see 'Meeting the Cost'. At A-level, what percentage of British students attend fee-paying schools? 13 Independent schools in the UK are private schools that charge fees.  These schools are also known as public schools. According to The Good Schools Guide, about 9% of children in the UK attend fee-paying schools at the GSCE level. At the A-level, which is a higher level of education, a higher percentage of students, 13%, attend fee-paying independent schools. Since 13% of students attend fee-paying schools at the A-level, and the question asks what percentage attend at the A-level specifically, So, the answer is 13. <p>Now to generate labels for this dataset, all we have to do is,</p> <pre><code>from autolabel import AutolabelDataset\nds = AutolabelDatset('data/squad_v2_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre> <p>Autolabel currently supports Chain-of-thought prompting for the following tasks:</p> <ol> <li>Classifcation (example)</li> <li>Entity Match</li> <li>Question Answering (example)</li> </ol> <p>Support for other tasks coming soon!</p>"},{"location":"autolabel/guide/accuracy/confidence/","title":"Confidence","text":"ChatGPT summarizing a non-existent New York Times article even without access to the Internet <p>One of the biggest criticisms of using a LLMs so far has been hallucinations - LLMs can seem very confidence in their language even when they are completely incorrect. <code>autolabel</code> provides a confidence score for each LLM output that is correlated with the likelihood of that output being incorrect, i.e. if the confidence score is high, then it is more likely that the output is correct, and if confidence score is low, it is likely that the LLM has produced an incorrect output.</p>"},{"location":"autolabel/guide/accuracy/confidence/#computing-confidence-scores","title":"Computing Confidence Scores","text":"<p>The <code>autolabel</code> library today relies on token level probabilities, also known as logprobs, to compute confidence scores. However, very few models today return token level probabilities alongside prediction. Out of all models supported by <code>autolabel</code> today, only the <code>text-davinci-003</code> model by <code>openai</code> can return logprobs. For all other models, Refuel has setup an in-house API to generate logprobs for a specific prediction given an input, regardless of the language model that was originally used to query for the prediction. For <code>text-davinci-003</code>, we use the logprobs returned by <code>openai</code>'s API instead of querying our in-house API.</p> <p>Generating confidence scores is simple - setting the key <code>compute_confidence</code> to <code>True</code> in the <code>model</code> dictionary of the config should initiate confidence score retrieval. Here is an example:</p> <pre><code>{\n\"task_name\": \"PersonLocationOrgMiscNER\",\n\"task_type\": \"named_entity_recognition\",\n\"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\",\n\"delimiter\": \"%\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-v1\",\n\"compute_confidence\": True\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at extracting entities from text.\",\n\"labels\": [\n\"Location\",\n\"Organization\",\n\"Person\",\n\"Miscellaneous\"\n],\n\"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n\"few_shot_examples\": \"../examples/conll2003/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5\n}\n}\n</code></pre> <p>In the above example, by setting <code>compute_confidence</code> to True, <code>autolabel</code> will start calling Refuel's api to generate token level probabilities and compute confidence scores for each prediction. In order for this to run successfully, ensure that the following setup has been completed:</p> <p>Set the following environment variable: <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> replacing <code>&lt;your-refuel-key&gt;</code> with your API key, which you can get from here</p>"},{"location":"autolabel/guide/accuracy/confidence/#interpreting-scores","title":"Interpreting Scores","text":"<p>To see how confidence scores can be used to make a tradeoff between task performance and completion rate, let's take a look at the following example:</p> <p> </p> Library output when confidence is enabled <p><code>autolabel</code> outputs a table consisting of metrics at various confidence thresholds when <code>compute_confidence</code> is set to <code>True</code>. Specifically, this is the table we get when we label 100 examples from the CONLL-2003 dataset with semantic similarity enabled. The first row in the table corresponds to the overall performance: we were able to successfully label 98% of examples at an F1 score of 0.885. However, we can use this table to decide on a confidence threshold to accept predictions at and increase our metrics. For example, note that according the highlighed row, if we accept labels with confidence scores above ~2.207, we can boost our F1 score to 0.95 while reducing completion rate to 79%. </p>"},{"location":"autolabel/guide/accuracy/few-shot/","title":"Few-shot Prompting","text":"<p>It has been shown that the specific seed examples used while constructing the prompt have an impact on the performance of the model. Seed examples are the labeled examples which are shown as demonstration to the LLM to help it understand the task better. Optimally selecting the seed examples can help boost performance and save on labeling costs by reducing the context size.</p> <p>We support the following few-shot example selection techniques:</p> <ol> <li>Fixed - The same set of seed examples are used for every input data point.</li> <li>Semantic_similarity - Embeddings are computed for all the examples in the seed set and a vector similarity search finds the few shot examples which are closest to the input datapoint. Closer datapoints from the seed set can give the model more context on how similar examples have been labeled, helping it improve performance.</li> <li>Max_marginal_relevance - Semantic similarity search is used to retrieve a set of candidate examples. Then, a diversity-driven selection strategy is used amongst these candidates to select a final subset of examples that have the most coverage of the initial pool of candidate examples.</li> <li>Label diversity - This strategy focuses on ensuring that the few-shot examples selected provide coverage across all the valid output labels.</li> <li>Label diversity with similarity - This strategy is a combination of (2) and (4) above - it samples a fixed number of examples per valid label, and within each label it selects the examples that are most similar to the input.</li> </ol> <p>Example:</p> <p></p> <p>Consider the following labeling runs for a classification task on the banking dataset. There are a total of 1998 items to be labeled and we assume a starting labeled seedset of 200 examples. Here is the config to label this dataset in zero-shot fashion:</p> <pre><code>config_zero_shot = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n\"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n\"atm_support\",\n\"automatic_top_up\",\n\"balance_not_updated_after_bank_transfer\",\n\"balance_not_updated_after_cheque_or_cash_deposit\",\n\"beneficiary_not_allowed\",\n\"cancel_transfer\",\n\"card_about_to_expire\",\n\"card_acceptance\",\n\"card_arrival\",\n\"card_delivery_estimate\",\n\"card_linking\",\n\"card_not_working\",\n\"card_payment_fee_charged\",\n\"card_payment_not_recognised\",\n\"card_payment_wrong_exchange_rate\",\n\"card_swallowed\",\n\"cash_withdrawal_charge\",\n\"cash_withdrawal_not_recognised\",\n\"change_pin\",\n\"compromised_card\",\n\"contactless_not_working\",\n\"country_support\",\n\"declined_card_payment\",\n\"declined_cash_withdrawal\",\n\"declined_transfer\",\n\"direct_debit_payment_not_recognised\",\n\"disposable_card_limits\",\n\"edit_personal_details\",\n\"exchange_charge\",\n\"exchange_rate\",\n\"exchange_via_app\",\n\"extra_charge_on_statement\",\n\"failed_transfer\",\n\"fiat_currency_support\",\n\"get_disposable_virtual_card\",\n\"get_physical_card\",\n\"getting_spare_card\",\n\"getting_virtual_card\",\n\"lost_or_stolen_card\",\n\"lost_or_stolen_phone\",\n\"order_physical_card\",\n\"passcode_forgotten\",\n\"pending_card_payment\",\n\"pending_cash_withdrawal\",\n\"pending_top_up\",\n\"pending_transfer\",\n\"pin_blocked\",\n\"receiving_money\",\n\"Refund_not_showing_up\",\n\"request_refund\",\n\"reverted_card_payment?\",\n\"supported_cards_and_currencies\",\n\"terminate_account\",\n\"top_up_by_bank_transfer_charge\",\n\"top_up_by_card_charge\",\n\"top_up_by_cash_or_cheque\",\n\"top_up_failed\",\n\"top_up_limits\",\n\"top_up_reverted\",\n\"topping_up_by_card\",\n\"transaction_charged_twice\",\n\"transfer_fee_charged\",\n\"transfer_into_account\",\n\"transfer_not_received_by_recipient\",\n\"transfer_timing\",\n\"unable_to_verify_identity\",\n\"verify_my_identity\",\n\"verify_source_of_funds\",\n\"verify_top_up\",\n\"virtual_card_not_working\",\n\"visa_or_mastercard\",\n\"why_verify_identity\",\n\"wrong_amount_of_cash_received\",\n\"wrong_exchange_rate_for_cash_withdrawal\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config=config_zero_shot)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_zero_shot)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>This zero-shot task execution results in an accuracy of 70.19%.</p> <p>Iterating on this, we compare a fixed few-shot example selection strategy, which randomly chooses k examples from the labeled seedset and appends these same k examples to each prompt for the 1998 items to be labeled. In this case, we use k=10 seed examples per prompt. To use this selection strategy, we need to modify the config:</p> <pre><code>config_fixed_few_shot = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": \"../examples/banking/seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_fixed_few_shot)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_fixed_few_shot)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>This leads to an accuracy of 73.16%, an improvement of ~3% over the zero-shot baseline.</p> <p>Finally, we compare a semantic similarity example selection strategy, which computes a text embedding for each of the 200 labeled seedset examples. Then, for each of the 1998 items to be labeled, we compute a text embedding and find the k most similar examples from the labeled seedset and append those k examples to the prompt for the current example. This leads to custom examples used for each item to be labeled, with the idea being that more similar examples and their corresponding labels may assist the LLM in labeling. Here is the config change to use semantic similarity as the example selection strategy:</p> <pre><code>config_semantic_similarity = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": \"../examples/banking/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_semantic_similarity)\nds = AutolabelDataset('../examples/banking/test.csv', config = config_semantic_similarity)\nlabeled_dataset = agent.run(ds)\n</code></pre> <p>With semantic similarity example selection, we obtain a 79.02% accuracy, a significant increase of ~6% over the fixed-shot strategy.</p> <p>Finally, let's take a look at label diversity set of example selection techniques in action:</p> <pre><code>config_label_diversity_random = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n\"few_shot_selection\": \"label_diversity_random\",\n\"few_shot_num\": 5,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_label_diversity_random)\nds = AutolabelDataset('../examples/civil_comments/test.csv', config = config_label_diversity_random)\nlabeled_dataset = agent.run(ds, max_items=200)\n</code></pre> <pre><code>config_label_diversity_similarity = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n...\n\"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n\"few_shot_selection\": \"label_diversity_similarity\",\n\"few_shot_num\": 5,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <pre><code>agent = LabelingAgent(config=config_label_diversity_similarity)\nds = AutolabelDataset('../examples/civil_comments/test.csv', config = config_label_diversity_similarity)\nlabeled_dataset = agent.run(ds, max_items=200)\n</code></pre> <p>For this run on the civil comments dataset, label diversity at random achieved 80% accuracy and label diversity with semantic similarity achieved 78% accuracy. For the same subset of data, the use of regular semantic similarity example selection obtained 72% accuracy, making for a significant improvement by using label diversity. </p> <p>Label diversity example selection strategies are likely best suited for labeling tasks with a small number of unique labels, which is the case for the civil comments dataset with only 2 labels. This is because equal representation of all the possible labels may be less likely to bias the LLM towards a particular label.</p> <p>By default, Autolabel uses OpenAI to compute text embeddings for few shot example selection strategies that require them (semantic similarity, max marginal relevance). However, Autolabel also supports alternative embedding model providers such as Google Vertex AI and Huggingface as outlined here.</p> <p>It is almost always advisable to use an example selection strategy over a zero-shot approach in your autolabeling workflows, but the choice of which example selection strategy to use is dependent upon the specific labeling task and dataset.</p>"},{"location":"autolabel/guide/accuracy/prompting-better/","title":"Prompting Better","text":"<p>Like most LLM tasks, a critical part of improving LLM performance in autolabeling tasks is selecting a good prompt. Often, this entails finding a good balance between a descriptive set of instructions, while still remaining concise and clear. </p> <p>Consider the following example of refining a prompt used for a classification task on the civil-comments dataset. Each labeling run below included 500 examples and used the same LLM: gpt-3.5-turbo and used a fixed-shot example selection strategy with 4 seed examples.</p> <p></p> <p>First attempt: <pre><code>config = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"compute_confidence\": True\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments and understanding if a comment is sexually explicit, obscene, toxic, insults a person, demographic or race. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": \"../examples/civil_comments/seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre></p> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config=config)\ndataset = AutolabelDataset('../examples/civil_comments/test.csv', config=config)\nlabeled_dataset = agent.run(dataset, max_items = 100)\n</code></pre> <p>Accuracy: 68%</p> <p>This first basic prompt seems clear and concise, but only attains a baseline accuracy of 68%. We can analyze some of the errors the LLM is making to get a better idea of how to improve our prompt. </p> <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']]\n</code></pre> <p>In doing so, we notice that a vast majority of the errors (97.2%) are misclassifications of civil comments as toxic by the LLM. For instance, one such example comment is:</p> <pre><code>'This is malfeasance by the Administrator and the Board. They are wasting our money!'\n</code></pre> <p>The presence of generally negative words such as \"malfeasance\" and \"wasting\" may be misleading the LLM. Our prompt may need to include details that guide the LLM to correctly identify cases where the vocabulary used could be mistaken as toxic, but the surrounding context suggests that the comment is actually civil.</p> <p>Adding nuance to the prompt:</p> <p>We can replace the prompt in the above config with the following updated guidelines and re-run the labeling task.</p> <pre><code>\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'.\\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n</code></pre> <pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config=config)\ndataset = AutolabelDataset('../examples/civil_comments/test.csv', config=config)\nlabeled_dataset = agent.run(dataset, max_items = 100)\n</code></pre> <p>Accuracy: 74%</p> <p>In this second iteration, we added more detail to the prompt such as addressing the nuances between \"fair criticisms\" vs. toxic comments. These additional details lead to better performance, reaching 74% accuracy. From a similar analysis of the LLM errors, we see that the previous misclassification example, along with several other similar ones, has now been correctly labeled.</p> <p>Further improvements:</p> <p>After subsequently experimenting with a few different variations to this prompt, we do not see significant improvements in performance for this task. As a result, after sufficient iteration of the prompt, it is better to look for performance gains through other modifications to the task configuration. For example, comparing different LLMs can often lead to significant improvements. With the same final prompt above, the text-davinci-003 model achieved 88% accuracy, a 14% increase compared to gpt-turbo-3.5.</p>"},{"location":"autolabel/guide/llms/benchmarks/","title":"Benchmarks","text":""},{"location":"autolabel/guide/llms/benchmarks/#benchmarking-llms-for-data-labeling","title":"Benchmarking LLMs for data labeling","text":"<p>Key takeaways from our technical report:</p> <ul> <li>State of the art LLMs can label text datasets at the same or better quality compared to skilled human annotators, but ~20x faster and ~7x cheaper.</li> <li>For achieving the highest quality labels, GPT-4 is the best choice among out of the box LLMs (88.4% agreement with ground truth, compared to 86% for skilled human annotators). </li> <li>For achieving the best tradeoff between label quality and cost, GPT-3.5-turbo, PaLM-2 and open source models like FLAN-T5-XXL are compelling.</li> <li>Confidence based thresholding can be a very effective way to mitigate impact of hallucinations and ensure high label quality.</li> </ul>"},{"location":"autolabel/guide/llms/embeddings/","title":"Embedding Models","text":"<p>Autolabel also supports various models to compute text embeddings that are used in some few shot example selection strategies such as semantic similarity and max marginal relevance. Like the LLMs that Autolabel supports, each embedding model belongs to a provider. Currently the library supports embedding models from 3 providers: OpenAI, Google Vertex AI, and Huggingface. By default, if no embedding config is present in the labeling config but a few shot strategy that requires text embeddings is enabled, Autolabel defaults to use OpenAI embeddings and an OpenAI API key will be required. </p> <p>Details on how to set up the embedding config for each provider are below.</p>"},{"location":"autolabel/guide/llms/embeddings/#openai","title":"OpenAI","text":"<p>To use models from OpenAI, you can set <code>provider</code> to <code>openai</code> under the <code>embedding</code> key in the labeling configuration. Then, the specific model that will be queried can be specified using the <code>model</code> key. The default embedding model, if none is provided, is <code>text-embedding-ada-002</code></p>"},{"location":"autolabel/guide/llms/embeddings/#setup","title":"Setup","text":"<p>To use OpenAI models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> and also setting the following environment variable: <pre><code>export OPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre> replacing <code>&lt;your-openai-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"autolabel/guide/llms/embeddings/#example-usage","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use OpenAI's <code>text-embedding-ada-002</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>openai</code> and <code>model</code> is not set so it will default to <code>text-embedding-ada-002</code>.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"embedding\": {\n\"provider\": \"openai\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/embeddings/#hugging-face","title":"Hugging Face","text":"<p>To use models from Hugging Face, you can set <code>provider</code> to <code>huggingface_pipeline</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. </p> <p>This will run the model locally on a GPU (if available). You can also specify  quantization strategy to load larger models in lower precision (and thus decreasing memory requirements).</p>"},{"location":"autolabel/guide/llms/embeddings/#setup_1","title":"Setup","text":"<p>To use Hugging Face models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[huggingface]'\n</code></pre></p>"},{"location":"autolabel/guide/llms/embeddings/#example-usage_1","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use the <code>sentence-transformers/all-mpnet-base-v2</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>huggingface_pipeline</code> and <code>model</code> is set to be <code>sentence-transformers/all-mpnet-base-v2</code>.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"google/flan-t5-small\",\n\"params\": {}\n},\n\"embedding\": {\n\"provider\": \"huggingface_pipeline\",\n\"model\": \"sentence-transformers/all-mpnet-base-v2\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/embeddings/#google-vertex-ai","title":"Google Vertex AI","text":"<p>To use models from Google, you can set the <code>provider</code> to <code>google</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>model</code> key. </p>"},{"location":"autolabel/guide/llms/embeddings/#setup_2","title":"Setup","text":"<p>To use Google models with Autolabel, make sure to first install the relevant packages by running: <pre><code>pip install 'refuel-autolabel[google]'\n</code></pre> and also setting up Google authentication locally.</p>"},{"location":"autolabel/guide/llms/embeddings/#example-usage_2","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use google's <code>textembedding-gecko</code> model for computing text embeddings. Specifically, note that in the dictionary provided by the <code>embedding</code> tag, <code>provider</code> is set to <code>google</code> and <code>model</code> is set to be <code>textembedding-gecko</code>.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"google\",\n\"name\": \"text-bison@001\",\n\"params\": {}\n},\n\"embedding\": {\n\"provider\": \"google\",\n\"model\": \"textembedding-gecko\"\n}\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/","title":"Large Language Models (LLMs)","text":"<p>Autolabel supports multiple LLMs for labeling data. Some LLMs are available by calling an API with the appropriate API keys (OpenAI, Anthropic, etc.) while others can be run locally (such as the ones available on Hugging Face). The LLM used to label can be controlled using the <code>provider</code> and <code>name</code> keys in the dictionary specified under <code>model</code> in the input config.</p> <p>Each LLM belongs to an LLM provider -- which refers to the organization or open-source framework through which we are able to access the LLM. A full list of LLM providers and LLMs that are currently supported is provided towards the end of this page.</p> <p>Autolabel makes it easy to try out different LLMs for your task and this page will walk you through how to get started with each LLM provider and model. Separately, we've also benchmarked multiple LLMs across different datasets - you can read the full technical report here [link to blog post] or check out the latest benchmark results here.</p>"},{"location":"autolabel/guide/llms/llms/#refuel","title":"Refuel","text":"<p>To use models hosted by Refuel, you can set <code>provider</code> to <code>refuel</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports two models:</p> <ul> <li><code>refuel-llm</code></li> <li><code>llama-13b-chat</code></li> </ul> <p>You can access RefuelLLM, our recently announced LLM purpose built for data labeling, through Autolabel (Read more about it in this blog post). Refuel LLM is a Llama-v2-13b base model, instruction tuned on over 2500 unique (5.24B tokens) labeling tasks spanning categories such as classification, entity resolution, matching, reading comprehension and information extraction. You can experiment with the model in the playground here.</p> <p></p> <p>You can request access to Refuel LLM here. Read the docs about using RefuelLLM in autolabel here.</p> <p>Llama-13b-chat is a 13 billion parameter model available on Huggingface. However, running such a huge model locally is a challenge, which is why we are currently hosting the model on our servers.</p>"},{"location":"autolabel/guide/llms/llms/#setup","title":"Setup","text":"<p>To use Refuel models with Autolabel, make sure set the following environment variable:</p> <pre><code>export REFUEL_API_KEY=&lt;your-refuel-key&gt;\n</code></pre> <p>replacing <code>&lt;your-refuel-key&gt;</code> with your API key.</p>"},{"location":"autolabel/guide/llms/llms/#getting-a-refuel-api-key","title":"Getting a Refuel API key","text":"<p>If you're interested in trying one of the LLMs hosted by Refuel, sign up for your Refuel API key by filling out the form here. We'll review your application and get back to you soon!</p>"},{"location":"autolabel/guide/llms/llms/#example-usage","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use Refuel's <code>refuel-llm</code> model. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>refuel</code> and <code>name</code> is set to be <code>refuel-llm</code>.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"refuel\",\n\"name\": \"refuel-llm\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>refuel</code> models to control the model behavior. For example:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"refuel\",\n\"name\": \"refuel-llm\",\n\"params\": {\n\"max_new_tokens\": 512,\n\"temperature\": 0.1,\n}\n}\n</code></pre> <p><code>refuel</code> hosted LLMs support all the parameters that can be passed as a part of GenerationConfig while calling generate functions of Hugging Face LLMs.</p>"},{"location":"autolabel/guide/llms/llms/#openai","title":"OpenAI","text":"<p>To use models from OpenAI, you can set <code>provider</code> to <code>openai</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from OpenAI:</p> <ul> <li><code>text-davinci-003</code></li> <li><code>gpt-3.5-turbo</code>, <code>gpt-3.5-turbo-0301</code> and <code>gpt-3.5-turbo-0613</code> (4,096 max tokens)</li> <li><code>gpt-3.5-turbo-16k</code> and <code>gpt-3.5-turbo-16k-0613</code> (16,384 max tokens)</li> <li><code>gpt-4</code>, <code>gpt-4-0314</code> and <code>gpt-4-0613</code> (8,192 max tokens)</li> <li><code>gpt-4-32k</code>, <code>gpt-4-32k-0314</code> and <code>gpt-4-32k-0613</code> (32,768 max tokens)</li> </ul> <p><code>gpt-4</code> set of models are the most capable (and most expensive) from OpenAI, while <code>gpt-3.5-turbo</code> set of models are cheap (but still quite capable). Detailed pricing for these models is available here.</p>"},{"location":"autolabel/guide/llms/llms/#setup_1","title":"Setup","text":"<p>To use OpenAI models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export OPENAI_API_KEY=&lt;your-openai-key&gt;\n</code></pre> <p>replacing <code>&lt;your-openai-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"autolabel/guide/llms/llms/#example-usage_1","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use OpenAI's <code>gpt-3.5-turbo</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>openai</code> and <code>name</code> is set to be <code>gpt-3.5-turbo</code>. <code>name</code> can be switched to use any of the three models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters_1","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>openai</code> models to tweak their behavior:</p> <ul> <li><code>max_tokens</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {\n\"max_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#anthropic","title":"Anthropic","text":"<p>To use models from Anthropic, you can set the <code>provider</code> to <code>anthropic</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Anthropic:</p> <ul> <li><code>claude-instant-1.2</code></li> <li><code>claude-2.0</code></li> <li><code>claude-2.1</code></li> <li><code>claude-3-opus-20240229</code></li> <li><code>claude-3-sonnet-20240229</code></li> <li><code>claude-3-haiku-20240307</code></li> </ul> <p><code>claude-3-opus-20240229</code> is a state-of-the-art high-performance model, while <code>claude-3-sonnet-20240229</code> is a lighter, less expensive, and much faster option. <code>claude-3-sonnet-20240229</code> is 5 times cheaper than <code>claude-3-opus-20240229</code>, at $3/1 million tokens. On the other hand <code>claude-3-opus-20240229</code> costs $15/1 million tokens. <code>claude-3-haiku-20240307</code> is the lightest and fastest model, and is 12 times cheaper than <code>claude-3-sonnet-20240229</code>, at $0.25/1 million tokens.</p> <p><code>claude-instant-1.2</code>, <code>claude-2.0</code>, and <code>claude-2.1</code> are older legacy models and are not recommended for new tasks. Detailed pricing for these models is available here. One may use <code>claude-instant-1.2</code> for a cheaper alternative to any model and <code>claude-2.0</code> and <code>claude-2.1</code> for a cheaper alternative to <code>claude-3-opus-20240229</code>.</p>"},{"location":"autolabel/guide/llms/llms/#setup_2","title":"Setup","text":"<p>To use Anthropic models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[anthropic]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export ANTHROPIC_API_KEY=&lt;your-anthropic-key&gt;\n</code></pre> <p>replacing <code>&lt;your-anthropic-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"autolabel/guide/llms/llms/#example-usage_2","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use anthropic's <code>claude-instant-v1</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>anthropic</code> and <code>name</code> is set to be <code>claude-instant-v1</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-instant-v1\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters_2","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>anthropic</code> models to control the model behavior:</p> <ul> <li><code>max_tokens_to_sample</code> (int): The maximum tokens to sample from the model</li> <li><code>temperature</code> (float): A float between 0 and 2 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-2.0\",\n\"params\": {\n\"max_tokens_to_sample\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#hugging-face","title":"Hugging Face","text":"<p>To use models from Hugging Face, you can set <code>provider</code> to <code>huggingface_pipeline</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports all Sequence2Sequence and Causal Language Models on Hugging Face. All models available on Hugging Face can be found here. Ensure that the model you choose can be loaded using <code>AutoModelForSeq2SeqLM</code> or <code>AutoModelForCausalLM</code>. Here are a few examples:</p> <p>Sequence2Sequence Language Models:</p> <ul> <li><code>google/flan-t5-small</code> (all flan-t5-* models)</li> <li><code>google/pegasus-x-base</code></li> <li><code>microsoft/prophetnet-large-uncased</code></li> </ul> <p>Causal Language Models:</p> <ul> <li><code>gpt2</code></li> <li><code>openlm-research/open_llama_3b</code></li> <li><code>meta-llama/Llama-2-7b</code></li> </ul> <p>This will run the model locally on a GPU (if available). You can also specify quantization strategy to load larger models in lower precision (and thus decreasing memory requirements).</p>"},{"location":"autolabel/guide/llms/llms/#setup_3","title":"Setup","text":"<p>To use Hugging Face models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[huggingface]'\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#example-usage_3","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use <code>google/flan-t5-small</code> model for labeling via Hugging Face. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>huggingface_pipeline</code> and <code>name</code> is set to be <code>google/flan-t5-small</code>. <code>name</code> can be switched to use any model that satisfies the constraints above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"google/flan-t5-small\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters_3","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>huggingface_pipeline</code> models to control the model behavior:</p> <ul> <li><code>max_new_tokens</code> (int) - The maximum tokens to sample from the model</li> <li><code>temperature</code> (float) - A float b/w 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling.</li> <li><code>quantize</code> (int) - The model quantization to use. 32 bit by default, but we also support 16 bit and 8 bit support for models which have been hosted on Hugging Face.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"google/flan-t5-small\",\n\"params\": {\n\"max_new_tokens\": 512,\n\"temperature\": 0.1,\n\"quantize\": 8\n}\n},\n</code></pre> <p>To use Llama 2, you can use the following model configuration:</p> <pre><code>\"model\": {\n\"provider\": \"huggingface_pipeline\",\n\"name\": \"meta-llama/Llama-2-7b\",\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#google-gemini","title":"Google Gemini","text":"<p>To use models from Google, you can set the <code>provider</code> to <code>google</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Google:</p> <ul> <li><code>gemini-pro</code></li> </ul> <p><code>gemini-pro</code> is the most capable (and most expensive) from Google. Catch the announcement of the model here.</p>"},{"location":"autolabel/guide/llms/llms/#setup_4","title":"Setup","text":"<p>To use Google models with Autolabel, make sure to first install the relevant packages by running:</p> <p>and also setting the following environment variable:</p> <pre><code>export GOOGLE_API_KEY=&lt;your-google-key&gt;\n</code></pre> <p>replacing <code>&lt;your-google-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"autolabel/guide/llms/llms/#example-usage_4","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use google's <code>text-bison@001</code> model for labeling. Specifically, note that in the dictionary provided by the <code>model</code> tag, <code>provider</code> is set to <code>google</code> and <code>name</code> is set to be <code>text-bison@001</code>. <code>name</code> can be switched to use any of the two models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"google\",\n\"name\": \"gemini-pro\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters_4","title":"Additional parameters","text":"<p>A few parameters can be passed in alongside <code>google</code> models to tweak their behavior:</p> <ul> <li><code>max_output_tokens</code> (int): Maximum number of tokens that can be generated in the response.</li> <li><code>temperature</code> (float): A float between 0 and 1 which indicates the diversity you want in the output. 0 uses greedy sampling (picks the most likely outcome).</li> </ul> <p>All the parameters that can be passed are given here. These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"google\",\n\"name\": \"gemini-pro\",\n\"params\": {\n\"max_output_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#model-behavior","title":"Model behavior","text":"<p><code>chat-bison@001</code> always responds in a \"chatty\" manner (example below), often returning more than just the requested label. This can cause problems on certain labeling tasks.</p>"},{"location":"autolabel/guide/llms/llms/#content-moderation","title":"Content moderation","text":"<p>Both Google LLMs seem to have much stricter content moderation rules than the other supported models. This can cause certain labeling jobs to completely fail as shown in our technical report [add link to technical report]. Consider a different model if your dataset has content that is likely to trigger Google's built-in content moderation.</p>"},{"location":"autolabel/guide/llms/llms/#cohere","title":"Cohere","text":"<p>To use models from Cohere, you can set the <code>provider</code> to <code>cohere</code> when creating a labeling configuration. The specific model that will be queried can be specified using the <code>name</code> key. Autolabel currently supports the following models from Cohere:</p> <ul> <li><code>command</code> (4096 max tokens)</li> <li><code>command-light</code> (4096 max tokens)</li> <li><code>base</code> (2048 max tokens)</li> <li><code>base-light</code> (2048 max tokens)</li> </ul> <p><code>command</code> is an instruction-following conversational model that performs language tasks with high quality, while <code>command-light</code> is an almost as capable, but much faster option. <code>base</code> is a model that performs generative language tasks, while <code>base-light</code> much faster but a little less capable. All models cost the same at $15/1 million tokens. Detailed pricing for these models is available here.</p>"},{"location":"autolabel/guide/llms/llms/#setup_5","title":"Setup","text":"<p>To use Cohere models with Autolabel, make sure to first install the relevant packages by running:</p> <pre><code>pip install 'refuel-autolabel[cohere]'\n</code></pre> <p>and also setting the following environment variable:</p> <pre><code>export COHERE_API_KEY=&lt;your-cohere-key&gt;\n</code></pre> <p>replacing <code>&lt;your-cohere-key&gt;</code> with your API key, which you can get from here.</p>"},{"location":"autolabel/guide/llms/llms/#example-usage_5","title":"Example usage","text":"<p>Here is an example of setting config to a dictionary that will use cohere's <code>command</code> model for labeling. Specifically, note that in the dictionary proivded by the <code>model</code> tag, <code>provider</code> is set to <code>cohere</code> and <code>name</code> is set to be <code>command</code>. <code>name</code> can be switched to use any of the four models mentioned above.</p> <pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"cohere\",\n\"name\": \"command\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions.\",\n\"example_template\": \"Question: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#additional-parameters_5","title":"Additional parameters","text":"<p>A few parameters that can be passed in for <code>cohere</code> models to control the model behavior:</p> <ul> <li><code>max_tokens</code> (int): The maximum number of tokens to predict per generation</li> <li><code>temperature</code> (float): The degree of randomness in generations from 0.0 to 5.0, lower is less random.</li> </ul> <p>These parameters can be passed in via the <code>params</code> dictionary under <code>model</code>. Here is an example:</p> <pre><code>\"model\": {\n\"provider\": \"cohere\",\n\"name\": \"command\",\n\"params\": {\n\"max_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"autolabel/guide/llms/llms/#provider-list","title":"Provider List","text":"<p>The table lists out all the provider, model combinations that Autolabel supports today:</p> Provider Name openai text-davinci-003 openai gpt-3.5-turbo models openai gpt-4 models anthropic claude-instant-1.2 anthropic claude-2.0 anthropic claude-2.1 anthropic claude-opus-20240229 anthropic claude-sonnet-20240229 huggingface_pipeline seq2seq models and causalLM models refuel flan-t5-xxl google gemini-pro cohere command cohere command-light cohere base cohere base-light"},{"location":"autolabel/guide/overview/getting-started/","title":"Getting Started with Autolabel","text":"<p>This page will walk you through your very first labeling task using Refuel Autolabel. Specifically, it'll go over:</p> <ul> <li>Installation</li> <li>Overview of a dataset to label</li> <li>Labeling the dataset using Autolabel</li> </ul>"},{"location":"autolabel/guide/overview/getting-started/#installation","title":"Installation","text":"<p>Autolabel is available on PyPI and can be installed by running:</p> <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre> <p>Separate from the Autolabel library, you'll also need to install an integration with your favorite LLM provider. In the example below, we'll be using OpenAI, so you'll need to install the OpenAI SDK and set your API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\n</code></pre> <p>To use a different LLM provider, follow the documentation here.</p>"},{"location":"autolabel/guide/overview/getting-started/#goal-sentiment-analysis-on-a-movie-review-dataset","title":"Goal: Sentiment Analysis on a Movie Review Dataset","text":"<p>Let's say we wanted to run sentiment analysis on a dataset of movie reviews. We want to train our own ML model, but first, we need to label some data for training.</p> <p>Now, we could label a few hundred examples by hand which would take us a few hours. Instead, let's use Autolabel to get a clean, labeled dataset in a few minutes.</p> <p>A dataset<sup>1</sup> containing 200 unlabeled movie reviews is available here, and a couple of examples (with labels) are shown below:</p> text label I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it. negative I loved this movie. I knew it would be chocked full of camp and silliness like the original series. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar all back together once again. Anyone who loved the Batman series from the 60's should have enjoyed Return to the Batcave. You could tell the actors had a lot of fun making this film, especially Adam West. And I'll bet he would have gladly jumped back into his Batman costume had the script required him to do so. I told a number of friends about this movie who chose not to view it... now they wished they had. I have all of the original 120 episodes on VHS. Now this movie will join my collection. Thank You for the reunion Adam and Burt. positive <p>Our goal is to label the full 200 examples using Autolabel.</p>"},{"location":"autolabel/guide/overview/getting-started/#labeling-with-autolabel","title":"Labeling with AutoLabel","text":"<p>Autolabel provides a simple 3-step process for labeling data:</p> <ul> <li>Specify the configuration of your labeling task as a JSON</li> <li>Preview the labeling task against your dataset</li> <li>Label your data!</li> </ul>"},{"location":"autolabel/guide/overview/getting-started/#specify-the-labeling-task-via-configuration","title":"Specify the labeling task via configuration","text":"<p>First, create a JSON file that specifies:</p> <ul> <li>Task: <code>task_name</code> is <code>MovieSentimentReview</code> and the <code>task_type</code> is <code>classification</code></li> <li>LLM: Choice of LLM provider and model - here we are using <code>gpt-3.5-turbo</code> from OpenAI</li> <li>Instructions: These are the labeling guidelines provided to the LLM for labeling</li> </ul> <pre><code>config = {\n\"task_name\": \"MovieSentimentReview\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at analyzing the sentiment of movie reviews. Your job is to classify the provided movie review into one of the following labels: {labels}\",\n\"labels\": [\n\"positive\",\n\"negative\",\n\"neutral\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"I got a fairly uninspired stupid film about how human industry is bad for nature.\",\n\"label\": \"negative\"\n},\n{\n\"example\": \"I loved this movie. I found it very heart warming to see Adam West, Burt Ward, Frank Gorshin, and Julie Newmar together again.\",\n\"label\": \"positive\"\n},\n{\n\"example\": \"This movie will be played next week at the Chinese theater.\",\n\"label\": \"neutral\"\n}\n],\n\"example_template\": \"Example: {example}\\nLabel: {label}\"\n}\n}\n</code></pre> <p>To create a custom configuration, you can use the CLI or write your own.</p>"},{"location":"autolabel/guide/overview/getting-started/#preview-the-labeling-against-your-dataset","title":"Preview the labeling against your dataset","text":"<p>First import <code>autolabel</code>, create a <code>LabelingAgent</code> object and then run the <code>plan</code> command against the dataset (available here and can be downloaded through the <code>autolabel.get_data</code> function):</p> <pre><code>from autolabel import LabelingAgent, AutolabelDataset, get_data\nget_data('movie_reviews')\nagent = LabelingAgent(config)\nds = AutolabelDataset('test.csv', config = config)\nagent.plan(ds)\n</code></pre> <p>This produces:</p> <pre><code>Computing embeddings... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100/100 0:00:00 0:00:00\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $0.538  \u2502\n\u2502 Number of Examples       \u2502 200     \u2502\n\u2502 Average cost per example \u2502 0.00269 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nYou are an expert at analyzing the sentiment of moview reviews. Your job is to classify the provided movie review as positive or negative.\n\nYou will return the answer with just one element: \"the correct label\"\n\nNow I want you to label the following example:\nInput: I was very excited about seeing this film, anticipating a visual excursus on the relation of artistic beauty and nature, containing the kinds of wisdom the likes of \"Rivers and Tides.\" However, that's not what I received. Instead, I get a fairly uninspired film about how human industry is bad for nature. Which is clearly a quite unorthodox claim.&lt;br /&gt;&lt;br /&gt;The photographer seems conflicted about the aesthetic qualities of his images and the supposed \"ethical\" duty he has to the workers occasionally peopling the images, along the periphery. And frankly, the images were not generally that impressive. And according to this \"artist,\" scale is the basis for what makes something beautiful.&lt;br /&gt;&lt;br /&gt;In all respects, a stupid film. For people who'd like to feel better about their environmental consciousness ... but not for any one who would like to think about the complexities of the issues surrounding it.\nOutput:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre> <p>This shows you:</p> <ul> <li>Number of examples to be labeled in the dataset: <code>200</code></li> <li>Estimated cost of running this labeling task: <code>&lt;$1</code></li> <li>Exact prompt being sent to the LLM</li> </ul> <p>Having previewed the labeling, we are ready to start labeling.</p>"},{"location":"autolabel/guide/overview/getting-started/#label-your-dataset","title":"Label your dataset","text":"<p>Now, you can use the <code>run</code> command to label:</p> <pre><code>ds = AutolabelDataset('docs/assets/movie_reviews.csv', config = config)\nds = agent.run(ds)\n</code></pre> <p>This takes just a few minutes to run, and returns the labeled data as an Autolabel Dataset. We can explore this by running:</p> <pre><code>ds.df.head()\n&gt;\ntext  ... MovieSentimentReview_llm_label\n0  I was very excited about seeing this film, ant...  ...                       negative\n1  Serum is about a crazy doctor that finds a ser...  ...                       negative\n2  This movie was so very badly written. The char...  ...                       negative\n3  Hmmmm, want a little romance with your mystery...  ...                       negative\n4  I loved this movie. I knew it would be chocked...  ...                       positive\n[5 rows x 4 columns]\n</code></pre> <p>At this point, we have a labeled dataset ready, and we can begin training our ML models.</p>"},{"location":"autolabel/guide/overview/getting-started/#using-hugging-face-datasets","title":"Using Hugging Face datasets","text":"<p>If you want to use a Hugging Face dataset directly, you can pass it into <code>agent.plan</code> and <code>agent.run</code> as you would a file path or <code>pandas.DataFrame</code>.</p> <pre><code>dataset = load_dataset(DATASET_NAME)\nagent = LabelingAgent(config)\nagent.plan(test_dataset)\nagent.run(test_dataset)\n</code></pre>"},{"location":"autolabel/guide/overview/getting-started/#summary","title":"Summary","text":"<p>In this simple walkthrough, we have installed <code>autolabel</code>, gone over an example dataset to label (sentiment analysis for moview reviews) and used <code>autolabel</code> to label this dataset in just a few minutes.</p> <p>We hope that this gives you a glimpse of what you can do with Refuel. There are many other labeling tasks available within Autolabel, and if you have any questions, join our community here or open an issue on Github.</p> <ol> <li> <p>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\u00a0\u21a9</p> </li> </ol>"},{"location":"autolabel/guide/overview/tutorial-classification/","title":"Tutorial - Toxic comment classification","text":"<p>This is a detailed tutorial that walks you through many features of the Autolabel library while solving a problem faced by many companies - labeling toxic comments for content moderation. We will be using OpenAI's <code>gpt-3.5-turbo</code> for the data labeling, and Refuel's LLM for confidence estimation.</p> <p>If you want to run this code as you follow along, check out this Colab notebook: </p>"},{"location":"autolabel/guide/overview/tutorial-classification/#autolabel-installation","title":"Autolabel installation","text":"<p>Since we'll be using OpenAI along with Autolabel, we can install all necessary libraries by simply running: <pre><code>pip install 'refuel-autolabel[openai]'\n</code></pre></p> <p>Now, we can set our OpenAI key as an environment variable to get started. You can always use an LLM of your choice - see more optioons and installation instructions here. </p>"},{"location":"autolabel/guide/overview/tutorial-classification/#download-and-review-dataset","title":"Download and review dataset","text":"<p>We'll be using a dataset called Civil Comments, which is available through Autolabel. You can download it locally, by simply running: <pre><code>from autolabel import get_data\nget_data('civil_comments')\n</code></pre></p> <p>The output is: <pre><code>Downloading seed example dataset to \"data/civil-comments/seed.csv\"...\n100% [..............................................................................] 65757 / 65757\nDownloading test dataset to \"data/civil-comments/test.csv\"...\n100% [............................................................................] 610663 / 610663\n</code></pre></p> <p>This results in two files being downloaded locally:</p> <ul> <li><code>seed.csv</code>: small dataset with labels that we'll rely on as helpful examples.</li> <li><code>test.csv</code>: larger dataset that we are trying to label.</li> </ul> <p>A few examples are shown below:</p> label examples <code>toxic</code> \"The ignorance and bigotry comes from your post!\" <code>not toxic</code> \"This is malfeasance by the Administrator and the Board. They are wasting our money!\""},{"location":"autolabel/guide/overview/tutorial-classification/#start-the-labeling-process","title":"Start the labeling process","text":"<p>Labeling with Autolabel is a 3-step process:</p> <ul> <li>First, we specify a labeling configuration (see <code>config</code> object below) and create a <code>LabelingAgent</code></li> <li>Next, we do a dry-run on our dataset using the LLM specified in <code>config</code> by running <code>agent.plan</code></li> <li>Finally, we run the labeling with <code>agent.run</code></li> </ul>"},{"location":"autolabel/guide/overview/tutorial-classification/#experiment-1-try-simple-labeling-guidelines","title":"Experiment #1: Try simple labeling guidelines","text":"<p>Define the configuration file below: <pre><code>config = {\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\", # classification task\n\"dataset\": {\n\"label_column\": \"label\",\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\" # the model we want to use\n},\n\"prompt\": {\n# very simple instructions for the LLM\n\"task_guidelines\": \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n\"labels\": [ # list of labels to choose from\n\"toxic\",\n\"not toxic\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> To create a custom configuration, you can use the CLI or write your own.</p> <p>Now, we do the dry-run with <code>agent.plan</code>: <pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/civil-comments/test.csv', config = config)\nagent.plan(ds)\n</code></pre></p> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.4442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0022 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\nYou will return the answer with just one element: \"the correct label\"\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.]. Does this apply to President Trump too?\nOutput: \n</code></pre></p> <p>Finally, we run the data labeling: <pre><code>ds = agent.run(ds, max_items=100)\n</code></pre></p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.54     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>54% accuracy is not very good! Let's see if we can improve this further!</p>"},{"location":"autolabel/guide/overview/tutorial-classification/#experiment-2-few-shot-prompting-to-provide-helpful-examples","title":"Experiment #2: Few-shot prompting to provide helpful examples","text":"<p>Similar to how human labelers find it helpful to use relevant examples when making a decision, LLM performance for labeling also goes up when choosing helpful examples in the prompt. For this next experiment, we will pick a few helpful examples from <code>seed.csv</code>. More information on few-shot prompting can be found here.</p> <p>We take the previous config, and just update the following fields: <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\":  \"Does the provided comment contain 'toxic' language? Say toxic or not toxic.\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre></p> <p>That's it! We are now ready to create a <code>LabelingAgent</code> and run the same <code>agent.plan</code> and <code>agent.run</code> commands.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Total Estimated Cost     \u2502 $4.9442 \u2502\n\u2502 Number of Examples       \u2502 2000    \u2502\n\u2502 Average cost per example \u2502 $0.0025 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Prompt Example \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDoes the provided comment contain 'toxic' language? Say toxic or not toxic.\nYou will return the answer with just one element: \"the correct label\"\nSome examples with their output answers are provided below:\nInput: It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\nOutput: toxic\nInput: This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\nOutput: not toxic\nInput: This bitch is nuts. Who would read a book by a woman\nOutput: toxic\nInput: It was a great show. Not a combo I'd of expected to be good together but it was.\nOutput: not toxic\nNow I want you to label the following example:\nInput: [ Integrity means that you pay your debts.] Does this apply to President Trump too?\nOutput:\n</code></pre> <p>With additional examples, the cost has gone up slightly. Now, we run the labeling with:</p> <pre><code>labels, df, metrics = agent.run(ds, max_items=100)\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.68     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nice! We improved performance from 54% to 68% by providing a few examples to the LLM.</p>"},{"location":"autolabel/guide/overview/tutorial-classification/#experiment-3-improving-task-guidelines-after-reviewing-errors-prompt-engineering","title":"Experiment #3: Improving task guidelines after reviewing errors (prompt engineering)","text":"<p>Typically, you can improve the accuracy by reviewing mistakes and updating the task guidelines (see another example here). You can review some of the mistakes from the previous run by looking at the output Pandas DataFrame produced called <code>df</code>: <pre><code>df[df['label'] != df['ToxicCommentClassification_llm_label']].head(10)\n</code></pre></p> <p>Let's say we update our task guidelines to be more explicit about how should the LLM make the decision about whether a comment is toxic or not:</p> <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>Now, when we run <code>agent.run</code>, we get the following results:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We now hit an accuracy of 78%, which is very promising! If we spend more time improving the guidelines or choosing different examples, we can push accuracy even further.</p>"},{"location":"autolabel/guide/overview/tutorial-classification/#experiment-4-experimenting-with-llms","title":"Experiment #4: Experimenting with LLMs","text":"<p>We've iterated a fair bit on prompts, and few-shot examples. Let's evaluate a few different LLMs provided by the library out of the box. For example, we observe that we can boost performance even further by using <code>text-davinci-003</code>: </p> <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"text-davinci-003\",\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"few_shot_examples\": [\n{\n\"example\": \"It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists.\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\",\n\"label\": \"not toxic\"\n},\n{\n\"example\": \"This bitch is nuts. Who would read a book by a woman\",\n\"label\": \"toxic\"\n},\n{\n\"example\": \"It was a great show. Not a combo I'd of expected to be good together but it was.\",\n\"label\": \"not toxic\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 4,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>While the per token API price for this model is higher, we're able to boost the accuracy to 88%!</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.88     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"autolabel/guide/overview/tutorial-classification/#experiment-5-using-confidence-scores","title":"Experiment #5: Using confidence scores","text":"<p>Refuel provides LLMs that can compute confidence scores for every label, if the LLM you've chosen doesn't provide token-level log probabilities. This is helpful, because you can calibrate a confidence threshold for your labeling task, and then route less confident labels to humans, while you still get the benefits of auto-labeling for the confident examples. Let's see how this works. </p> <p>First, set your Refuel API key as an environment variable (and if you don't have this key yet, sign up here). <pre><code>os.environ['REFUEL_API_KEY'] = '&lt;your-api-key&gt;'\n</code></pre></p> <p>Now, update your configuration: <pre><code>config[\"model\"][\"compute_confidence\"] = True\n</code></pre></p> <p>Finally, let's run <code>agent.run</code> as before - this produces the table below: <pre><code>Metric: auroc: 0.8858\nActual Cost: 0.0376\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.78     \u2502 1.0             \u2502\n\u2502 1       \u2502 0.9988    \u2502 1.0      \u2502 0.01            \u2502\n\u2502 12      \u2502 0.9957    \u2502 1.0      \u2502 0.12            \u2502\n\u2502 13      \u2502 0.9949    \u2502 0.9231   \u2502 0.13            \u2502\n\u2502 54      \u2502 0.9128    \u2502 0.9815   \u2502 0.54            \u2502\n\u2502 55      \u2502 0.9107    \u2502 0.9636   \u2502 0.55            \u2502\n\u2502 63      \u2502 0.6682    \u2502 0.9683   \u2502 0.63            \u2502\n\u2502 66      \u2502 0.6674    \u2502 0.9242   \u2502 0.66            \u2502\n\u2502 67      \u2502 0.6673    \u2502 0.9254   \u2502 0.67            \u2502\n\u2502 69      \u2502 0.6671    \u2502 0.8986   \u2502 0.69            \u2502\n\u2502 71      \u2502 0.6667    \u2502 0.9014   \u2502 0.71            \u2502\n\u2502 72      \u2502 0.6667    \u2502 0.8889   \u2502 0.72            \u2502\n\u2502 78      \u2502 0.4819    \u2502 0.8974   \u2502 0.78            \u2502\n\u2502 79      \u2502 0.4774    \u2502 0.8861   \u2502 0.79            \u2502\n\u2502 87      \u2502 0.4423    \u2502 0.8966   \u2502 0.87            \u2502\n\u2502 100     \u2502 0.0402    \u2502 0.78     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>The rows in this table show labeling performance at different confidence thresholds, and set an autolabeling confidence threshold at the desired accuracy. For instance, from the table above we can set the confidence threshold at 0.6682 which allows us to label at 96% accuracy with a completion rate of 63%.</p> <p>If you want to run this code as you follow along, check out this Colab notebook: </p>"},{"location":"autolabel/guide/overview/tutorial-classification/#final-thoughts","title":"Final thoughts","text":"<p>Hopefully, this tutorial was helpful in understanding how Autolabel can help you label datasets quickly and at high quality. A Jupyter notebook for this tutorial can be found here.</p> <p>You can find more example notebooks here, including for tasks such as question answering, named entity recognition, etc. </p> <p>Drop us a message in our Discord if you want to chat with us, or go to Github to report any issues!</p>"},{"location":"autolabel/guide/reliability/llm-output-caching/","title":"LLM Output Caching","text":"<p>To help reduce time and cost when iterating the prompt for better labeling accuracy, we cache the calls made to the LLM.</p>"},{"location":"autolabel/guide/reliability/llm-output-caching/#cache-entry","title":"Cache Entry","text":"<p>A cache entry has the following attributes:</p> <ul> <li><code>Model Name</code></li> <li><code>Prompt</code></li> <li><code>Model Params</code></li> </ul> <p>This means that anytime there are changes to either the language model or the prompt, the model will be called for producing label. Also, changes to the model parameters like the <code>max_tokens</code> or <code>temperature</code> could affect the label output and therefore modifying such parameters result in new calls to the model instead of using cached calls.</p>"},{"location":"autolabel/guide/reliability/llm-output-caching/#caching-storage","title":"Caching Storage","text":"<p>The cached entries are stored in a SQLite database. We will be adding support for In Memory cache and Redis cache in future.</p>"},{"location":"autolabel/guide/reliability/llm-output-caching/#disable-caching","title":"Disable Caching","text":"<p>The cache is enabled by default and if you wish to disable it, you can set <code>cache=False</code> when initializing the LabelingAgent.</p> <pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config='examples/configs/civil_comments.json', cache=False)\n</code></pre>"},{"location":"autolabel/guide/reliability/state-management/","title":"State Management","text":"<p>Labeling a large dataset can take some time and if you're running the task on a Jupyter notebook and your machine decides to sleep during the time, it could be really frustrating. (we've been there! ).</p> <p>Therefore, we periodically save the progress of the labeling task in a SQLite database, so if the task is interrupted, you can resume it from where you left off.</p>"},{"location":"autolabel/guide/reliability/state-management/#task-run-state","title":"Task Run State","text":"<p>When a labeling task is triggered, a task run entry gets initialized inside the database. We maintain the dataset index till where the labels have been computed. After every small chunk (size 5) of data gets labeled, the dataset index gets updated and the labels are persisted.</p> <p>In case the labeling process get interrupted/terminated and you trigger the task with the same parameters again, the library first checks for a previous instance of the same task.</p> <p>If there was an incomplete task present, you would be prompted with details of the previous run and asked to resume the task. If you choose to resume the previous task, it gets loaded into the memory and resumed from previous state otherwise the previous entry gets deleted.</p>"},{"location":"autolabel/guide/reliability/state-management/#deep-dive","title":"Deep Dive","text":"<p>You'd likely never have to interact with the database directly but in case you wish to look at the state of the database, you can do that using any CLI or GUI that supports SQL. The database is saved in the same directory from where you run the LabelingAgent notebook and is named <code>.autolabel.db</code>.</p> <p>We have the following tables:</p> <ul> <li><code>datasets</code>: Stores the dataset file information</li> <li><code>tasks</code>: Stores the labeling task attributes</li> <li><code>task_runs</code>: Stores the current state of a labeling task run</li> <li><code>annotations</code>: Stores the LLM annotation corresponding to the task run</li> <li><code>generation_cache</code>: Cache for the LLM calls</li> </ul>"},{"location":"autolabel/guide/resources/CLI/","title":"CLI","text":"<p>The Autolabel CLI was created to make the config file creation process easier. It is a simple command line interface that will ask you a series of questions and then generate a config file for you. To use it, simply run the following command:</p> <pre><code>autolabel config\n</code></pre>"},{"location":"autolabel/guide/resources/CLI/#walkthrough-creating-a-config-for-civil-comments","title":"Walkthrough: Creating a Config for Civil Comments","text":"<ol> <li> The first step is to run the <code>autolabel</code> command with the <code>config</code> argument:  <pre><code>autolabel config\n</code></pre> </li> <li> The program will prompt you to enter the task name.  <pre><code>Enter the task name: ToxicCommentClassification\n</code></pre> </li> <li> Next, you need to choose the task type from the provided options.:  <pre><code>Choose a task type\n&gt; classification\n  named_entity_recognition\n  question_answering\n  entity_matching\n  multilabel_classification\n</code></pre> </li> <li> Now, the program will ask for dataset configuration details. You need to specify the delimiter used in your dataset, the label column name, and an optional explanation column name:  <pre><code>Dataset Configuration\nEnter the delimiter (,):\nEnter the label column name: label\nEnter the explanation column name (optional):\n</code></pre> Anything surrounded by parenthesis at the end of a prompt will be used as the default value if you don't input anything. Make sure to change this if it does not line up with your task. </li> <li> The program will then ask for model configuration. You will need to specify the model provider from the options. Next, enter the model name, optional model parameters, whether the model should compute confidence, and the strength of the logit bias:  <pre><code>Model Configuration\nEnter the model provider\n&gt; openai\n  anthropic\n  huggingface_pipeline\n  refuel\n  google\n  cohere\nEnter the model name: gpt-3.5-turbo\nEnter a model parameter name (or leave blank for none):\nShould the model compute confidence? [y/n] (n):\nWhat is the strength of logit bias? (0.0): 100\n</code></pre> </li> <li> Next, you will configure the task prompt. First, enter the task guidelines. In the task guidelines, <code>{num_labels}</code> and <code>{labels}</code> will be replaced by the number of labels and the labels list respectively. Next, specify the labels. Then, write the example template with placeholders for the column names you want to use in the prompt. You can also add an output guideline and format if needed. Lastly, you can choose whether to use a chain of thought:  <pre><code>Prompt Configuration\nEnter the task guidelines (Your job is to correctly label the provided input example into one of the following {num_labels} categories.\nCategories:\n{labels}\n):\nEnter a valid label (or leave blank for none): toxic\nEnter a valid label (or leave blank to finish): not toxic\nEnter a valid label (or leave blank to finish):\nEnter the example template: Example: {example}\\nLabel: {label}\nEnter the value for example (or leave blank for none):\nEnter the output guideline (optional):\nEnter the output format (optional):\nShould the prompt use a chain of thought? [y/n] (n):\n</code></pre> </li> <li> The program will then display the configuration that you have provided as a python dictionary:  <pre><code>{\n'task_name': 'ToxicCommentClassification',\n'task_type': 'classification',\n'dataset': {'delimiter': ',', 'label_column': 'label'},\n'model': {'provider': 'openai', 'name': 'gpt-3.5-turbo', 'compute_confidence': False, 'logit_bias': 100.0},\n'prompt': {\n'task_guidelines': 'Your job is to correctly label the provided input example into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n',\n'labels': ['toxic', 'not toxic'],\n'example_template': 'Example: {example}\\nLabel: {label}',\n'chain_of_thought': False\n}\n}\n</code></pre> </li> <li> Finally, the program will write the configuration to a file named \"{your_task_name}_config.json\".  <pre><code>Writing config to ToxicCommentClassification_config.json\n</code></pre> </li> </ol> <p>That's it! You have successfully created a config for a task using the CLI program. The generated configuration file can now be used for any labeling runs with autolabel!</p>"},{"location":"autolabel/guide/resources/CLI/#providing-a-seed-file","title":"Providing a seed file","text":"<p>You can provide a seed file to the CLI to help it generate the config file. Providing a seed file to the CLI allows it to automatically provide drop-down menus for column name inputs, detect labels that are already present in the seed file, and fill the few shot examples by row number in the seed file. To do this, simply run the following command:</p> <pre><code>autolabel config &lt;path-to-seed-file&gt;\n</code></pre> <p>For example, if you have a file called <code>seed.csv</code> in the current directory, you would run the following command:</p> <pre><code>autolabel config seed.csv\n</code></pre> <p>Here's an example of what the prompt configuration section would look like with a seed file:</p> <pre><code>Detected 2 unique labels in seed dataset. Use these labels? [y/n]: y\nEnter the example template: Example: {example}\\nLabel: {label}\nUse seed.csv as few shot example dataset? [y/n]: n\nEnter the value for example or row number (or leave blank for none): 3\n{'example': \"When all else fails, change the subject to Hillary's emails.\", 'label': 'not toxic'}\nEnter the value for example or row number (or leave blank to finish): 7\n{\n    'example': 'He may like the internal forum, but the reality is he has affirmed traditional doctrine and practices. While he does like the internal forum he has not changed anything.',\n    'label': 'not toxic'\n}\nEnter the value for example or row number (or leave blank to finish): 24\n{'example': '........... said the blind dumb and deaf lemming.', 'label': 'toxic'}\nEnter the value for example or row number (or leave blank to finish): 64\n{\n    'example': 'Do you have a citation for that statement or did you just make it up yourself? BTW, this thread is about the unhealthy liar the Democrats have\nnominated.',\n    'label': 'not toxic'\n}\nEnter the value for example or row number (or leave blank to finish):\nEnter the few shot selection algorithm\n&gt; fixed\n  semantic_similarity\n  max_marginal_relevance\n  label_diversity_random\n  label_diversity_similarity\nEnter the number of few shot examples to use (4):\n</code></pre> <p>As you can see, the CLI automatically detected the labels in the seed file and used them to generate the labels list. It also automatically filled the few shot examples with the examples from the seed file after letting the user choose the rows to use.</p>"},{"location":"autolabel/guide/resources/CLI/#specifying-model-parameters","title":"Specifying Model Parameters","text":"<p>To specify model parameters, you can simply enter the parameter name and value when prompted. For example, if you wanted to specify the <code>temperature</code> parameter for the <code>gpt-3.5-turbo</code> model, you would run the following command:</p> <pre><code>Enter a model parameter name (or leave blank for none): temperature\nEnter the value for max_tokens: 0.5\n</code></pre>"},{"location":"autolabel/guide/resources/CLI/#providing-few-shot-examples","title":"Providing Few Shot Examples","text":"<p>To provide few shot examples, you can simply input the example when prompted (after entering the example template). The CLI will go through the example template and ask for any values specified in that. For example, if you template is <code>Example: {example}\\nLabel: {label}</code>, you could add a few shot example as shown below:</p> <pre><code>Enter the example template: Example: {example}\\nLabel: {label}\nEnter the value for example (or leave blank for none): You're ugly and dumb\nEnter the value for label: toxic\nEnter the value for example (or leave blank to finish): I love your art!\nEnter the value for label: not toxic\nEnter the value for example (or leave blank to finish): It was a great show. Not a combo I'd of expected to be good together but it was.\nEnter the value for label: not toxic\nEnter the value for example (or leave blank to finish): It's ridiculous that these guys are being called 'protesters'. Being armed is a threat of violence, which makes them terrorists\nEnter the value for label: toxic\nEnter the value for example (or leave blank to finish):\nEnter the few shot selection algorithm\n&gt; fixed\n  semantic_similarity\n  max_marginal_relevance\n  label_diversity_random\n  label_diversity_similarity\nEnter the number of few shot examples to use (4):\n</code></pre> <p>Since we only added 4 examples, we chose the <code>fixed</code> few shot selection algorithm and left the number of few shot examples to use at 4 since we want to use all of them in every prompt.</p>"},{"location":"autolabel/guide/resources/CLI/#the-init-command","title":"The <code>init</code> command","text":"<p>If you would prefer to edit a json file directly, you can use the <code>init</code> command to generate a config file for you. To do this, simply run the following command:</p> <pre><code>autolabel init\n</code></pre> <p>By default, this will create a config file that looks like the one below:</p> <pre><code>{\n\"task_name\": \"[TODO] Enter task name\",\n\"task_type\": \"[TODO] Enter task type\",\n\"dataset\": {\n\"delimiter\": \"[TODO] Enter delimiter\",\n\"label_column\": \"[TODO] Enter label column name\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"[TODO] Enter task guidelines\",\n\"example_template\": \"[TODO] Enter example template\",\n\"few_shot_examples\": \"[TODO] Enter few shot examples\",\n\"few_shot_selection\": \"[TODO] Enter few shot selection\",\n\"few_shot_num\": \"[TODO] Enter few shot num\"\n}\n}\n</code></pre> <p><code>init</code> will also take a seed file as an argument. Combined with other options, this can result in a very quick config file generation process. For example, if you have a file called <code>seed.csv</code> in the current directory, you could run the following command:</p> <pre><code>autolabel init seed.csv --task-name ToxicCommentClassification --task-type classification --delimiter , --label-column label --task-guidelines \"You are an expert at identifying toxic comments.\" --example-template \"Example: {example}\\nLabel: {label}\" --few-shot-examples seed.csv --few-shot-selection semantic_similarity --few-shot-num 5 --guess-labels\n</code></pre> <p>Resulting in the following config file for the civil comments dataset:</p> <pre><code>{\n\"task_name\": \"ToxicCommentClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"delimiter\": \",\",\n\"label_column\": \"label\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments.\",\n\"example_template\": \"Example: {example}\\nLabel: {label}\",\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5,\n\"labels\": [\"not toxic\", \"toxic\"]\n}\n}\n</code></pre>"},{"location":"autolabel/guide/resources/CLI/#the-plan-command","title":"The <code>plan</code> command","text":"<p>The <code>plan</code> command works identically to running <code>LabelingAgent({config}).plan({dataset})</code> in python. To use it, simply run the following command:</p> <pre><code>autolabel plan &lt;path-to-dataset&gt; &lt;path-to-config&gt;\n</code></pre>"},{"location":"autolabel/guide/resources/CLI/#the-run-command","title":"The <code>run</code> command","text":"<p>The <code>run</code> command works identically to running <code>LabelingAgent({config}).run({dataset})</code> in python. To use it, simply run the following command:</p> <pre><code>autolabel run &lt;path-to-dataset&gt; &lt;path-to-config&gt;\n</code></pre>"},{"location":"autolabel/guide/resources/CLI/#help","title":"Help","text":"<p>If any of the commands are unclear, you can run <code>autolabel --help</code> to see the help menu or <code>autolabel &lt;command&gt; --help</code> to see the help menu for a specific command.</p>"},{"location":"autolabel/guide/resources/autolabel_dataset/","title":"AutolabelDataset","text":""},{"location":"autolabel/guide/resources/autolabel_dataset/#autolabel-dataset","title":"Autolabel Dataset","text":"<p>Autolabel interacts primarily with dataset objects. These dataset objects are the input and the output for every agent function. <code>agent.run</code>, <code>agent.plan</code> and <code>agent.transform</code> all accept AutolabelDataset as an input and output an Autolabel Dataset. Use this object to talk to autolabel and run evaluations, transformations as well as understand the labels that a model outputs. We provide utility functions to help with understanding where the labeling process can be improved.</p> <p>The dataset for handling all operations on the dataset.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>class AutolabelDataset:\n\"\"\"The dataset for handling all operations on the dataset.\"\"\"\ninputs: List[Dict]\ndf: pd.DataFrame\ngt_labels: List\nconfig: AutolabelConfig\nclass Config:\narbitrary_types_allowed = True\ndef __init__(\nself,\ndataset: Union[pd.DataFrame, str],\nconfig: Union[AutolabelConfig, str, Dict],\nmax_items: int = None,\nstart_index: int = 0,\nvalidate: bool = False,\n) -&gt; None:\n\"\"\"\n        Initializes the dataset.\n        Args:\n            dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.\n            config: The config to be used for labeling. Could be a path to a json file or a dictionary.\n            max_items: The maximum number of items to be parsed into the dataset object.\n            start_index: The index to start parsing the dataset from.\n            validate: Whether to validate the dataset or not.\n        \"\"\"\nif not (isinstance(config, AutolabelConfig)):\nself.config = AutolabelConfig(config)\nelse:\nself.config = config\nif isinstance(dataset, str):\nif dataset.endswith(\".csv\"):\ndelimiter = self.config.delimiter()\nquoting = 0\nif self.config.disable_quoting():\nquoting = 3\ndf = pd.read_csv(dataset, sep=delimiter, dtype=\"str\", quoting=quoting)\nelif dataset.endswith(\".jsonl\"):\ndf = pd.read_json(dataset, lines=True, dtype=\"str\")\nelif isinstance(dataset, pd.DataFrame):\ndf = dataset.copy()\ndf = df[start_index:]\nif max_items and max_items &gt; 0:\nmax_items = min(max_items, len(df))\ndf = df[:max_items]\ninputs = df.to_dict(orient=\"records\")\nlabel_column = self.config.label_column()\nif not self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\ngt_labels = (\nNone\nif not label_column or not len(inputs) or label_column not in inputs[0]\nelse df[label_column].tolist()\n)\nelse:\ngt_labels = {}\nfor attr in self.config.attributes():\nname = attr[\"name\"]\ncolumn_name = attr[\"label_column\"] if \"label_column\" in attr else name\ngt_labels[name] = (\ndf[column_name].tolist() if column_name in df.keys() else None\n)\nself.df = df\nself.inputs = inputs\nself.gt_labels = gt_labels\nif validate:\nself._validate()\ndef __repr__(self):\n\"\"\"\n        Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.\n        \"\"\"\nif self.df is not None:\nreturn self.df.__repr__()\ndef __str__(self):\nif self.df is not None:\nreturn self.df.__str__()\ndef get_slice(self, max_items: int = None, start_index: int = 0):\ndf = self.df[start_index:]\nif max_items and max_items &gt; 0:\nmax_items = min(max_items, len(df))\ndf = df[:max_items]\nreturn AutolabelDataset(df, self.config)\ndef process_labels(\nself, llm_labels: List[LLMAnnotation], metrics: List[MetricResult] = None\n):\n# Add the LLM labels to the dataframe\nself.df[self.generate_label_name(\"label\")] = [x.label for x in llm_labels]\nif self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\nfor attr in self.config.attributes():\nattribute_labels = []\nfor x in llm_labels:\nif x.successfully_labeled:\nattribute_labels.append(x.label.get(attr[\"name\"], \"\"))\nelse:\nattribute_labels.append(BaseTask.NULL_LABEL_TOKEN)\nself.df[\nself.generate_label_name(\"label\", attr[\"name\"])\n] = attribute_labels\n# Add the LLM errors to the dataframe\nself.df[self.generate_label_name(\"error\")] = [x.error for x in llm_labels]\n# Add the LLM prompts to the dataframe\nself.df[self.generate_label_name(\"prompt\")] = [x.prompt for x in llm_labels]\n# Add labeled success column to the dataframe\nself.df[self.generate_label_name(\"successfully_labeled\")] = [\nx.successfully_labeled for x in llm_labels\n]\n# Add the LLM annotations to the dataframe\nself.df[self.generate_label_name(\"annotation\")] = llm_labels\n# Add row level LLM metrics to the dataframe\nif metrics is not None:\nfor metric in metrics:\nif (\nisinstance(metric.value, list)\nand len(metric.value) == self.df.shape[0]\n):\nself.df[self.generate_label_name(metric.name)] = metric.value\n# Add the LLM confidence scores to the dataframe if confidence is set in config\nif self.config.confidence():\nself.df[self.generate_label_name(\"confidence\")] = [\nx.confidence_score for x in llm_labels\n]\nif self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\nfor attr in self.config.attributes():\nattr_confidence_scores = []\nfor x in llm_labels:\nif x.successfully_labeled:\nattr_confidence_scores.append(\nx.confidence_score.get(attr[\"name\"], 0.0)\n)\nelse:\nattr_confidence_scores.append(0.0)\nself.df[\nself.generate_label_name(\"confidence\", attr[\"name\"])\n] = attr_confidence_scores\n# Add the LLM explanations to the dataframe if chain of thought is set in config\nif self.config.chain_of_thought():\nself.df[self.generate_label_name(\"explanation\")] = [\nl.explanation for l in llm_labels\n]\ndef save(self, output_file_name: str):\n\"\"\"\n        Saves the dataset to a file based on the file extension.\n        Args:\n            output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.\n        \"\"\"\nif output_file_name.endswith(\".csv\"):\nself.df.to_csv(\nstr(output_file_name),\nsep=self.config.delimiter(),\nheader=True,\nindex=False,\n)\nelif output_file_name.endswith(\".jsonl\"):\nself.df.to_json(\nstr(output_file_name),\norient=\"records\",\nlines=True,\nforce_ascii=False,\n)\nelse:\nraise ValueError(f\"Unsupported output file format: {output_file_name}\")\ndef filter(\nself,\nlabel: str = None,\nground_truth: str = None,\nfilter_func: Callable = None,\nlabel_column: str = None,\n):\n\"\"\"\n        Filter the dataset based on the label, ground truth or a custom filter function.\n        In case multiple filters are applied, the filters are applied in the following order:\n            label -&gt; ground_truth -&gt; filter_func\n        Args:\n            label: The llm label to filter on.\n            ground_truth: The ground truth label to filter on.\n            filter_func: A custom filter function to filter on.\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\nfiltered_df = self.df\nif label:\nfiltered_df = filtered_df[\nfiltered_df[self.generate_label_name(\"label\", label_column)] == label\n]\nif ground_truth:\nfiltered_df = filtered_df[\nfiltered_df[(label_column or self.config.label_column())]\n== ground_truth\n]\nif filter_func:\nfiltered_df = filtered_df.apply(filter_func, axis=1)\nreturn AutolabelDataset(\nfiltered_df,\nself.config,\n)\ndef non_completed(self):\n\"\"\"\n        Filter the dataset to only include non completed items. This means the labels\n        where the llm was not able to generate a label or there was some error while\n        generating the label.\n        \"\"\"\nfiltered_df = self.df[self.df[self.generate_label_name(\"error\")].notnull()]\nreturn AutolabelDataset(filtered_df, self.config)\ndef completed(self):\n\"\"\"\n        Filter the dataset to only include completed items. This means the labels\n        where the llm was able to generate a label successfully.\n        \"\"\"\nfiltered_df = self.df[self.df[self.generate_label_name(\"error\")].isnull()]\nreturn AutolabelDataset(filtered_df, self.config)\ndef incorrect(\nself, label: str = None, ground_truth: str = None, label_column: str = None\n):\n\"\"\"\n        Filter the dataset to only include incorrect items. This means the labels\n        where the llm label was incorrect.\n        Args:\n            label: The llm label to filter on.\n            ground_truth: The ground truth label to filter on.\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\ngt_label_column = label_column or self.config.label_column()\nif gt_label_column is None:\nraise ValueError(\n\"Cannot compute mistakes without ground truth label column\"\n)\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"label\", label_column)]\n!= self.df[gt_label_column]\n]\nif label:\nfiltered_df = filtered_df[\nfiltered_df[self.generate_label_name(\"label\", label_column)] == label\n]\nif ground_truth:\nfiltered_df = filtered_df[filtered_df[gt_label_column] == ground_truth]\nreturn AutolabelDataset(filtered_df, self.config)\ndef correct(self, label_column: str = None):\n\"\"\"\n        Filter the dataset to only include correct items. This means the labels\n        where the llm label was correct.\n        Args:\n            label_column: The column to filter on. This is only used for attribute extraction tasks.\n        \"\"\"\ngt_label_column = label_column or self.config.label_column()\nif gt_label_column is None:\nraise ValueError(\"Cannot compute correct without ground truth label column\")\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"label\", label_column)]\n== self.df[gt_label_column]\n]\nreturn AutolabelDataset(filtered_df, self.config)\ndef filter_by_confidence(self, threshold: float = 0.5):\n\"\"\"\n        Filter the dataset to only include items with confidence scores greater than the threshold.\n        Args:\n            threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.\n        \"\"\"\nif not self.config.confidence():\nraise ValueError(\n\"Cannot compute correct and confident without confidence scores\"\n)\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"confidence\")] &gt;= threshold\n]\nreturn AutolabelDataset(filtered_df, self.config)\ndef eval(self):\n\"\"\"\n        Evaluate the dataset based on the task. We run the metrics that were\n        specified by the task being run.\n        \"\"\"\nllm_labels = self.df[self.generate_label_name(\"annotation\")].tolist()\ntask = TaskFactory.from_config(self.config)\nmetrics = task.eval(llm_labels, self.gt_labels)\ntable = {}\nfor metric in metrics:\nif not isinstance(metric.value, list):\ntable[metric.name] = metric.value\nprint_table(table, console=Console(), default_style=METRIC_TABLE_STYLE)\nreturn metrics\ndef columns(self):\n\"\"\"\n        Returns the columns in the dataframe.\n        \"\"\"\nreturn self.df.columns.tolist()\ndef _validate(self):\n\"\"\"\n        Validate the dataset by looking at all rows and making sure\n        that they follow the schema.\n        \"\"\"\ndata_validation = TaskDataValidation(config=self.config)\n# Validate columns\ndata_validation.validate_dataset_columns(dataset_columns=self.columns())\n# Validate datatype and data format\nself.__malformed_records = data_validation.validate(data=self.inputs)\ntable = tabulate(\nself.__malformed_records[0 : self.MAX_ERROR_DISPLAYED],\nheaders=\"keys\",\ntablefmt=\"fancy_grid\",\nnumalign=\"center\",\nstralign=\"left\",\n)\nif len(self.__malformed_records) &gt; 0:\nlogger.warning(\nf\"Data Validation failed for {len(self.__malformed_records)} records: \\n Stats: \\n {table}\"\n)\nraise DataValidationFailed(\nf\"Validation failed for {len(self.__malformed_records)} rows.\"\n)\ndef generate_label_name(self, col_name: str, label_column: str = None):\nlabel_column = label_column or f\"{self.config.task_name()}_task\"\nreturn f\"{label_column}_{col_name}\"\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.__init__","title":"<code>__init__(dataset, config, max_items=None, start_index=0, validate=False)</code>","text":"<p>Initializes the dataset. Args:     dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.     config: The config to be used for labeling. Could be a path to a json file or a dictionary.     max_items: The maximum number of items to be parsed into the dataset object.     start_index: The index to start parsing the dataset from.     validate: Whether to validate the dataset or not.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def __init__(\nself,\ndataset: Union[pd.DataFrame, str],\nconfig: Union[AutolabelConfig, str, Dict],\nmax_items: int = None,\nstart_index: int = 0,\nvalidate: bool = False,\n) -&gt; None:\n\"\"\"\n    Initializes the dataset.\n    Args:\n        dataset: The dataset to be used for labeling. Could be a path to a csv/jsonl file or a pandas dataframe.\n        config: The config to be used for labeling. Could be a path to a json file or a dictionary.\n        max_items: The maximum number of items to be parsed into the dataset object.\n        start_index: The index to start parsing the dataset from.\n        validate: Whether to validate the dataset or not.\n    \"\"\"\nif not (isinstance(config, AutolabelConfig)):\nself.config = AutolabelConfig(config)\nelse:\nself.config = config\nif isinstance(dataset, str):\nif dataset.endswith(\".csv\"):\ndelimiter = self.config.delimiter()\nquoting = 0\nif self.config.disable_quoting():\nquoting = 3\ndf = pd.read_csv(dataset, sep=delimiter, dtype=\"str\", quoting=quoting)\nelif dataset.endswith(\".jsonl\"):\ndf = pd.read_json(dataset, lines=True, dtype=\"str\")\nelif isinstance(dataset, pd.DataFrame):\ndf = dataset.copy()\ndf = df[start_index:]\nif max_items and max_items &gt; 0:\nmax_items = min(max_items, len(df))\ndf = df[:max_items]\ninputs = df.to_dict(orient=\"records\")\nlabel_column = self.config.label_column()\nif not self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\ngt_labels = (\nNone\nif not label_column or not len(inputs) or label_column not in inputs[0]\nelse df[label_column].tolist()\n)\nelse:\ngt_labels = {}\nfor attr in self.config.attributes():\nname = attr[\"name\"]\ncolumn_name = attr[\"label_column\"] if \"label_column\" in attr else name\ngt_labels[name] = (\ndf[column_name].tolist() if column_name in df.keys() else None\n)\nself.df = df\nself.inputs = inputs\nself.gt_labels = gt_labels\nif validate:\nself._validate()\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def __repr__(self):\n\"\"\"\n    Returns the representation of the dataset. We currently represent the dataset as a pandas dataframe.\n    \"\"\"\nif self.df is not None:\nreturn self.df.__repr__()\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.columns","title":"<code>columns()</code>","text":"<p>Returns the columns in the dataframe.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def columns(self):\n\"\"\"\n    Returns the columns in the dataframe.\n    \"\"\"\nreturn self.df.columns.tolist()\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.completed","title":"<code>completed()</code>","text":"<p>Filter the dataset to only include completed items. This means the labels where the llm was able to generate a label successfully.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def completed(self):\n\"\"\"\n    Filter the dataset to only include completed items. This means the labels\n    where the llm was able to generate a label successfully.\n    \"\"\"\nfiltered_df = self.df[self.df[self.generate_label_name(\"error\")].isnull()]\nreturn AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.correct","title":"<code>correct(label_column=None)</code>","text":"<p>Filter the dataset to only include correct items. This means the labels where the llm label was correct. Args:     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def correct(self, label_column: str = None):\n\"\"\"\n    Filter the dataset to only include correct items. This means the labels\n    where the llm label was correct.\n    Args:\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\ngt_label_column = label_column or self.config.label_column()\nif gt_label_column is None:\nraise ValueError(\"Cannot compute correct without ground truth label column\")\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"label\", label_column)]\n== self.df[gt_label_column]\n]\nreturn AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.eval","title":"<code>eval()</code>","text":"<p>Evaluate the dataset based on the task. We run the metrics that were specified by the task being run.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def eval(self):\n\"\"\"\n    Evaluate the dataset based on the task. We run the metrics that were\n    specified by the task being run.\n    \"\"\"\nllm_labels = self.df[self.generate_label_name(\"annotation\")].tolist()\ntask = TaskFactory.from_config(self.config)\nmetrics = task.eval(llm_labels, self.gt_labels)\ntable = {}\nfor metric in metrics:\nif not isinstance(metric.value, list):\ntable[metric.name] = metric.value\nprint_table(table, console=Console(), default_style=METRIC_TABLE_STYLE)\nreturn metrics\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.filter","title":"<code>filter(label=None, ground_truth=None, filter_func=None, label_column=None)</code>","text":"<p>Filter the dataset based on the label, ground truth or a custom filter function. In case multiple filters are applied, the filters are applied in the following order:     label -&gt; ground_truth -&gt; filter_func Args:     label: The llm label to filter on.     ground_truth: The ground truth label to filter on.     filter_func: A custom filter function to filter on.     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def filter(\nself,\nlabel: str = None,\nground_truth: str = None,\nfilter_func: Callable = None,\nlabel_column: str = None,\n):\n\"\"\"\n    Filter the dataset based on the label, ground truth or a custom filter function.\n    In case multiple filters are applied, the filters are applied in the following order:\n        label -&gt; ground_truth -&gt; filter_func\n    Args:\n        label: The llm label to filter on.\n        ground_truth: The ground truth label to filter on.\n        filter_func: A custom filter function to filter on.\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\nfiltered_df = self.df\nif label:\nfiltered_df = filtered_df[\nfiltered_df[self.generate_label_name(\"label\", label_column)] == label\n]\nif ground_truth:\nfiltered_df = filtered_df[\nfiltered_df[(label_column or self.config.label_column())]\n== ground_truth\n]\nif filter_func:\nfiltered_df = filtered_df.apply(filter_func, axis=1)\nreturn AutolabelDataset(\nfiltered_df,\nself.config,\n)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.filter_by_confidence","title":"<code>filter_by_confidence(threshold=0.5)</code>","text":"<p>Filter the dataset to only include items with confidence scores greater than the threshold. Args:     threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def filter_by_confidence(self, threshold: float = 0.5):\n\"\"\"\n    Filter the dataset to only include items with confidence scores greater than the threshold.\n    Args:\n        threshold: The threshold to filter on. This means that only items with confidence scores greater than the threshold will be included.\n    \"\"\"\nif not self.config.confidence():\nraise ValueError(\n\"Cannot compute correct and confident without confidence scores\"\n)\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"confidence\")] &gt;= threshold\n]\nreturn AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.incorrect","title":"<code>incorrect(label=None, ground_truth=None, label_column=None)</code>","text":"<p>Filter the dataset to only include incorrect items. This means the labels where the llm label was incorrect. Args:     label: The llm label to filter on.     ground_truth: The ground truth label to filter on.     label_column: The column to filter on. This is only used for attribute extraction tasks.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def incorrect(\nself, label: str = None, ground_truth: str = None, label_column: str = None\n):\n\"\"\"\n    Filter the dataset to only include incorrect items. This means the labels\n    where the llm label was incorrect.\n    Args:\n        label: The llm label to filter on.\n        ground_truth: The ground truth label to filter on.\n        label_column: The column to filter on. This is only used for attribute extraction tasks.\n    \"\"\"\ngt_label_column = label_column or self.config.label_column()\nif gt_label_column is None:\nraise ValueError(\n\"Cannot compute mistakes without ground truth label column\"\n)\nfiltered_df = self.df[\nself.df[self.generate_label_name(\"label\", label_column)]\n!= self.df[gt_label_column]\n]\nif label:\nfiltered_df = filtered_df[\nfiltered_df[self.generate_label_name(\"label\", label_column)] == label\n]\nif ground_truth:\nfiltered_df = filtered_df[filtered_df[gt_label_column] == ground_truth]\nreturn AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.non_completed","title":"<code>non_completed()</code>","text":"<p>Filter the dataset to only include non completed items. This means the labels where the llm was not able to generate a label or there was some error while generating the label.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def non_completed(self):\n\"\"\"\n    Filter the dataset to only include non completed items. This means the labels\n    where the llm was not able to generate a label or there was some error while\n    generating the label.\n    \"\"\"\nfiltered_df = self.df[self.df[self.generate_label_name(\"error\")].notnull()]\nreturn AutolabelDataset(filtered_df, self.config)\n</code></pre>"},{"location":"autolabel/guide/resources/autolabel_dataset/#src.autolabel.dataset.dataset.AutolabelDataset.save","title":"<code>save(output_file_name)</code>","text":"<p>Saves the dataset to a file based on the file extension. Args:     output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.</p> Source code in <code>autolabel/src/autolabel/dataset/dataset.py</code> <pre><code>def save(self, output_file_name: str):\n\"\"\"\n    Saves the dataset to a file based on the file extension.\n    Args:\n        output_file_name: The name of the file to save the dataset to. Based on the extension we can save to a csv or jsonl file.\n    \"\"\"\nif output_file_name.endswith(\".csv\"):\nself.df.to_csv(\nstr(output_file_name),\nsep=self.config.delimiter(),\nheader=True,\nindex=False,\n)\nelif output_file_name.endswith(\".jsonl\"):\nself.df.to_json(\nstr(output_file_name),\norient=\"records\",\nlines=True,\nforce_ascii=False,\n)\nelse:\nraise ValueError(f\"Unsupported output file format: {output_file_name}\")\n</code></pre>"},{"location":"autolabel/guide/resources/configs/","title":"Configs","text":"<p>Each labeling run with the autolabel library requires a config to be specified. The config has 5 top-level keys and several nested keys, many of which are optional.</p>"},{"location":"autolabel/guide/resources/configs/#task-name","title":"Task Name","text":"<p>The task name is just a user-provided name for the labeling task and is only used to construct display names for various labeling artifacts (i.e. column names in the output labeled csv/dataframe)</p> Example<pre><code>\"task_name\": \"CompanyEntityMatch\"\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#task-type","title":"Task Type","text":"<p>The task type determines how the Autolabel library should construct the request to the LLM as well as how the LLM response should be parsed and which metrics should be computed. Currently, the library supports the following task types:</p> <ul> <li>entity_matching</li> <li>classification</li> <li>named_entity_recognition</li> <li>question_answering</li> </ul> Example<pre><code>\"task_type\": \"entity_matching\"\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#dataset","title":"Dataset","text":"<p>The dataset config contains information about the dataset to be labeled. Specifically, there are 4 dataset config keys:</p> <ol> <li>label_column (optional): The label column specifies the column containing the labels for each item to use for metric computation if labels are available for the dataset</li> <li>explanation_column (optional): The explanation column specifies the column containing explanations for each item to use for chain-of-thought prompting if it is enabled in the config.</li> <li>delimiter (optional): This key specifies the delimiter used for parsing the dataset CSV. By default, it is assumed to be a comma: \",\"</li> <li>text_column (required for named entity recognition): The text column is only necessary for named entity recognition tasks and specifies the column containing the text that we intend to label and is used for determining text spans.</li> </ol> Example 1: Classification task<pre><code>\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n}\n</code></pre> Example 2: Chain of thought<pre><code>   \"dataset\": {\n\"label_column\": \"answer\",\n\"explanation_column\": \"explanation\",\n\"delimiter\": \",\"\n}\n</code></pre> Example 3: Named entity recognition task<pre><code>   \"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\",\n\"delimiter\": \",\"\n}\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#model","title":"Model","text":"<p>The model config contains information about the LLM provider and specific model we intend to use for labeling. There are 4 model config keys:</p> <ol> <li>provider: This key specifies the LLM provider.</li> <li>name: The model name specifies which of the provider's models to use for generating labels.</li> <li>params (optional): Params is a dictionary that allows the user to configure model-specific paramaters. Here is an example model params dict:</li> <li>max_tokens: Max tokens specifies the maximum total input and output tokens for each LLM call.</li> <li>temperature: The temperature controls how deterministic the LLM responses should be.</li> <li> <p>model_kwargs: The model kwargs contains the logprobs key which, when present, configures the LLM request to have the LLM return log probabilities</p> </li> <li> <p>compute_confidence (optional): This boolean determines whether to compute and output confidence scores.</p> </li> </ol> Example 1: Compute confidence<pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"compute_confidence\": True\n}\n</code></pre> Example 2: Defining model params<pre><code>\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {\n\"max_tokens\": 512,\n\"temperature\": 0.1\n}\n}\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#embedding","title":"Embedding","text":"<p>The embedding config contains information about the text embedding model provider and the specific model we intend to use for computing text embeddings. There are 2 embedding config keys:</p> <ol> <li>provider: This key specifies the text embedding model provider.</li> <li>model: The model specifies which of the provider's text embedding models to use for generating labels. This key is optional and a default text embedding model is used if no model is specified</li> </ol> Example 1: Huggingface sentence transformers model<pre><code>\"embedding\": {\n\"provider\": \"huggingface_pipeline\",\n\"model\": \"sentence-transformers/all-mpnet-base-v2\"\n}\n</code></pre> Example 2: Google model with no model name<pre><code>\"embedding\": {\n\"provider\": \"google\"\n}\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#prompt","title":"Prompt","text":"<p>The prompt config contains information about how the prompt should be constructed in the request to the LLM. There are 9 prompt config keys.</p> <ol> <li>task_guidelines: The task guidelines should contain a description of the specific labeling task, including any nuanced details about how to correctly label each item.</li> <li>labels (required for some tasks): The labels defines the full list of labels for the model.</li> <li>few_shot_examples (optional): The few shot examples is either a list or path to the CSV of possible seed examples to append to the prompt.</li> <li> <p>few_shot_selection (optional): The few shot selection is the specific strategy to use for selecting examples to use in the prompt. Currently, there are 3 example selection strategies implemented:</p> <ul> <li>fixed</li> <li>semantic_similarity</li> <li>max_marginal_relevance</li> </ul> </li> <li> <p>few_shot_num (optional): The few shot number determines how many seed examples to select and include in the prompt</p> </li> <li>example_template: The example template determines how each example should be formatted in the prompt. You can reference columns from the dataset by wrapping the column name with curly braces</li> <li>output_guidelines (optional): The output guidelines specify how the output should be returned by the LLM (i.e. just return the label vs. format as CSV). It is not recommended to add output guidelines for most use cases as default guidelines are already set.</li> <li>output_format (optional): The format of the output is either \"csv\" or \"json\", but it is not recommended to override the default selection.</li> <li>chain_of_thought (optional): This boolean determines whether to use chain of thought in the prompt or not.</li> </ol> Example 1: Classification task<pre><code>\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying toxic comments. You aim to act in a fair and balanced manner, where comments that provide fair criticism of something or someone are labelled 'not toxic'. Similarly, criticisms of policy and politicians are marked 'not toxic', unless the comment includes obscenities, racial slurs or sexually explicit material. Any comments that are sexually explicit, obscene, or insults a person, demographic or race are not allowed and labeled 'toxic'. \\nYour job is to correctly label the provided input example into one of the following categories:\\n{labels}\",\n\"labels\": [\n\"toxic\",\n\"not toxic\"\n],\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n</code></pre> Example 2: Use seed examples<pre><code>   \"prompt\": {\n\"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"not duplicate\",\n\"duplicate\"\n],\n\"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n\"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n\"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n\"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3\n}\n</code></pre>"},{"location":"autolabel/guide/resources/configs/#full-example-configs","title":"Full Example Configs","text":"Example 1: Company Entity Match<pre><code>{\n\"task_name\": \"CompanyEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are provided with descriptions of companies from their websites, and wikipedia pages. Your job is to categorize whether the descriptions are about the same company (duplicate) or different companies (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"not duplicate\",\n\"duplicate\"\n],\n\"example_template\": \"Company 1 description: {entity1}\\nCompany 2 description: {entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"entity1\": \"lac wisconsin branding 95 1 &amp; 96 1 the rock frequency 96.1 mhz translator s 95.1 w236ag fond du lac first air date 1965 as wcwc fm at 95.9 format mainstream rock erp 4 000 watts haat 123 meters 404 ft class a facility id 54510 transmitter coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 coordinates 43 49 10.00 n 88 43 20.00 w 43.8194444 n 88.7222222 w 43.8194444 ; 88.7222222 former callsigns wcwc fm 1965 1980 wyur 1980 1994 former frequencies 95.9 mhz 1965 affiliations cbs radio network westwood one premiere radio networks owner radio plus inc. sister stations wfdl wfdl fm wmdc webcast listen live website 961tcx . com studios in fond du lac wtcx 96.1 fm 95 1 &amp; 96 1 the rock is a radio station broadcasting a mainstream rock music format . 1 licensed to ripon wisconsin usa the station is currently owned by radio plus inc. and features programing from cbs radio network dial global and premiere radio networks . 2 wtcx was originally on 95.9 mhz . be\",\n\"entity2\": \"closings contact next racing rocks local news breaking wiaa releases football playoffs matchups and brackets october 15 2016 local news here are the full brackets for the state of wisconsin division 1 2 seed fond du lac hosts 7 seed milwaukee washington friday october 21 at 7pm division 5 3 seed wla hosts 6 seed ... read more 10 15 16 fdl man injured in hit and run car vs. bike crash october 15 2016 local news a fond du lac man received non life threatening injuries in a car versus bicycle hit and run crash in dodge county . the dodge county sheriff s office says shortly after 8pm friday a car ... read more 10 15 16 ripon woman remains in critical condition following one vehicle crash october 15 2016 local news a ripon woman injured in a one vehicle crash after apparently falling asleep at the wheel remains in critical condition . the fond du lac county sheriff s office says 29 year old raquel amador ... read more wiaa releases football groupings october 15 2016 local news 2016 wiaa fo\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"stacy spikes hamet watt headquarters new york city united states website http www.moviepass.com moviepass is a subscription based service for going to movie theaters available in the united states . the service gives members across the country the ability to see up to one 2d movie every 24 hours for a fixed monthly fee . members may choose which theaters they wish to attend and there are no blackout dates . moviepass works in nearly all movie theaters that accept the mastercard credit card making it one of the largest subscription based theater networks in america . prices vary by local market and start at 30 per month . moviepass was launched in february 2011 and is headquartered in new york city . 1 contents 1 service 2 purchasing a ticket 3 history 4 media coverage 5 references service edit the moviepass service works via a smartphone app iphone android and a specially designed reloadable debit card which is mailed to new subscribers when they sign up . purchasing a ticket edit in o\",\n\"entity2\": \"repair buy warranty get service buy warranty home warranty pricing &amp; plans planning on moving home matters blog what s covered service professionals customer reviews benefits faqs appliance discount contract policies decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech close home warranty learn more what s covered service professionals faqs pricing and plans get a quote see plans planning on moving real estate plans buying a home selling a home home matters blog decor cost savers lawn &amp; garden lifestyle quick tips real estate repair &amp; maintenance tech our partner sites real estate professionals contractors 888 429 8247 email us log in back to top get a personalized quote explore plans in your area get covered in 3 easy steps . please correct highlighted fields request service log in create account oven on the fritz appliance breakdowns happen . get covered . get a personalized quote explore plans in your area get covered in 3 easy steps . please co\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"of over 110 gyms worldwide including 86 franchise locations in ma pa ny nj ct wa or ca tx fl ky va puerto rico and australia and is rapidly expanding across the u.s. and around the globe . contents 1 history 2 description 3 references 4 external links history edit crunch was founded in a basement level aerobics studio in new york city s east village in 1989 by doug levine . 1 with the collaboration of fitness instructors the group fitness programming was started at crunch . offerings such as hip hop aerobics co ed action wrestling and cyked yoga cycling were introduced . 2 in clubs members have access to innovative group fitness classes state of the art equipment personal and group training full service locker rooms and much more . select locations offer an exclusive crunch retail line that can also be purchased from the crunch online store . 3 in january 2014 crunch released its online workout extension called crunch live . this subscription based online video library has over 95 work\",\n\"entity2\": \"gallery esp en best rate guarantee check availability call us room only 1 800 990 8250 hotel air 1 800 219 2727 canada 1 855 478 2811 airport transportation travel agents close best rate guaranteebook your all inclusive stay hotel hotel air arrive departure adults 1 2 3 4 5 6 7 8 children 0 1 2 3 4 5 6 7 8 select property pacifica golf &amp; spa resort the towers at pacifica sunset beach golf &amp; spa resort ros resort &amp; spa los cabos montecristo estates mazatl n emerald bay resort &amp; spa emerald estates luxury villas departure country argentina australia austria bahamas belgium brazil canada chile colombia costa rica denmark ecuador finland france germany greece honduras iceland israel italy japan luxembourg mexico netherlands new zealand nicaragua norway panama paraguay peru portugal puerto rico republic of ireland republic of korea south africa spain sweden switzerland turks and caicos islands united kingdom united states uruguay venezuela departure city akron canton ohio reg . albany ny al\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3\n}\n}\n</code></pre> Example 2: Banking Complaints Classification<pre><code>{\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n\"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n\"atm_support\",\n\"automatic_top_up\",\n\"balance_not_updated_after_bank_transfer\",\n\"balance_not_updated_after_cheque_or_cash_deposit\",\n\"beneficiary_not_allowed\",\n\"cancel_transfer\",\n\"card_about_to_expire\",\n\"card_acceptance\",\n\"card_arrival\",\n\"card_delivery_estimate\",\n\"card_linking\",\n\"card_not_working\",\n\"card_payment_fee_charged\",\n\"card_payment_not_recognised\",\n\"card_payment_wrong_exchange_rate\",\n\"card_swallowed\",\n\"cash_withdrawal_charge\",\n\"cash_withdrawal_not_recognised\",\n\"change_pin\",\n\"compromised_card\",\n\"contactless_not_working\",\n\"country_support\",\n\"declined_card_payment\",\n\"declined_cash_withdrawal\",\n\"declined_transfer\",\n\"direct_debit_payment_not_recognised\",\n\"disposable_card_limits\",\n\"edit_personal_details\",\n\"exchange_charge\",\n\"exchange_rate\",\n\"exchange_via_app\",\n\"extra_charge_on_statement\",\n\"failed_transfer\",\n\"fiat_currency_support\",\n\"get_disposable_virtual_card\",\n\"get_physical_card\",\n\"getting_spare_card\",\n\"getting_virtual_card\",\n\"lost_or_stolen_card\",\n\"lost_or_stolen_phone\",\n\"order_physical_card\",\n\"passcode_forgotten\",\n\"pending_card_payment\",\n\"pending_cash_withdrawal\",\n\"pending_top_up\",\n\"pending_transfer\",\n\"pin_blocked\",\n\"receiving_money\",\n\"Refund_not_showing_up\",\n\"request_refund\",\n\"reverted_card_payment?\",\n\"supported_cards_and_currencies\",\n\"terminate_account\",\n\"top_up_by_bank_transfer_charge\",\n\"top_up_by_card_charge\",\n\"top_up_by_cash_or_cheque\",\n\"top_up_failed\",\n\"top_up_limits\",\n\"top_up_reverted\",\n\"topping_up_by_card\",\n\"transaction_charged_twice\",\n\"transfer_fee_charged\",\n\"transfer_into_account\",\n\"transfer_not_received_by_recipient\",\n\"transfer_timing\",\n\"unable_to_verify_identity\",\n\"verify_my_identity\",\n\"verify_source_of_funds\",\n\"verify_top_up\",\n\"virtual_card_not_working\",\n\"visa_or_mastercard\",\n\"why_verify_identity\",\n\"wrong_amount_of_cash_received\",\n\"wrong_exchange_rate_for_cash_withdrawal\"\n],\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 10,\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n}\n}\n</code></pre>"},{"location":"autolabel/guide/resources/refuel_datasets/","title":"Refuel-provided Datasets","text":"<p>Autolabel provides datasets out-of-the-box so you can easily get started with LLM-powered labeling. The full list of datasets is below:</p> Dataset Task Type banking Classification civil_comments Classification ledgar Classification movie_reviews Classification walmart_amazon Entity Matching company Entity Matching squad_v2 Question Answering sciq Question Answering conll2003 Named Entity Matching"},{"location":"autolabel/guide/resources/refuel_datasets/#downloading-any-dataset","title":"Downloading any dataset","text":"<p>To download a specific dataset, such as <code>civil_comments</code>, run: <pre><code>from autolabel import get_data\nget_data('civil_comments')\n&gt; Downloading seed example dataset to \"data/civil_comments/seed.csv\"...\n&gt; 100% [..............................................................................] 65757 / 65757\n&gt; Downloading test dataset to \"data/civil_comments/test.csv\"...\n&gt; 100% [............................................................................] 610663 / 610663\n</code></pre></p>"},{"location":"autolabel/guide/resources/synthetic_dataset_generation/","title":"Synthetic Dataset Generation","text":"<p>Few shot learning is one of the most powerful tools that autolabel offers to improve the accuracy of LLM generated labels. However, curating a seed dataset to use for few shot learning can be a time consuming and tedious process. To make this process easier, autolabel's LabelingAgent provides a method to generate synthetic datasets. These datasets can be used as seed datasets for few shot learning or any other purpose. This guide will walk you through the process of generating a synthetic dataset using autolabel.</p> <p>Currently, autolabel supports synthetic dataset generation for classification and entity matching tasks. We plan to add support for other task types in the future.</p>"},{"location":"autolabel/guide/resources/synthetic_dataset_generation/#walkthrough-creating-a-synthetic-dataset-for-banking","title":"Walkthrough: Creating a Synthetic Dataset for Banking","text":"<ol> <li>The first step is to import the LabelingAgent from autolabel. This is the main class that we will use to generate the synthetic dataset.  <pre><code>from autolabel import LabelingAgent\n</code></pre> </li> <li>The next step is to create the task config. Make sure to add the <code>dataset_generation</code> section to the config. This section contains the parameters for the dataset generation process. The <code>guidelines</code> parameter is a string containing the guidelines for the dataset generation task. The <code>num_rows</code> parameter is an integer indicating the number of rows per label to generate in the dataset.  <pre><code>config = {\n\"task_name\": \"BankingComplaintsClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding bank customers support complaints and queries.\\nYour job is to correctly classify the provided input example into one of the following categories.\\nCategories:\\n{labels}\",\n\"output_guidelines\": \"You will answer with just the the correct output label and nothing else.\",\n\"labels\": {\n\"activate_my_card\": \"the customer cannot activate their credit or debit card\",\n\"age_limit\": \"the customer is under the age limit\",\n\"apple_pay_or_google_pay\": \"the customer is having trouble using apple pay or google pay\",\n... # more labels\n},\n\"example_template\": \"Input: {example}\\nOutput: {label}\"\n},\n\"dataset_generation\": {\n\"num_rows\": 5,\n\"guidelines\": \"You are an expert at generating synthetic data. You will generate a dataset that satisfies the following criteria:\\n1. The data should be diverse and cover a wide range of scenarios.\\n2. The data should be as realistic as possible, closely mimicking real-world data.\\n3. The data should vary in length, some shorter and some longer.\\n4. The data should be generated in a csv format.\\n\\nEach row should contain a realistic bank complaint. Use CSV format, with each line containing just the complaint and nothing else.\"\n}\n}\n</code></pre>  Note that here, we defined <code>labels</code> as a dictionary where the keys are the valid labels and the values are descriptions for those labels. This helps the LLM understand what each label means and can result in a higher quality dataset.  </li> <li>Now all that's left is to run the code that generates the dataset!  <pre><code>agent = LabelingAgent(config)\nds = agent.generate_synthetic_dataset()\n</code></pre> </li> </ol> <p>That's it! You now have a synthetic dataset that you can use for few shot learning or for any other purpose. You can save the dataset to a csv file using the following code:</p> <pre><code>ds.save(\"synthetic_dataset.csv\")\n</code></pre>"},{"location":"autolabel/guide/resources/synthetic_dataset_generation/#model-and-model-parameters","title":"Model and Model Parameters","text":"<p>To edit the model used for synthetic dataset generation, simply change the <code>model</code> section of the config. We've found that setting a higher temperature for this task generally results in more realistic datasets. We recommend experimenting with different models and model parameters to see what works best for your use case.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/","title":"Attribute Extraction","text":""},{"location":"autolabel/guide/tasks/attribute_extraction/#introduction","title":"Introduction","text":"<p>Attribute Extraction is a task that shows up in real world frequently. This task extracts multiple attributes or features from a single piece of text. For eg. extracting the colour, price and name from a product description paragraph. Instead of making multiple calls to the llm, we can extract all attributes in one call! Additionally, if the attributes are related to each other, doing attribute extraction means that the relationships between the outputs are respected i.e suppose we extract the length of a shirt along with its letter size. Doing attribute extraction would make sure the letter and the integer length are consistent.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/attribute_extraction/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for attribute extraction on the ethos dataset. The ethos dataset comprises of hate speech on social media platforms. Every datapoints consists of an exmaple with hate speech and corresponding to it, there are three attributes, i.e violence, gender and directed_vs_generalized.</p> <pre><code>{\n\"example\": \"tweet containing hate speech\",\n\"violence\": \"violent\",\n\"directed_vs_generalized\": \"directed\",\n\"gender\": \"false\"\n}\n</code></pre> <p>Thus the dataset contains of 4 columns, the example along with the 3 attributes. Here, Autolabel would be given the example input for a new datapoint and told to predict the labels for the 3 attributes.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <pre><code>config = {\n\"task_name\": \"EthosAttributeExtraction\",\n\"task_type\": \"attribute_extraction\",\n\"dataset\": {\n\"text_column\": \"text\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at classifying hate speech and identifying the type of hate speech. Read the following tweets and extract the following attributes from the text.\",\n\"attributes\": [\n{\n\"name\": \"violence\",\n\"options\": [\"not_violent\", \"violent\"],\n\"description\": \"If the tweet mentions violence towards a person or a group.\"\n},\n{\n\"name\": \"directed_vs_generalized\",\n\"options\": [\n\"generalized\",\n\"directed\"\n],\n\"description\": \"If the hate speech is generalized towards a group or directed towards a specific person.\"\n},\n{\n\"name\": \"gender\",\n\"options\": [\n\"true\",\n\"false\"\n],\n\"description\": \"If the hate speech uses gendered language and attacks a particular gender.\"\n}\n],\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 5,\n\"example_template\": \"Text: {text}\\nOutput: {output_dict}\"\n}\n}\n</code></pre> <p>The <code>task_type</code> sets up the config for a specific task, attribute_extraction in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.</p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at classifying hate speech.</p> <p>The <code>attributes</code> key is the most important key for defining attribute extraction well. For every attribute, we have atleast 2 keys -  a. <code>name</code> - This is the name of the attribute. b. <code>description</code> - This is the description of an attribute. This describes the attribute more concretely and prompts the model to extract the corresponding attribute. c. <code>options</code> - You can also define a list of options for the LLM. This is an optional field. In case the attribute has a list of values from which to choose the value, fill this list. Otherwise, the attribute is prompted to be any possible textual value.</p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset. Here we define the <code>output_dict</code> key, which is used in the example template for attribute extraction tasks. This will create a json of all the attributes, as key value pairs. The LLM is also prompted to output the attributes in a json.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/attribute_extraction/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items.</p> <pre><code>Actual Cost: 0.0665\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 violence:\u2026 \u2503 violence:\u2026 \u2503 violence:\u2026 \u2503 directed_\u2026 \u2503 directed\u2026 \u2503 directed_\u2026 \u2503 gender:s\u2026 \u2503 gender:co\u2026 \u2503 gender:a\u2026 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100        \u2502 1.0        \u2502 0.89       \u2502 100        \u2502 1.0       \u2502 0.89       \u2502 100       \u2502 1.0        \u2502 0.94      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/#confidence","title":"Confidence","text":"<p>You can calculate per attribute confidence metric as well by setting compute_confidence as true in the model config. This can help you decide which examples to keep per attribute.</p>"},{"location":"autolabel/guide/tasks/attribute_extraction/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here.</p>"},{"location":"autolabel/guide/tasks/classification_task/","title":"Classification Task","text":""},{"location":"autolabel/guide/tasks/classification_task/#introduction","title":"Introduction","text":"<p>Text classification is a fundamental task in natural language processing (NLP) that involves categorizing textual data into predefined classes or categories. It is employed in various applications such as sentiment analysis, spam detection, topic classification, intent recognition, and document categorization and can be used in any setting where there are well defined categories which the LLM can understand and put an input into.</p>"},{"location":"autolabel/guide/tasks/classification_task/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/classification_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for text classification on the Banking77 dataset. The Banking77 dataset comprises of 13,083 customer service queries labeled with 77 intents. It focuses on fine-grained single-domain intent detection. Every datapoint consists of an example and its corresponding label as shown below. The label belongs to a set of 77 predefined intents that the customer had for the particular datapoint for eg. activate_my_card, card_delivery_estimate, get_physical_card.</p> <pre><code>{\n\"example\": \"What can I do if my card still hasn't arrived after 2 weeks?\",\n\"label\": \"card_arrival\"\n}\n</code></pre> <p>Thus the dataset consists of just two columns, example and label. Here, Autolabel would be given the example input for a new datapoint and told to predict the label column which in this case is label.</p>"},{"location":"autolabel/guide/tasks/classification_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"BankingClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n\"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n...\n],\n\"example_template\": \"Example: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, classification in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding banking transaction complaints. Next, we define the task more concretely using the num_labels and labels appropriately. <code>{num_labels}</code> will be internally translated by the library to be the number of elements in the <code>labels</code> list (defined below).  <code>{labels}</code> will be translated to be all the labels in the <code>labels</code> list separated by a newline. These are essential for setting up classification tasks by telling it the labels that it is constrained to, along with any meaning associated with a label.  </p> <p>The <code>labels</code> key defines the list of possible labels for the banking77 dataset which is a list of 77 possible labels.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is label in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p>"},{"location":"autolabel/guide/tasks/classification_task/#few-shot-config","title":"Few Shot Config","text":"<p>Let's assume we have access to a dataset of labeled seed examples. Here is a config which details how to use it.</p> <pre><code>config = {\n\"task_name\": \"BankingClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n\"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n...\n],\n\"few_shot_examples\": \"../examples/banking/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5,\n\"example_template\": \"Example: {example}\\nOutput: {label}\"\n}\n}\n</code></pre> <p>The <code>few_shot_examples</code> key defines the seed set of labeled examples that are present for the model to learn from. A subset of these examples will be picked while querying the LLM in order to help it understand the task better, and understand corner cases.  </p> <p>For the banking dataset, we found <code>semantic_similarity</code> search to work really well. This looks for examples similar to a query example from the seed set and sends those to the LLM when querying for a particular input. This is defined in the <code>few_shot_selection</code> key.  </p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p>"},{"location":"autolabel/guide/tasks/classification_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/banking77.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/classification_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Cost in $=0.00, support=50, threshold=-inf, accuracy=0.6600, completion_rate=1.0000\nActual Cost: 0.0058579999999999995\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.76     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - We use accuracy as the main metric for evaluating classification tasks. This is done by checking the fraction of examples which are given the correct label in the training dataset.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/tasks/classification_task/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here</p>"},{"location":"autolabel/guide/tasks/classification_task/#classification-tasks-with-a-large-number-of-classes","title":"Classification Tasks with a Large Number of Classes","text":"<p>For classification tasks with a wide variety of possible classes, it is beneficial to run autolabel with <code>label_selection</code> turned on. In this mode, Autolabel will prune the list of possible classes to only include those that are similar to the example being labeled. This not only helps improve accuracy, but also substantially reduces labeling costs, as the size of the prompt decreases when classes are pruned.</p> <p>To enable label_selection, simply set <code>label_selection</code> to <code>true</code> in your config file. Similarly, you can choose how many classes to select in the similarity search by setting <code>label_selection_count</code> to a value of your choosing.</p> <pre><code>    \"label_selection\": true,\n\"label_selection_count\": 10\n</code></pre> <p>In this example, the list of classes will be reduced to only the 10 classes most similar to the example being labeled.</p> <pre><code>config = {\n\"task_name\": \"BankingClassification\",\n\"task_type\": \"classification\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"\"\"You are an expert at understanding banking transaction complaints.\\nYour job is to correctly label the provided input example into one of the following {num_labels} categories:\\n{labels}\"\"\",\n\"output_guidelines\": \"You will just return one line consisting of the label for the given example.\",\n\"labels\": [\n\"activate_my_card\",\n\"age_limit\",\n\"apple_pay_or_google_pay\",\n...\n],\n\"few_shot_examples\": \"../examples/banking/seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5,\n\"example_template\": \"Example: {example}\\nOutput: {label}\",\n\"label_selection\": true,\n\"label_selection_count\": 10\n}\n}\n</code></pre>"},{"location":"autolabel/guide/tasks/entity_matching_task/","title":"Entity Matching Task","text":""},{"location":"autolabel/guide/tasks/entity_matching_task/#introduction","title":"Introduction","text":"<p>Entity matching in natural language processing (NLP) is a task that involves identifying and matching entities from different sources or datasets based on various fields or attributes. The goal is to determine if two entities refer to the same real-world object or entity, even if they are described differently or come from different data sources.</p>"},{"location":"autolabel/guide/tasks/entity_matching_task/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/entity_matching_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for entity matching on the Walmart-Amazon dataset. This dataset consists of duplicate products listed on both Walmart and Amazon. These products would have different names and descriptions but would be the same product. The dataset consists of such examples, where given the name and the description, the task is to predict if the products are duplicate or not. An example from the Walmart-Amazon dataset,</p> <pre><code>{\n\"entity1\": \"Title: zotac geforce gt430 1gb ddr3 pci-express 2.0 graphics card; Category: electronics - general; Brand: zotac; ModelNo: zt-40604-10l; Price: 88.88;\",\n\"entity2\": \"Title: evga geforce gts450 superclocked 1 gb gddr5 pci-express 2.0 graphics card 01g-p3-1452-tr; Category: graphics cards; Brand: evga; ModelNo: 01g-p3-1452-tr; Price: 119.88;\",\n\"label\": \"not duplicate\"\n}\n</code></pre> <p>The the dataset consists of two columns <code>entity1</code> and <code>entity2</code> which define the two entities. There could also be multiple columns defining an entity. The <code>label</code> column here defines if the two entities are duplicates or not.</p>"},{"location":"autolabel/guide/tasks/entity_matching_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"ProductCatalogEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"duplicate\",\n\"not duplicate\"\n],\n\"few_shot_examples\": [\n{\n\"entity1\": \"Title: lexmark extra high yield return pgm print cartridge - magenta; Category: printers; Brand: lexmark; ModelNo: c782u1mg; Price: 214.88;\",\n\"entity2\": \"Title: lexmark 18c1428 return program print cartridge black; Category: inkjet printer ink; Brand: lexmark; ModelNo: 18c1428; Price: 19.97;\",\n\"label\": \"not duplicate\"\n},\n{\n\"entity1\": \"Title: edge tech proshot 4gb sdhc class 6 memory card; Category: usb drives; Brand: edge tech; ModelNo: pe209780; Price: 10.88;\",\n\"entity2\": \"Title: 4gb edge proshot sdhc memory card class6; Category: computers accessories; Brand: edge; ModelNo: nan; Price: 17.83;\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"Title: tomtom one carry case; Category: gps; Brand: tomtom; ModelNo: 9n00 .181; Price: 19.96;\",\n\"entity2\": \"Title: tomtom one carrying case; Category: cases; Brand: tomtom; ModelNo: 9n00 .181; Price: 4.99;\",\n\"label\": \"duplicate\"\n},\n{\n\"entity1\": \"Title: iosafe rugged 250gb usb 3.0 portable external hard drive; Category: hard drives; Brand: iosafe; ModelNo: pa50250u5yr; Price: 249.99;\",\n\"entity2\": \"Title: lacie rugged all-terrain 500 gb firewire 800 firewire 400 usb 2.0 portable external hard drive 301371; Category: external hard drives; Brand: lacie; ModelNo: 301371; Price: nan;\",\n\"label\": \"not duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Entity1: {entity1}\\nEntity2: {entity2}\\nOutput: {label}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, entity_matching in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at identifying duplicate products. Next we explain the task to the model, saying that it has two identify if the given products are duplicate or not. We also make the output format clear by telling the model it has to choose from the options duplicate or not duplicate. </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we give the model both the entities separated by newlines and ask if the entities are duplicate or not duplicate.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"autolabel/guide/tasks/entity_matching_task/#alternate-config-with-multiple-columns","title":"Alternate config with multiple columns","text":"<p>Let's consider the case in which there are multiple columns in the dataset which are combined to create an input for the model.</p> <pre><code>config = {\n\"task_name\": \"ProductCatalogEntityMatch\",\n\"task_type\": \"entity_matching\",\n\"dataset\": {\n\"label_column\": \"label\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at identifying duplicate products from online product catalogs.\\nYou will be given information about two product entities, and your job is to tell if they are the same (duplicate) or different (not duplicate). Your answer must be from one of the following options:\\n{labels}\",\n\"labels\": [\n\"duplicate\",\n\"not duplicate\"\n],\n\"example_template\": \"Title of entity1: {Title_entity1}; category of entity1: {Category_entity1}; brand of entity1: {Brand_entity1}; model number of entity1: {ModelNo_entity1}; price of entity1: {Price_entity1}\\nTitle of entity2: {Title_entity2}; category of entity2: {Category_entity2}; brand of entity2: {Brand_entity2}; model number of entity2: {ModelNo_entity2}; price of entity2: {Price_entity2}\\nDuplicate or not: {label}\",\n\"few_shot_examples\": [\n{\n\"Title_entity1\": \"lexmark extra high yield return pgm print cartridge - magenta\",\n\"Category_entity1\": \"printers\",\n\"Brand_entity1\": \"lexmark\",\n\"ModelNo_entity1\": \"c782u1mg\",\n\"Price_entity1\": \"214.88\",\n\"Title_entity2\": \"lexmark 18c1428 return program print cartridge black\",\n\"Category_entity2\": \"inkjet printer ink\",\n\"Brand_entity2\": \"lexmark\",\n\"ModelNo_entity2\": \"18c1428\",\n\"Price_entity2\": \"19.97\",\n\"label\": \"not duplicate\"\n},\n{\n\"Title_entity1\": \"edge tech proshot 4gb sdhc class 6 memory card\",\n\"Category_entity1\": \"usb drives\",\n\"Brand_entity1\": \"edge tech\",\n\"ModelNo_entity1\": \"pe209780\",\n\"Price_entity1\": \"10.88\",\n\"Title_entity2\": \"4gb edge proshot sdhc memory card class6\",\n\"Category_entity2\": \"computers accessories\",\n\"Brand_entity2\": \"edge\",\n\"ModelNo_entity2\": \"nan\",\n\"Price_entity2\": \"17.83\",\n\"label\": \"duplicate\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 2\n}\n}\n</code></pre> <p>Notice how in this case, we specify how the different columns defining different aspects of every column are stitched together to form the final example template.</p>"},{"location":"autolabel/guide/tasks/entity_matching_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/walmart_amazon_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/entity_matching_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 100     \u2502 -inf      \u2502 0.96     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This measures the proportion of examples which are marked correctly by the model - for eg which mark duplicate entities correctly.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/","title":"Multilabel Classification Task","text":""},{"location":"autolabel/guide/tasks/multilabel_classification_task/#introduction","title":"Introduction","text":"<p>Multilabel text classification is a fundamental task in natural language processing (NLP) where textual data is categorized into predefined classes or categories. It expands upon traditional text classification by assigning multiple labels to each text instance. This approach finds applications in sentiment analysis, spam detection, topic classification, intent recognition, and document categorization. By considering multiple labels, it allows for a more nuanced representation of text data, accommodating scenarios where multiple topics or attributes are associated with a document. Multilabel text classification enables a flexible and comprehensive approach to categorizing textual data, providing a richer understanding of content and facilitating more nuanced decision-making in various NLP applications.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/multilabel_classification_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for multilabel text classification on the sem_eval_2018_task_1 dataset which we call twitter-emotion-detection for clarity. The twitter-emotion-detection dataset comprises of 10,983 English tweets and 11 emotions. If no emotions were selected for a row, we classified it as <code>neutral</code>.</p> <pre><code>{\n\"example\": \"I blew that opportunity -__- #mad\",\n\"label\": \"anger, disgust, sadness\"\n}\n</code></pre> <p>Thus the dataset consists of just two columns, example and labels. Here, Autolabel would be given the example input for a new datapoint and told to predict the label column which in this case is labels.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <pre><code>config = {\n\"task_name\": \"EmotionClassification\",\n\"task_type\": \"multilabel_classification\",\n\"dataset\": {\n\"label_column\": \"labels\",\n\"label_separator\": \", \",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at classifying tweets as neutral or one or more of the given emotions that best represent the mental state of the poster.\\nYour job is to correctly label the provided input example into one or more of the following categories:\\n{labels}\",\n\"output_guidelines\": \"You will return the answer as a comma separated list of labels sorted in alphabetical order. For example: \\\"label1, label2, label3\\\"\",\n\"labels\": [\n\"neutral\",\n\"anger\",\n\"anticipation\",\n...\n],\n\"example_template\": \"Input: {example}\\nOutput: {labels}\"\n}\n}\n</code></pre> <p>The <code>task_type</code> sets up the config for a specific task, multilabel_classification in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.</p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at classifying tweets. Next, we define the task more concretely using labels appropriately. <code>{labels}</code> will be translated to be all the labels in the <code>labels</code> list separated by a newline. These are essential for setting up classification tasks by telling it the labels that it is constrained to, along with any meaning associated with a label.</p> <p>The <code>labels</code> key defines the list of possible labels for the twitter-emotion-detection dataset which is a list of 12 possible labels.</p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is labels in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the labels. The <code>example_template</code> will be used to format all seed examples.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#few-shot-config","title":"Few Shot Config","text":"<p>Let's assume we have access to a dataset of labeled seed examples. Here is a config which details how to use it.</p> <pre><code>config = {\n\"task_name\": \"EmotionClassification\",\n\"task_type\": \"multilabel_classification\",\n\"dataset\": {\n\"label_column\": \"labels\",\n\"label_separator\": \", \",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at classifying tweets as neutral or one or more of the given emotions that best represent the mental state of the poster.\\nYour job is to correctly label the provided input example into one or more of the following categories:\\n{labels}\",\n\"output_guidelines\": \"You will return the answer as a comma separated list of labels sorted in alphabetical order. For example: \\\"label1, label2, label3\\\"\",\n\"labels\": [\n\"neutral\",\n\"anger\",\n\"anticipation\",\n...\n],\n\"few_shot_examples\": \"seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5,\n\"example_template\": \"Input: {example}\\nOutput: {labels}\"\n}\n}\n</code></pre> <p>The <code>few_shot_examples</code> key defines the seed set of labeled examples that are present for the model to learn from. A subset of these examples will be picked while querying the LLM in order to help it understand the task better, and understand corner cases.</p> <p>For the twitter dataset, we found <code>semantic_similarity</code> search to work really well. This looks for examples similar to a query example from the seed set and sends those to the LLM when querying for a particular input. This is defined in the <code>few_shot_selection</code> key.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('twitter_emotion_detection.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items.</p> <pre><code>Actual Cost: 0.0025\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1     \u2503 support \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.4507 \u2502 100     \u2502 0.08     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>F1 - This is calculated using the precision and recall of the predicted tokens and their classes. We use a macro average to get to one F1 score for all classes.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/tasks/multilabel_classification_task/#notebook","title":"Notebook","text":"<p>You can find a Jupyter notebook with code that you can run on your own here.</p>"},{"location":"autolabel/guide/tasks/named_entity_recognition_task/","title":"Named Entity Recognition Task","text":""},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#introduction","title":"Introduction","text":"<p>Named Entity Recognition (NER) is a crucial task in natural language processing (NLP) that involves identifying and classifying named entities in text. Named entities refer to specific individuals, organizations, locations, dates, quantities, and other named entities present in the text. The goal of NER is to extract and classify these entities accurately, providing valuable information for various NLP applications such as information extraction, question answering, and sentiment analysis.</p>"},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for named entity recognition on the CONLL2003 dataset. The CONLL2003 dataset comprises of sentences with entities in the sentence labeled LOC (location), ORG (organization), PER (person) or MISC (Miscellaneous).  </p> <pre><code>{\n\"example\": \"The role of the 70,000 mainly Kurdish village guards who fight Kurdistan Workers Party ( PKK ) guerrillas in the southeast has been questioned recently after media allegations that many of them are involved in common crime .\",\n\"CategorizedLabels\": \"{'Location': [], 'Organization': ['Kurdistan Workers Party', 'PKK'], 'Person': [], 'Miscellaneous': ['Kurdish']}\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>example</code> and <code>CategorizedLabels</code> columns. Here <code>example</code> mentions the sentence which needs to be labeled. The <code>CategorizedLabels</code> contains the entities for every label as a list.</p>"},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"PersonLocationOrgMiscNER\",\n\"task_type\": \"named_entity_recognition\",\n\"dataset\": {\n\"label_column\": \"CategorizedLabels\",\n\"text_column\": \"example\"\n},\n\"model\": {\n\"provider\": \"anthropic\",\n\"name\": \"claude-v1\"\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at extracting Person, Organization, Location, and Miscellaneous entities from text. Your job is to extract named entities mentioned in text, and classify them into one of the following categories.\\nCategories:\\n{labels}\\n \",\n\"labels\": [\n\"Location\",\n\"Organization\",\n\"Person\",\n\"Miscellaneous\"\n],\n\"example_template\": \"Example: {example}\\nOutput: {CategorizedLabels}\",\n\"few_shot_examples\": \"data/conll2003_seed.csv\",\n\"few_shot_selection\": \"semantic_similarity\",\n\"few_shot_num\": 5\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, named_entity_recognition in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at extracting entities from text and classifying them into the necessary labels. Next, we tell the model the list of categories that it should classify every entity into. This ensures that every entity is assigned to one category.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is <code>CategorizedLabels</code> in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples.  </p> <p>The <code>few_shot_examples</code> here is a path to a csv which defines a set of labeled examples which the model can use to understand the task better. These examples will be used as a reference by the model.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to <code>semantic_similarity</code> in this case as we want to use a subset of examples as seed examples from a larger set to get dynamically good seed examples.</p>"},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('examples/squad_v2/test.csv', config = config)\nagent.plan(ds, max_items = 100)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/named_entity_recognition_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1     \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7834 \u2502 100     \u2502 -inf      \u2502 0.7834   \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Accuracy - This is calculated by taking the exact match of the predicted tokens and their correct class. This may suffer from class imbalance.</p> <p>F1 - This is calculated using the precision and recall of the predicted tokens and their classes. We use a macro average to get to one F1 score for all classes.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/tasks/question_answering_task/","title":"Question Answering Task","text":""},{"location":"autolabel/guide/tasks/question_answering_task/#introduction","title":"Introduction","text":"<p>Question answering is the most fundamental task that can be solved using LLMs. Most tasks can be reduced to some form of question answering where the model is optionally given some context and then asked to answer a question. There can be a broad classification of question answering tasks into 2 categories -  </p> <ol> <li> <p>Open Book QA - In this variant, the model is given a context along with a question and then asked to answer using the context. Here, we do not rely on knowledge present in the model parameters and instead rely on the reasoning abilities and commonsense properties of the model to answer correctly.</p> </li> <li> <p>Closed Book QA - In this variant, the model is just given a question, without any context or knowledge source and asked to answer based on pretrained knowledge. This requires more knowledge to be present in the model parameters and thus favours bigger LLMs.</p> </li> </ol> <p>In addition to context, question answering tasks can also differ in the way that the answers are generated. The easiest form is one where there is a predefined set of options (for eg. yes or no) and the model needs to choose from one of these options. Another variant allows separate options for each question similar to SAT questions. The last variant is one where the model is free to generate its own answers. This variant is harder to evaluate because multiple answers could mean the same thing.</p>"},{"location":"autolabel/guide/tasks/question_answering_task/#example","title":"Example","text":""},{"location":"autolabel/guide/tasks/question_answering_task/#dataset","title":"Dataset","text":"<p>Lets walk through using Autolabel for question answering on the Squad dataset. The Squad dataset comprises of 100k questions and answers along with a context for each question which contains the answer for the question. Additionally, the correct answer is a continuous text span from the context. However, in addition to correct answers, it also contains 50k pairs where the question is unanswerable given the context, that is, the context does not have enough information to answer the question correctly. Here is an example datapoint from the dataset,</p> <pre><code>{\n\"question\": \"When did Beyonce start becoming popular?\",\n\"context\": \"Beyonc\u00e9 Giselle Knowles-Carter (/bi\u02d0\u02c8j\u0252nse\u026a/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonc\u00e9's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles 'Crazy in Love' and 'Baby Boy'.\",\n\"answer\": \"in the late 1990s\"\n}\n</code></pre> <p>Thus the dataset consists of the <code>question</code>, <code>context</code> and <code>answer</code>. For datasets like SciQ, there may be an additional field called <code>options</code> which is a list of strings which are possible answers for a particular question.</p>"},{"location":"autolabel/guide/tasks/question_answering_task/#config","title":"Config","text":"<p>In order to run Autolabel, we need a config defining the 3 important things - task, llm and dataset. Let's assume gpt-3.5-turbo as the LLM for this section.</p> <p><pre><code>config = {\n\"task_name\": \"OpenbookQAWikipedia\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering questions based on wikipedia articles. Your job is to answer the following questions using the context provided with the question. The answer is a continuous span of words from the context. Use the context to answer the question. If the question cannot be answered using the context, answer the question as unanswerable.\",\n\"few_shot_examples\": [\n{\n\"question\": \"What was created by the modern Conservative Party in 1859 to define basic Conservative principles?\",\n\"answer\": \"unanswerable\",\n\"context\": \"The modern Conservative Party was created out of the 'Pittite' Tories of the early 19th century. In the late 1820s disputes over political reform broke up this grouping. A government led by the Duke of Wellington collapsed amidst dire election results. Following this disaster Robert Peel set about assembling a new coalition of forces. Peel issued the Tamworth Manifesto in 1834 which set out the basic principles of Conservatism; \u2013 the necessity in specific cases of reform in order to survive, but an opposition to unnecessary change, that could lead to 'a perpetual vortex of agitation'. Meanwhile, the Whigs, along with free trade Tory followers of Robert Peel, and independent Radicals, formed the Liberal Party under Lord Palmerston in 1859, and transformed into a party of the growing urban middle-class, under the long leadership of William Ewart Gladstone.\"\n},\n{\n\"question\": \"When is King Mom symbolically burnt?\",\n\"answer\": \"On the evening before Lent\",\n\"context\": \"Carnival means weeks of events that bring colourfully decorated floats, contagiously throbbing music, luxuriously costumed groups of celebrants of all ages, King and Queen elections, electrifying jump-ups and torchlight parades, the Jouvert morning: the Children's Parades and finally the Grand Parade. Aruba's biggest celebration is a month-long affair consisting of festive 'jump-ups' (street parades), spectacular parades and creative contests. Music and flamboyant costumes play a central role, from the Queen elections to the Grand Parade. Street parades continue in various districts throughout the month, with brass band, steel drum and roadmarch tunes. On the evening before Lent, Carnival ends with the symbolic burning of King Momo.\"\n},\n{\n\"question\": \"How far does the Alps range stretch?\",\n\"answer\": \"the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland\",\n\"context\": \"The Alps are a crescent shaped geographic feature of central Europe that ranges in a 800 km (500 mi) arc from east to west and is 200 km (120 mi) in width. The mean height of the mountain peaks is 2.5 km (1.6 mi). The range stretches from the Mediterranean Sea north above the Po basin, extending through France from Grenoble, eastward through mid and southern Switzerland. The range continues toward Vienna in Austria, and east to the Adriatic Sea and into Slovenia. To the south it dips into northern Italy and to the north extends to the south border of Bavaria in Germany. In areas like Chiasso, Switzerland, and Neuschwanstein, Bavaria, the demarcation between the mountain range and the flatlands are clear; in other places such as Geneva, the demarcation is less clear. The countries with the greatest alpine territory are Switzerland, France, Austria and Italy.\"\n}\n],\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Context: {context}\\nQuestion: {question}\\nAnswer: {answer}\"\n}\n}\n</code></pre> The <code>task_type</code> sets up the config for a specific task, question_answering in this case.</p> <p>Take a look at the prompt section of the config. This defines the settings related to defining the task and the machinery around it.  </p> <p>The <code>task_guidelines</code> key is the most important key, it defines the task for the LLM to understand and execute on. In this case, we first set up the task and tell the model the kind of data present in the dataset, by telling it that it is an expert at understanding wikipedia articles. Next, we define the task more concretely by telling the model how to answer the question given the context. We tell the model that the answer is a continuous text span from the context and that in some cases, the answer can be unanswerable and how the model should handle such questions.  </p> <p>The <code>example_template</code> is one of the most important keys to set for a task. This defines the format of every example that will be sent to the LLM. This creates a prompt using the columns from the input dataset, and sends this prompt to the LLM hoping for the llm to generate the column defined under the <code>label_column</code>, which is answer in our case. For every input, the model will be given the example with all the columns from the datapoint filled in according to the specification in the <code>example_template</code>. The <code>label_column</code> will be empty, and the LLM will generate the label. The <code>example_template</code> will be used to format all seed examples. Here we also see the ordering of the context followed by question and answer, and also see the <code>Context:</code> string to inform the model which part of the text is the context.</p> <p>The <code>few_shot_examples</code> here is a list of json inputs which define handpicked examples to use as seed examples for the model. These labeled examples help the model understand the task better and how it supposed to answer a question. If there is a larger number of examples, we can specify a path to a csv instead of a list of examples.</p> <p><code>few_shot_num</code> defines the number of examples selected from the seed set and sent to the LLM. Experiment with this number based on the input token budget and performance degradation with longer inputs.</p> <p><code>few_shot_selection</code> is set to fixed in this case as we want to use all examples as seed examples. However, if we want to use a subset of examples as seed examples from a larger set, we can set the appropriate strategy like <code>semantic_similarity</code> here to get dynamic good seed examples.</p>"},{"location":"autolabel/guide/tasks/question_answering_task/#alternate-config-for-closedbook-qa","title":"Alternate config for ClosedBook QA","text":"<p>Let's consider a dataset like sciq which is a closed book QA with multiple choice questions. Here we have an example config for this dataset,</p> <pre><code>config = {\n\"task_name\": \"ClosedBookQAScienceQuestions\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n\"label_column\": \"answer\",\n\"delimiter\": \",\"\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\",\n\"params\": {}\n},\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at answering science questions. Choose an answer from the given options. Use your knowledge of science and common sense to best answer the question.\",\n\"few_shot_examples\": \"../examples/squad_v2/seed.csv\",\n\"few_shot_selection\": \"fixed\",\n\"few_shot_num\": 3,\n\"example_template\": \"Question: {question}\\nOptions: {options}\\nAnswer: {answer}\"\n}\n}\n</code></pre> <p>Notice in this case we don't have the <code>context</code> and pass in the <code>options</code> as list of string options. These are present in the dataset and are appropriately called in the example template.</p>"},{"location":"autolabel/guide/tasks/question_answering_task/#run-the-task","title":"Run the task","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = AutolabelDataset('data/squad_v2_test.csv', config = config)\nagent.plan(ds)\nagent.run(ds, max_items = 100)\n</code></pre>"},{"location":"autolabel/guide/tasks/question_answering_task/#evaluation-metrics","title":"Evaluation metrics","text":"<p>On running the above config, this is an example output expected for labeling 100 items. <pre><code>Actual Cost: 0.13500600000000001\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 f1                 \u2503 support \u2503 threshold \u2503 accuracy \u2503 completion_rate \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 0.7018720299348971 \u2502 100     \u2502 -inf      \u2502 0.59     \u2502 1.0             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre></p> <p>Accuracy - This is the exact match performance based on the reference answer. Here we give the model 1 if the answer matches exactly with the correct answer and 0 otherwise. This is particularly harsh for the model in cases where there isnt a multi choice given to the model for eg. Squad. Even if the model gets one word wrong without changing the meaning, the model will get penalized.</p> <p>F1 - This is calculated by treating the predicted and the ground truth tokens as a list of tokens. Using this, an F1 score is calculated for every examples. This score can then be averaged over the entire dataset to get the final score. An exact match would get an F1 score of 1. This metric allows the model to make small mistakes in the predicted tokens and might be a more accurate metric for cases where the answers are not restricted to a set of options.</p> <p>Completion Rate - There can be errors while running the LLM related to labeling for eg. the LLM may give a label which is not in the label list or provide an answer which is not parsable by the library. In this cases, we mark the example as not labeled successfully. The completion rate refers to the proportion of examples that were labeled successfully.</p>"},{"location":"autolabel/guide/transforms/image_transform/","title":"Image Transform","text":"<p>The image transform allows users to extract text from image files. Autolabel uses optical character recognition (OCR) to read the images. To use this transform, follow these steps:</p>"},{"location":"autolabel/guide/transforms/image_transform/#installation","title":"Installation","text":"<p>Use the following command to download all dependencies for the image transform.</p> <pre><code>pip install pillow pytesseract\n</code></pre> <p>The tesseract engine is also required for OCR text extraction. See the tesseract docs for installation instructions.</p>"},{"location":"autolabel/guide/transforms/image_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li>file_path_column: the name of the column containing the file paths of the pdf files to extract text from</li> <li>lang: a string indicating the language of the text in the pdf file. See the tesseract docs for a full list of supported languages</li> </ol>"},{"location":"autolabel/guide/transforms/image_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of an image transform to extract text from an image file:</p> <pre><code>{\n..., # other config parameters\n\"transforms\": [\n..., # other transforms\n{\n\"name\": \"image\",\n\"params\": {\n\"file_path_column\": \"file_path\",\n\"lang\": \"eng\"\n},\n\"output_columns\": {\n\"content_column\": \"content\",\n\"metadata_column\": \"metadata\"\n}\n}\n]\n}\n</code></pre>"},{"location":"autolabel/guide/transforms/image_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"autolabel/guide/transforms/introduction/","title":"Introduction","text":"<p>Autolabel supports transformation of the input data! Input datasets are available in many shapes and form(at)s. We help you ingest your data in the format that you want in a way that is most useful for the downstream LLM or labeling task that you have in mind. We have tried to make the transforms performant, configurable and the outputs formatted in a way useful for the LLM.</p>"},{"location":"autolabel/guide/transforms/introduction/#example","title":"Example","text":"<p>Here we will show you how to run an example transform. We will use the Webpage Transform to ingest national park websites and label the state that every national park belongs to. You can find a Jupyter notebook with code that you can run on your own here </p> <p>Use this webpage transform yourself here in a Colab - </p>"},{"location":"autolabel/guide/transforms/introduction/#changes-to-config","title":"Changes to config","text":"<pre><code>{\n\"task_name\": \"NationalPark\",\n\"task_type\": \"question_answering\",\n\"dataset\": {\n},\n\"model\": {\n\"provider\": \"openai\",\n\"name\": \"gpt-3.5-turbo\"\n},\n\"transforms\": [{\n\"name\": \"webpage_transform\",\n\"params\": {\n\"url_column\": \"url\"\n},\n\"output_columns\": {\n\"content_column\": \"content\"\n}\n}],\n\"prompt\": {\n\"task_guidelines\": \"You are an expert at understanding websites of national parks. You will be given a webpage about a national park. Answer with the US State that the national park is located in.\",\n\"output_guidelines\": \"Answer in one word the state that the national park is located in.\",\n\"example_template\": \"Content of wikipedia page: {content}\\State:\",\n}\n}\n</code></pre> <p>Notice the <code>transforms</code> key in the config. This is where we define our transforms. Notice that this is a list meaning we can define multiple transforms here. Every element of this list is a transform. A transform is a json requiring 3 inputs - 1. <code>name</code>: This tells the agent which transform needs to be loaded. Here we are using the webpage transform. 2. <code>params</code>: This is the set of parameters that will be passed to the transform. Read the documentation of the separate transform to see what params can be passed to the transform here. Here we pass the url_column, i.e the column containing the webpages that need to be loaded. 3. <code>output_columns</code>: Each transform can define multiple outputs. In this dictionary we map the output we need, in case <code>content_column</code> to the name of the column in the output dataset in which we want to populate this.</p>"},{"location":"autolabel/guide/transforms/introduction/#running-the-transform","title":"Running the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"autolabel/guide/transforms/introduction/#running-the-labeling-job","title":"Running the labeling job","text":"<pre><code>ds = agent.run(ds)\n</code></pre> <p>Simply run the labeling job on the transformed dataset. This will extract the state of the national park from each webpage.</p> <p> </p> Output of the transformation labeling run"},{"location":"autolabel/guide/transforms/introduction/#custom-transforms","title":"Custom Transforms","text":"<p>We support the following transforms -</p> <ol> <li>Webpage Transform</li> <li>PDF Transform</li> </ol> <p>We expect this list to grow in the future and need the help of the community to build transforms that work the best for their data. For this, we provide an abstraction that is easy to use. Any new transform just needs to be extend the <code>BaseTransform</code> class as penciled down below.</p> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform","title":"<code>BaseTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all transforms.</p> Source code in <code>autolabel/src/autolabel/transforms/base.py</code> <pre><code>class BaseTransform(ABC):\n\"\"\"Base class for all transforms.\"\"\"\nTTL_MS = 60 * 60 * 24 * 7 * 1000  # 1 week\nNULL_TRANSFORM_TOKEN = \"NO_TRANSFORM\"\ndef __init__(self, cache: BaseCache, output_columns: Dict[str, Any]) -&gt; None:\n\"\"\"\n        Initialize a transform.\n        Args:\n            cache: A cache object to use for caching the results of this transform.\n            output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.\n        \"\"\"\nsuper().__init__()\nself._output_columns = output_columns\nself.cache = cache\n@staticmethod\n@abstractmethod\ndef name() -&gt; str:\n\"\"\"\n        Returns the name of the transform.\n        \"\"\"\npass\n@property\ndef output_columns(self) -&gt; Dict[str, Any]:\n\"\"\"\n        Returns a dictionary of output columns. The keys are the names of the output columns\n        as expected by the transform. The values are the column names they should be mapped to in\n        the dataset.\n        \"\"\"\nreturn {k: self._output_columns.get(k, None) for k in self.COLUMN_NAMES}\n@property\ndef transform_error_column(self) -&gt; str:\n\"\"\"\n        Returns the name of the column that stores the error if transformation fails.\n        \"\"\"\nreturn f\"{self.name()}_error\"\n@abstractmethod\nasync def _apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n        Applies the transform to the given row.\n        Args:\n            row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values.\n        Returns:\n            A dictionary representing the transformed row. The keys are the column names and the values are the column values.\n        \"\"\"\npass\n@abstractmethod\ndef params(self) -&gt; Dict[str, Any]:\n\"\"\"\n        Returns a dictionary of parameters that can be used to uniquely identify this transform.\n        Returns:\n            A dictionary of parameters that can be used to uniquely identify this transform.\n        \"\"\"\nreturn {}\nasync def apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\nif self.cache is not None:\ncache_entry = TransformCacheEntry(\ntransform_name=self.name(),\ntransform_params=self.params(),\ninput=row,\nttl_ms=self.TTL_MS,\n)\noutput = self.cache.lookup(cache_entry)\nif output is not None:\n# Cache hit\nreturn output\ntry:\noutput = await self._apply(row)\nexcept Exception as e:\nlogger.error(f\"Error applying transform {self.name()}. Exception: {str(e)}\")\noutput = {\nk: self.NULL_TRANSFORM_TOKEN\nfor k in self.output_columns.values()\nif k is not None\n}\noutput[self.transform_error_column] = str(e)\nreturn output\nif self.cache is not None:\ncache_entry.output = output\nself.cache.update(cache_entry)\nreturn output\ndef _return_output_row(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n        Returns the output row with the correct column names.\n        Args:\n            row: The output row.\n        Returns:\n            The output row with the correct column names.\n        \"\"\"\n# remove null key\nrow.pop(None, None)\nreturn row\n</code></pre>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.output_columns","title":"<code>output_columns: Dict[str, Any]</code>  <code>property</code>","text":"<p>Returns a dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.</p>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.transform_error_column","title":"<code>transform_error_column: str</code>  <code>property</code>","text":"<p>Returns the name of the column that stores the error if transformation fails.</p>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.__init__","title":"<code>__init__(cache, output_columns)</code>","text":"<p>Initialize a transform. Args:     cache: A cache object to use for caching the results of this transform.     output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.</p> Source code in <code>autolabel/src/autolabel/transforms/base.py</code> <pre><code>def __init__(self, cache: BaseCache, output_columns: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Initialize a transform.\n    Args:\n        cache: A cache object to use for caching the results of this transform.\n        output_columns: A dictionary of output columns. The keys are the names of the output columns as expected by the transform. The values are the column names they should be mapped to in the dataset.\n    \"\"\"\nsuper().__init__()\nself._output_columns = output_columns\nself.cache = cache\n</code></pre>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.name","title":"<code>name()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Returns the name of the transform.</p> Source code in <code>autolabel/src/autolabel/transforms/base.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef name() -&gt; str:\n\"\"\"\n    Returns the name of the transform.\n    \"\"\"\npass\n</code></pre>"},{"location":"autolabel/guide/transforms/introduction/#src.autolabel.transforms.base.BaseTransform.params","title":"<code>params()</code>  <code>abstractmethod</code>","text":"<p>Returns a dictionary of parameters that can be used to uniquely identify this transform. Returns:     A dictionary of parameters that can be used to uniquely identify this transform.</p> Source code in <code>autolabel/src/autolabel/transforms/base.py</code> <pre><code>@abstractmethod\ndef params(self) -&gt; Dict[str, Any]:\n\"\"\"\n    Returns a dictionary of parameters that can be used to uniquely identify this transform.\n    Returns:\n        A dictionary of parameters that can be used to uniquely identify this transform.\n    \"\"\"\nreturn {}\n</code></pre>"},{"location":"autolabel/guide/transforms/introduction/#_apply-abstractmethod","title":"<code>_apply()</code> <code>abstractmethod</code>","text":"<p>Applies the transform to the given row. Args:     row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values. Returns:     A dictionary representing the transformed row. The keys are the column names and the values are the column values.</p> Source code in <code>autolabel/src/autolabel/transforms/base.py</code> <pre><code>@abstractmethod\nasync def _apply(self, row: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n    Applies the transform to the given row.\n    Args:\n        row: A dictionary representing a row in the dataset. The keys are the column names and the values are the column values.\n    Returns:\n        A dictionary representing the transformed row. The keys are the column names and the values are the column values.\n    \"\"\"\npass\n</code></pre>"},{"location":"autolabel/guide/transforms/pdf_transform/","title":"PDF Transform","text":"<p>The PDF transform allows users to extract text from pdf files. Autolabel offers both direct text extraction, useful for extracting text from pdfs that contain text, and optical character recognition (OCR) text extraction, useful for extracting text from pdfs that contain images. To use this transform, follow these steps:</p>"},{"location":"autolabel/guide/transforms/pdf_transform/#installation","title":"Installation","text":"<p>For direct text extraction, install the <code>pdfplumber</code> package:</p> <pre><code>pip install pdfplumber\n</code></pre> <p>For OCR text extraction, install the <code>pdf2image</code> and <code>pytesseract</code> packages:</p> <pre><code>pip install pdf2image pytesseract\n</code></pre> <p>The tesseract engine is also required for OCR text extraction. See the tesseract docs for installation instructions.</p>"},{"location":"autolabel/guide/transforms/pdf_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li>file_path_column: the name of the column containing the file paths of the pdf files to extract text from</li> <li>ocr_enabled: a boolean indicating whether to use OCR text extraction or not</li> <li>page_format: a string containing the format to use for each page of the pdf file. The following fields can be used in the format string: <ul> <li>page_num: the page number of the page</li> <li>page_content: the content of the page</li> </ul></li> <li>page_sep: a string containing the separator to use between each page of the pdf file</li> <li>lang: a string indicating the language of the text in the pdf file. See the [tesseract docs](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) for a full list of supported languages</li> </ol>"},{"location":"autolabel/guide/transforms/pdf_transform/#output-format","title":"Output Format","text":"<p>The page_format and page_sep parameters define how the text extracted from the pdf will be formatted. For example, if the pdf file contained 2 pages with \"Hello,\" on the first page and \"World!\" on the second, a page_format of <code>{page_num} - {page_content}</code> and a page_sep of <code>\\n</code> would result in the following output:</p> <pre><code>\"1 - Hello,\\n2 - World!\"\n</code></pre> <p>The metadata column contains a dict with the field \"num_pages\" indicating the number of pages in the pdf file.</p>"},{"location":"autolabel/guide/transforms/pdf_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of a pdf transform to extract text from a pdf file:</p> <pre><code>{\n..., # other config parameters\n\"transforms\": [\n..., # other transforms\n{\n\"name\": \"pdf\",\n\"params\": {\n\"file_path_column\": \"file_path\",\n\"ocr_enabled\": true,\n\"page_format\": \"Page {page_num}: {page_content}\",\n\"page_sep\": \"\\n\\n\"\n},\n\"output_columns\": {\n\"content_column\": \"content\",\n\"metadata_column\": \"metadata\"\n}\n}\n]\n}\n</code></pre>"},{"location":"autolabel/guide/transforms/pdf_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the correct column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"autolabel/guide/transforms/webpage_transform/","title":"Webpage Transform","text":"<p>The Webpage transform supports loading and processing webpage urls. Given a url, this transform will send the request to load the webpage and then parse the webpage returned to collect the text to send to the LLM.</p> <p>Use this transform yourself here in a Colab - </p> <p>In order to use this transform, use the following steps:</p>"},{"location":"autolabel/guide/transforms/webpage_transform/#installation","title":"Installation","text":"<p>Use the following command to download all dependencies for the webpage transform. <code>beautifulsoup4</code> must be version <code>4.12.2</code> or higher.</p> <pre><code>pip install beautifulsoup4 httpx fake_useragent\n</code></pre> <p>Make sure to do this before running the transform.</p>"},{"location":"autolabel/guide/transforms/webpage_transform/#parameters-for-this-transform","title":"Parameters for this transform","text":"<ol> <li><code>url_column: str (Required)</code>: The column to retrieve the url from. This is the webpage that will be loaded by the transform.</li> <li><code>timeout: int (Optional: Default = 5)</code>: The timeout to wait until for loading the webpage. The request to the webpage will timeout after this. We will log an error and send an empty response after the timeout is reached.</li> <li><code>headers: Dict[str,str] (Optional: Default = {})</code>: Any headers that need to be passed into the webpage load request. Underneath we use requests to get the webpage and the headers are passed to request.</li> </ol>"},{"location":"autolabel/guide/transforms/webpage_transform/#using-the-transform","title":"Using the transform","text":"<p>Below is an example of a webpage transform to extract text from a webpage:</p> <pre><code>{\n..., # other config parameters\n\"transforms\": [\n..., # other transforms\n{\n\"name\": \"webpage_transform\",\n\"params\": {\n\"url_column\": \"url\"\n},\n\"output_columns\": {\n\"content_column\": \"webpage_content\",\n}\n}\n]\n}\n</code></pre>"},{"location":"autolabel/guide/transforms/webpage_transform/#run-the-transform","title":"Run the transform","text":"<pre><code>from autolabel import LabelingAgent, AutolabelDataset\nagent = LabelingAgent(config)\nds = agent.transform(ds)\n</code></pre> <p>This runs the transformation. We will see the content in the webpage_content column. Access this using <code>ds.df</code> in the AutolabelDataset.</p>"},{"location":"autolabel/reference/cache/","title":"Cache","text":"<p>               Bases: <code>ABC</code></p> <p>used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.</p> Source code in <code>autolabel/src/autolabel/cache/base.py</code> <pre><code>class BaseCache(ABC):\n\"\"\"used to store AutoLabeling results, allowing for interrupted labeling runs to be continued from a save point without the need to restart from the beginning. Any custom Cache classes should extend from BaseCache.\"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__()\n@abstractmethod\ndef initialize():\n\"\"\"initialize the cache. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n@abstractmethod\ndef lookup(self, entry):\n\"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n@abstractmethod\ndef update(self, entry):\n\"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n@abstractmethod\ndef clear(self) -&gt; None:\n\"\"\"abstract method to clear the cache. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>               Bases: <code>BaseCache</code></p> <p>A cache system implemented with SQL Alchemy</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>class SQLAlchemyGenerationCache(BaseCache):\n\"\"\"A cache system implemented with SQL Alchemy\"\"\"\ndef __init__(self):\nself.engine = None\nself.base = Base\nself.session = None\ndef initialize(self):\nself.engine = create_db_engine()\nself.base.metadata.create_all(self.engine)\nself.session = sessionmaker(bind=self.engine)()\ndef lookup(\nself, entry: GenerationCacheEntry\n) -&gt; List[Union[Generation, ChatGeneration]]:\n\"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n        Args:\n            entry: GenerationCacheEntry we wish to retrieve from the Cache\n        Returns:\n            result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.\n        \"\"\"\ncache_entry = GenerationCacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nlogger.debug(\"Cache miss\")\nreturn []\nlogger.debug(\"Cache hit\")\nreturn cache_entry.generations\ndef update(self, entry: GenerationCacheEntry) -&gt; None:\n\"\"\"Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists\n        Args:\n            entry: GenerationCacheEntry we wish to put into the Cache\n        \"\"\"\nGenerationCacheEntryModel.insert(self.session, entry)\ndef clear(self) -&gt; None:\n\"\"\"Clears the entire Cache\"\"\"\nGenerationCacheEntryModel.clear(self.session)\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p> <p>               Bases: <code>BaseCache</code></p> <p>A cache system implemented with SQL Alchemy for storing the output of transforms. This cache system is used to avoid re-computing the output of transforms that have already been computed. This currently stores the input and the outputs of the transform. Caching is based on the transform name, params and input.</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>class SQLAlchemyTransformCache(BaseCache):\n\"\"\"\n    A cache system implemented with SQL Alchemy for storing the output of transforms.\n    This cache system is used to avoid re-computing the output of transforms that have already been computed.\n    This currently stores the input and the outputs of the transform.\n    Caching is based on the transform name, params and input.\n    \"\"\"\ndef __init__(self):\nself.engine = None\nself.base = Base\nself.session = None\ndef initialize(self):\nself.engine = create_db_engine()\nself.base.metadata.create_all(self.engine)\nself.session = sessionmaker(bind=self.engine)()\ndef lookup(self, entry: TransformCacheEntry) -&gt; Optional[Dict[str, Any]]:\n\"\"\"Retrieves an entry from the Cache. Returns None if not found.\n        Args:\n            entry: TransformCacheEntry we wish to retrieve from the Cache\n        Returns:\n            result: The output of the transform for this input. None if not found.\n        \"\"\"\ncache_entry = TransformCacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nreturn None\nreturn cache_entry.output\ndef update(self, entry: TransformCacheEntry) -&gt; None:\n\"\"\"Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists\n        Args:\n            entry: TransformCacheEntry we wish to put into the Cache\n        \"\"\"\nTransformCacheEntryModel.insert(self.session, entry)\ndef clear(self, use_ttl: bool = True) -&gt; None:\n\"\"\"Clears the entire Cache based on ttl\"\"\"\nTransformCacheEntryModel.clear(self.session, use_ttl=use_ttl)\n</code></pre> <p>rendering: show_root_heading: yes show_root_full_path: no</p>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.base.BaseCache.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>abstract method to clear the cache. Must be implemented by classes derived from BaseCache.</p> Source code in <code>autolabel/src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n\"\"\"abstract method to clear the cache. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.base.BaseCache.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>initialize the cache. Must be implemented by classes derived from BaseCache.</p> Source code in <code>autolabel/src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef initialize():\n\"\"\"initialize the cache. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.base.BaseCache.lookup","title":"<code>lookup(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>autolabel/src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef lookup(self, entry):\n\"\"\"abstract method to retrieve a cached entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.base.BaseCache.update","title":"<code>update(entry)</code>  <code>abstractmethod</code>","text":"<p>abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.</p> Source code in <code>autolabel/src/autolabel/cache/base.py</code> <pre><code>@abstractmethod\ndef update(self, entry):\n\"\"\"abstract method to update the cache with a new entry. Must be implemented by classes derived from BaseCache.\"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.clear","title":"<code>clear()</code>","text":"<p>Clears the entire Cache</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clears the entire Cache\"\"\"\nGenerationCacheEntryModel.clear(self.session)\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.lookup","title":"<code>lookup(entry)</code>","text":"<p>Retrieves an entry from the Cache. Returns an empty list [] if not found. Args:     entry: GenerationCacheEntry we wish to retrieve from the Cache Returns:     result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def lookup(\nself, entry: GenerationCacheEntry\n) -&gt; List[Union[Generation, ChatGeneration]]:\n\"\"\"Retrieves an entry from the Cache. Returns an empty list [] if not found.\n    Args:\n        entry: GenerationCacheEntry we wish to retrieve from the Cache\n    Returns:\n        result: A list of langchain Generation objects, containing the results of the labeling run for this GenerationCacheEntry. Empty list [] if not found.\n    \"\"\"\ncache_entry = GenerationCacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nlogger.debug(\"Cache miss\")\nreturn []\nlogger.debug(\"Cache hit\")\nreturn cache_entry.generations\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_generation_cache.SQLAlchemyGenerationCache.update","title":"<code>update(entry)</code>","text":"<p>Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists Args:     entry: GenerationCacheEntry we wish to put into the Cache</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_generation_cache.py</code> <pre><code>def update(self, entry: GenerationCacheEntry) -&gt; None:\n\"\"\"Inserts the provided GenerationCacheEntry into the Cache, overriding it if it already exists\n    Args:\n        entry: GenerationCacheEntry we wish to put into the Cache\n    \"\"\"\nGenerationCacheEntryModel.insert(self.session, entry)\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.clear","title":"<code>clear(use_ttl=True)</code>","text":"<p>Clears the entire Cache based on ttl</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def clear(self, use_ttl: bool = True) -&gt; None:\n\"\"\"Clears the entire Cache based on ttl\"\"\"\nTransformCacheEntryModel.clear(self.session, use_ttl=use_ttl)\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.lookup","title":"<code>lookup(entry)</code>","text":"<p>Retrieves an entry from the Cache. Returns None if not found. Args:     entry: TransformCacheEntry we wish to retrieve from the Cache Returns:     result: The output of the transform for this input. None if not found.</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def lookup(self, entry: TransformCacheEntry) -&gt; Optional[Dict[str, Any]]:\n\"\"\"Retrieves an entry from the Cache. Returns None if not found.\n    Args:\n        entry: TransformCacheEntry we wish to retrieve from the Cache\n    Returns:\n        result: The output of the transform for this input. None if not found.\n    \"\"\"\ncache_entry = TransformCacheEntryModel.get(self.session, entry)\nif cache_entry is None:\nreturn None\nreturn cache_entry.output\n</code></pre>"},{"location":"autolabel/reference/cache/#src.autolabel.cache.sqlalchemy_transform_cache.SQLAlchemyTransformCache.update","title":"<code>update(entry)</code>","text":"<p>Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists Args:     entry: TransformCacheEntry we wish to put into the Cache</p> Source code in <code>autolabel/src/autolabel/cache/sqlalchemy_transform_cache.py</code> <pre><code>def update(self, entry: TransformCacheEntry) -&gt; None:\n\"\"\"Inserts the provided TransformCacheEntry into the Cache, overriding it if it already exists\n    Args:\n        entry: TransformCacheEntry we wish to put into the Cache\n    \"\"\"\nTransformCacheEntryModel.insert(self.session, entry)\n</code></pre>"},{"location":"autolabel/reference/configs/","title":"Config","text":""},{"location":"autolabel/reference/configs/#src.autolabel.configs.base.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.</p> Source code in <code>autolabel/src/autolabel/configs/base.py</code> <pre><code>class BaseConfig:\n\"\"\"Used for parsing, validating, and storing information about the labeling task passed to the LabelingAgent. Additional config classes should extend from this base class.\"\"\"\ndef __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\nif isinstance(config, str):\nself.config = self._safe_load_json(config)\nelse:\nself.config = config\nif validate:\nself._validate()\ndef _safe_load_json(self, json_file_path: str) -&gt; Dict:\n\"\"\"Loads config settings from a provided json file\"\"\"\ntry:\nwith open(json_file_path, \"r\") as config_file:\nreturn json.load(config_file)\nexcept ValueError as e:\nlogger.error(\nf\"JSON file: {json_file_path} not loaded successfully. Error: {repr(e)}\"\n)\nreturn {}\ndef get(self, key: str, default_value: Any = None) -&gt; Any:\nreturn self.config.get(key, default_value)\ndef keys(self) -&gt; List:\nreturn list(self.config.keys())\ndef __getitem__(self, key):\nreturn self.config[key]\ndef to_json(self) -&gt; str:\n\"\"\"Returns the BaseConfig object in JSON format\"\"\"\nreturn json.dumps(self.config, sort_keys=True)\ndef __str__(self):\nreturn self.to_json()\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.base.BaseConfig.to_json","title":"<code>to_json()</code>","text":"<p>Returns the BaseConfig object in JSON format</p> Source code in <code>autolabel/src/autolabel/configs/base.py</code> <pre><code>def to_json(self) -&gt; str:\n\"\"\"Returns the BaseConfig object in JSON format\"\"\"\nreturn json.dumps(self.config, sort_keys=True)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig","title":"<code>AutolabelConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Class to parse and store configs passed to Autolabel agent.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>class AutolabelConfig(BaseConfig):\n\"\"\"Class to parse and store configs passed to Autolabel agent.\"\"\"\n# Top-level config keys\nTASK_NAME_KEY = \"task_name\"\nTASK_TYPE_KEY = \"task_type\"\nDATASET_CONFIG_KEY = \"dataset\"\nMODEL_CONFIG_KEY = \"model\"\nEMBEDDING_CONFIG_KEY = \"embedding\"\nPROMPT_CONFIG_KEY = \"prompt\"\nDATASET_GENERATION_CONFIG_KEY = \"dataset_generation\"\nCHUNKING_CONFIG_KEY = \"chunking\"\n# Dataset config keys (config[\"dataset\"][&lt;key&gt;])\nLABEL_COLUMN_KEY = \"label_column\"\nLABEL_SEPARATOR_KEY = \"label_separator\"\nEXPLANATION_COLUMN_KEY = \"explanation_column\"\nIMAGE_COLUMN_KEY = \"image_url_column\"\nTEXT_COLUMN_KEY = \"text_column\"\nINPUT_COLUMNS_KEY = \"input_columns\"\nDELIMITER_KEY = \"delimiter\"\nDISABLE_QUOTING = \"disable_quoting\"\n# Model config keys (config[\"model\"][&lt;key&gt;])\nPROVIDER_KEY = \"provider\"\nMODEL_NAME_KEY = \"name\"\nMODEL_PARAMS_KEY = \"params\"\nCOMPUTE_CONFIDENCE_KEY = \"compute_confidence\"\nLOGIT_BIAS_KEY = \"logit_bias\"\nJSON_MODE = \"json_mode\"\n# Embedding config keys (config[\"embedding\"][&lt;key&gt;])\nEMBEDDING_PROVIDER_KEY = \"provider\"\nEMBEDDING_MODEL_NAME_KEY = \"model\"\n# Prompt config keys (config[\"prompt\"][&lt;key&gt;])\nTASK_GUIDELINE_KEY = \"task_guidelines\"\nVALID_LABELS_KEY = \"labels\"\nFEW_SHOT_EXAMPLE_SET_KEY = \"few_shot_examples\"\nFEW_SHOT_SELECTION_ALGORITHM_KEY = \"few_shot_selection\"\nFEW_SHOT_NUM_KEY = \"few_shot_num\"\nVECTOR_STORE_PARAMS_KEY = \"vector_store_params\"\nEXAMPLE_TEMPLATE_KEY = \"example_template\"\nOUTPUT_GUIDELINE_KEY = \"output_guidelines\"\nOUTPUT_FORMAT_KEY = \"output_format\"\nCHAIN_OF_THOUGHT_KEY = \"chain_of_thought\"\nLABEL_SELECTION_KEY = \"label_selection\"\nLABEL_SELECTION_COUNT_KEY = \"label_selection_count\"\nLABEL_SELECTION_THRESHOLD = \"label_selection_threshold\"\nATTRIBUTES_KEY = \"attributes\"\nTRANSFORM_KEY = \"transforms\"\n# Dataset generation config keys (config[\"dataset_generation\"][&lt;key&gt;])\nDATASET_GENERATION_GUIDELINES_KEY = \"guidelines\"\nDATASET_GENERATION_NUM_ROWS_KEY = \"num_rows\"\n# Chunking config keys (config[\"chunking\"][&lt;key&gt;])\nCONFIDENCE_CHUNK_COLUMN_KEY = \"confidence_chunk_column\"\nCONFIDENCE_CHUNK_SIZE_KEY = \"confidence_chunk_size\"\nCONFIDENCE_MERGE_FUNCTION_KEY = \"confidence_merge_function\"\ndef __init__(self, config: Union[str, Dict], validate: bool = True) -&gt; None:\nsuper().__init__(config, validate=validate)\ndef _validate(self) -&gt; bool:\n\"\"\"Returns true if the config settings are valid\"\"\"\nfrom autolabel.configs.schema import schema\nvalidate(\ninstance=self.config,\nschema=schema,\n)\nreturn True\n@cached_property\ndef _dataset_config(self) -&gt; Dict:\n\"\"\"Returns information about the dataset being used for labeling (e.g. label_column, text_column, delimiter)\"\"\"\nreturn self.config.get(self.DATASET_CONFIG_KEY, {})\n@cached_property\ndef _model_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for labeling (e.g. provider name, model name, parameters)\"\"\"\nreturn self.config[self.MODEL_CONFIG_KEY]\n@cached_property\ndef _embedding_config(self) -&gt; Dict:\n\"\"\"Returns information about the model being used for computing embeddings (e.g. provider name, model name)\"\"\"\nreturn self.config.get(self.EMBEDDING_CONFIG_KEY, {})\n@cached_property\ndef _prompt_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt we are passing to the model (e.g. task guidelines, examples, output formatting)\"\"\"\nreturn self.config[self.PROMPT_CONFIG_KEY]\n@cached_property\ndef _dataset_generation_config(self) -&gt; Dict:\n\"\"\"Returns information about the prompt for synthetic dataset generation\"\"\"\nreturn self.config.get(self.DATASET_GENERATION_CONFIG_KEY, {})\n@cached_property\ndef _chunking_config(self) -&gt; Dict:\n\"\"\"Returns information about the chunking config\"\"\"\nreturn self.config.get(self.CHUNKING_CONFIG_KEY, {})\n# project and task definition config\ndef task_name(self) -&gt; str:\nreturn self.config[self.TASK_NAME_KEY]\ndef task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n# Dataset config\ndef label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\ndef label_separator(self) -&gt; str:\n\"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\nreturn self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\ndef text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\ndef input_columns(self) -&gt; List[str]:\n\"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\nreturn self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\ndef explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\ndef image_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an image url for the given item\"\"\"\nreturn self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\ndef delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\ndef disable_quoting(self) -&gt; bool:\n\"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\nreturn self._dataset_config.get(self.DISABLE_QUOTING, False)\n# Model config\ndef provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\ndef model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\ndef model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\ndef confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\ndef logit_bias(self) -&gt; float:\n\"\"\"Returns the logit bias for the labels specified in the config\"\"\"\nreturn self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n# Embedding config\ndef embedding_provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\ndef embedding_model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n# Prompt config\ndef task_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.TASK_GUIDELINE_KEY, \"\")\ndef labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\nelse:\nreturn list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\ndef label_descriptions(self) -&gt; Dict[str, str]:\n\"\"\"Returns a dict of label descriptions\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn {}\nelse:\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, {})\ndef few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\ndef few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\ndef few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\ndef vector_store_params(self) -&gt; Dict:\n\"\"\"Returns any parameters to be passed to the vector store\"\"\"\nreturn self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\ndef example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\ndef output_format(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_FORMAT_KEY, None)\ndef output_guidelines(self) -&gt; str:\nreturn self._prompt_config.get(self.OUTPUT_GUIDELINE_KEY, None)\ndef chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\ndef label_selection(self) -&gt; bool:\n\"\"\"Returns true if label selection is enabled. Label selection is the process of\n        narrowing down the list of possible labels by similarity to a given input. Useful for\n        classification tasks with a large number of possible classes.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\ndef max_selected_labels(self) -&gt; int:\n\"\"\"Returns the number of labels to select in LabelSelector\"\"\"\nk = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\nif k &lt; 1:\nreturn len(self.labels_list())\nreturn k\ndef label_selection_threshold(self) -&gt; float:\n\"\"\"Returns the threshold for label selection in LabelSelector\n        If the similarity score ratio with the top Score is above this threshold,\n        the label is selected.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\ndef attributes(self) -&gt; List[Dict]:\n\"\"\"Returns a list of attributes to extract from the text.\"\"\"\nreturn self._prompt_config.get(self.ATTRIBUTES_KEY, [])\ndef transforms(self) -&gt; List[Dict]:\n\"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\nreturn self.config.get(self.TRANSFORM_KEY, [])\ndef dataset_generation_guidelines(self) -&gt; str:\n\"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n)\ndef dataset_generation_num_rows(self) -&gt; int:\n\"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_NUM_ROWS_KEY, 1\n)\ndef confidence_chunk_column(self) -&gt; str:\n\"\"\"Returns the column name to use for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\ndef confidence_chunk_size(self) -&gt; int:\n\"\"\"Returns the chunk size for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\ndef confidence_merge_function(self) -&gt; str:\n\"\"\"Returns the function to use when merging confidence scores\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\ndef json_mode(self) -&gt; bool:\n\"\"\"Returns true if the model should be used in json mode. Currently only used for OpenAI models.\"\"\"\nreturn self._model_config.get(self.JSON_MODE, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.attributes","title":"<code>attributes()</code>","text":"<p>Returns a list of attributes to extract from the text.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def attributes(self) -&gt; List[Dict]:\n\"\"\"Returns a list of attributes to extract from the text.\"\"\"\nreturn self._prompt_config.get(self.ATTRIBUTES_KEY, [])\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.chain_of_thought","title":"<code>chain_of_thought()</code>","text":"<p>Returns true if the model is able to perform chain of thought reasoning.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def chain_of_thought(self) -&gt; bool:\n\"\"\"Returns true if the model is able to perform chain of thought reasoning.\"\"\"\nreturn self._prompt_config.get(self.CHAIN_OF_THOUGHT_KEY, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence","title":"<code>confidence()</code>","text":"<p>Returns true if the model is able to return a confidence score along with its predictions</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence(self) -&gt; bool:\n\"\"\"Returns true if the model is able to return a confidence score along with its predictions\"\"\"\nreturn self._model_config.get(self.COMPUTE_CONFIDENCE_KEY, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_column","title":"<code>confidence_chunk_column()</code>","text":"<p>Returns the column name to use for confidence chunking</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_column(self) -&gt; str:\n\"\"\"Returns the column name to use for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_COLUMN_KEY)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_chunk_size","title":"<code>confidence_chunk_size()</code>","text":"<p>Returns the chunk size for confidence chunking</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_chunk_size(self) -&gt; int:\n\"\"\"Returns the chunk size for confidence chunking\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_CHUNK_SIZE_KEY, 3400)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.confidence_merge_function","title":"<code>confidence_merge_function()</code>","text":"<p>Returns the function to use when merging confidence scores</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def confidence_merge_function(self) -&gt; str:\n\"\"\"Returns the function to use when merging confidence scores\"\"\"\nreturn self._chunking_config.get(self.CONFIDENCE_MERGE_FUNCTION_KEY, \"max\")\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_guidelines","title":"<code>dataset_generation_guidelines()</code>","text":"<p>Returns a string containing guidelines for how to generate a synthetic dataset</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_guidelines(self) -&gt; str:\n\"\"\"Returns a string containing guidelines for how to generate a synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_GUIDELINES_KEY, \"\"\n)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.dataset_generation_num_rows","title":"<code>dataset_generation_num_rows()</code>","text":"<p>Returns the number of rows to generate for the synthetic dataset</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def dataset_generation_num_rows(self) -&gt; int:\n\"\"\"Returns the number of rows to generate for the synthetic dataset\"\"\"\nreturn self._dataset_generation_config.get(\nself.DATASET_GENERATION_NUM_ROWS_KEY, 1\n)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.delimiter","title":"<code>delimiter()</code>","text":"<p>Returns the token used to seperate cells in the dataset. Defaults to a comma ','</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def delimiter(self) -&gt; str:\n\"\"\"Returns the token used to seperate cells in the dataset. Defaults to a comma ','\"\"\"\nreturn self._dataset_config.get(self.DELIMITER_KEY, \",\")\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.disable_quoting","title":"<code>disable_quoting()</code>","text":"<p>Returns true if quoting is disabled. Defaults to false</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def disable_quoting(self) -&gt; bool:\n\"\"\"Returns true if quoting is disabled. Defaults to false\"\"\"\nreturn self._dataset_config.get(self.DISABLE_QUOTING, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.embedding_model_name","title":"<code>embedding_model_name()</code>","text":"<p>Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def embedding_model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for computing embeddings (e.g. sentence-transformers/all-mpnet-base-v2)\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_MODEL_NAME_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.embedding_provider","title":"<code>embedding_provider()</code>","text":"<p>Returns the name of the entity that provides the model used for computing embeddings</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def embedding_provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the model used for computing embeddings\"\"\"\nreturn self._embedding_config.get(self.EMBEDDING_PROVIDER_KEY, self.provider())\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.example_template","title":"<code>example_template()</code>","text":"<p>Returns a string containing a template for how examples will be formatted in the prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def example_template(self) -&gt; str:\n\"\"\"Returns a string containing a template for how examples will be formatted in the prompt\"\"\"\nexample_template = self._prompt_config.get(self.EXAMPLE_TEMPLATE_KEY, None)\nif not example_template:\nraise ValueError(\"An example template needs to be specified in the config.\")\nreturn example_template\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.explanation_column","title":"<code>explanation_column()</code>","text":"<p>Returns the name of the column containing an explanation as to why the data is labeled a certain way</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def explanation_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an explanation as to why the data is labeled a certain way\"\"\"\nreturn self._dataset_config.get(self.EXPLANATION_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_algorithm","title":"<code>few_shot_algorithm()</code>","text":"<p>Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_algorithm(self) -&gt; str:\n\"\"\"Returns which algorithm is being used to construct the set of examples being given to the model about the labeling task\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_SELECTION_ALGORITHM_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_example_set","title":"<code>few_shot_example_set()</code>","text":"<p>Returns examples of how data should be labeled, used to guide context to the model about the task it is performing</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_example_set(self) -&gt; Union[str, List]:\n\"\"\"Returns examples of how data should be labeled, used to guide context to the model about the task it is performing\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_EXAMPLE_SET_KEY, [])\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.few_shot_num_examples","title":"<code>few_shot_num_examples()</code>","text":"<p>Returns how many examples should be given to the model in its instruction prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def few_shot_num_examples(self) -&gt; int:\n\"\"\"Returns how many examples should be given to the model in its instruction prompt\"\"\"\nreturn self._prompt_config.get(self.FEW_SHOT_NUM_KEY, 0)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.image_column","title":"<code>image_column()</code>","text":"<p>Returns the name of the column containing an image url for the given item</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def image_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing an image url for the given item\"\"\"\nreturn self._dataset_config.get(self.IMAGE_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.input_columns","title":"<code>input_columns()</code>","text":"<p>Returns the names of the input columns from the dataset that are used in the prompt</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def input_columns(self) -&gt; List[str]:\n\"\"\"Returns the names of the input columns from the dataset that are used in the prompt\"\"\"\nreturn self._dataset_config.get(self.INPUT_COLUMNS_KEY, [])\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.json_mode","title":"<code>json_mode()</code>","text":"<p>Returns true if the model should be used in json mode. Currently only used for OpenAI models.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def json_mode(self) -&gt; bool:\n\"\"\"Returns true if the model should be used in json mode. Currently only used for OpenAI models.\"\"\"\nreturn self._model_config.get(self.JSON_MODE, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_column","title":"<code>label_column()</code>","text":"<p>Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing labels for the dataset. Used for comparing accuracy of autolabel results vs ground truth\"\"\"\nreturn self._dataset_config.get(self.LABEL_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_descriptions","title":"<code>label_descriptions()</code>","text":"<p>Returns a dict of label descriptions</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_descriptions(self) -&gt; Dict[str, str]:\n\"\"\"Returns a dict of label descriptions\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn {}\nelse:\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, {})\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_selection","title":"<code>label_selection()</code>","text":"<p>Returns true if label selection is enabled. Label selection is the process of narrowing down the list of possible labels by similarity to a given input. Useful for classification tasks with a large number of possible classes.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_selection(self) -&gt; bool:\n\"\"\"Returns true if label selection is enabled. Label selection is the process of\n    narrowing down the list of possible labels by similarity to a given input. Useful for\n    classification tasks with a large number of possible classes.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_KEY, False)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_selection_threshold","title":"<code>label_selection_threshold()</code>","text":"<p>Returns the threshold for label selection in LabelSelector If the similarity score ratio with the top Score is above this threshold, the label is selected.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_selection_threshold(self) -&gt; float:\n\"\"\"Returns the threshold for label selection in LabelSelector\n    If the similarity score ratio with the top Score is above this threshold,\n    the label is selected.\"\"\"\nreturn self._prompt_config.get(self.LABEL_SELECTION_THRESHOLD, 0.0)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.label_separator","title":"<code>label_separator()</code>","text":"<p>Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def label_separator(self) -&gt; str:\n\"\"\"Returns the token used to seperate multiple labels in the dataset. Defaults to a semicolon ';'\"\"\"\nreturn self._dataset_config.get(self.LABEL_SEPARATOR_KEY, \";\")\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.labels_list","title":"<code>labels_list()</code>","text":"<p>Returns a list of valid labels</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def labels_list(self) -&gt; List[str]:\n\"\"\"Returns a list of valid labels\"\"\"\nif isinstance(self._prompt_config.get(self.VALID_LABELS_KEY, []), List):\nreturn self._prompt_config.get(self.VALID_LABELS_KEY, [])\nelse:\nreturn list(self._prompt_config.get(self.VALID_LABELS_KEY, {}).keys())\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.logit_bias","title":"<code>logit_bias()</code>","text":"<p>Returns the logit bias for the labels specified in the config</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def logit_bias(self) -&gt; float:\n\"\"\"Returns the logit bias for the labels specified in the config\"\"\"\nreturn self._model_config.get(self.LOGIT_BIAS_KEY, 0.0)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.max_selected_labels","title":"<code>max_selected_labels()</code>","text":"<p>Returns the number of labels to select in LabelSelector</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def max_selected_labels(self) -&gt; int:\n\"\"\"Returns the number of labels to select in LabelSelector\"\"\"\nk = self._prompt_config.get(self.LABEL_SELECTION_COUNT_KEY, 10)\nif k &lt; 1:\nreturn len(self.labels_list())\nreturn k\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_name","title":"<code>model_name()</code>","text":"<p>Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def model_name(self) -&gt; str:\n\"\"\"Returns the name of the model being used for labeling (e.g. gpt-4, claude-v1)\"\"\"\nreturn self._model_config[self.MODEL_NAME_KEY]\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.model_params","title":"<code>model_params()</code>","text":"<p>Returns a dict of configured settings for the model (e.g. hyperparameters)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def model_params(self) -&gt; Dict:\n\"\"\"Returns a dict of configured settings for the model (e.g. hyperparameters)\"\"\"\nreturn self._model_config.get(self.MODEL_PARAMS_KEY, {})\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.provider","title":"<code>provider()</code>","text":"<p>Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def provider(self) -&gt; str:\n\"\"\"Returns the name of the entity that provides the currently configured model (e.g. OpenAI, Anthropic, Refuel)\"\"\"\nreturn self._model_config[self.PROVIDER_KEY]\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.task_type","title":"<code>task_type()</code>","text":"<p>Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def task_type(self) -&gt; str:\n\"\"\"Returns the type of task we have configured the labeler to perform (e.g. Classification, Question Answering)\"\"\"\nreturn self.config[self.TASK_TYPE_KEY]\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.text_column","title":"<code>text_column()</code>","text":"<p>Returns the name of the column containing text data we intend to label</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def text_column(self) -&gt; str:\n\"\"\"Returns the name of the column containing text data we intend to label\"\"\"\nreturn self._dataset_config.get(self.TEXT_COLUMN_KEY, None)\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.transforms","title":"<code>transforms()</code>","text":"<p>Returns a list of transforms to apply to the data before sending to the model.</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def transforms(self) -&gt; List[Dict]:\n\"\"\"Returns a list of transforms to apply to the data before sending to the model.\"\"\"\nreturn self.config.get(self.TRANSFORM_KEY, [])\n</code></pre>"},{"location":"autolabel/reference/configs/#src.autolabel.configs.config.AutolabelConfig.vector_store_params","title":"<code>vector_store_params()</code>","text":"<p>Returns any parameters to be passed to the vector store</p> Source code in <code>autolabel/src/autolabel/configs/config.py</code> <pre><code>def vector_store_params(self) -&gt; Dict:\n\"\"\"Returns any parameters to be passed to the vector store\"\"\"\nreturn self._prompt_config.get(self.VECTOR_STORE_PARAMS_KEY, {})\n</code></pre>"},{"location":"autolabel/reference/example_select/","title":"Example Selector","text":"<p>               Bases: <code>BaseExampleSelector</code>, <code>BaseModel</code></p> <p>Example selector to handle the case of fixed few-shot context i.e. every input prompt to the labeling model has the same few-shot examples</p> Source code in <code>autolabel/src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class FixedExampleSelector(BaseExampleSelector, BaseModel):\n\"\"\"Example selector to handle the case of fixed few-shot context\n    i.e. every input prompt to the labeling model has the same few-shot examples\n    \"\"\"\nexamples: List[dict]\n\"\"\"A list of the examples that the prompt template expects.\"\"\"\nk: int = 4\n\"\"\"Number of examples to select\"\"\"\nclass Config:\n\"\"\"Configuration for this pydantic object.\"\"\"\nextra = Extra.forbid\narbitrary_types_allowed = True\ndef add_example(self, example: Dict[str, str]) -&gt; None:\nself.examples.append(example)\ndef select_examples(\nself,\ninput_variables: Dict[str, str],\n**kwargs,\n) -&gt; List[dict]:\n\"\"\"Select which examples to use based on the input lengths.\"\"\"\nlabel_column = kwargs.get(\"label_column\")\nselected_labels = kwargs.get(\"selected_labels\")\nif not selected_labels:\nreturn self.examples[: self.k]\nif not label_column:\nprint(\"No label column provided, returning all examples\")\nreturn self.examples[: self.k]\n# get the examples where label matches the selected labels\nvalid_examples = [\nexample\nfor example in self.examples\nif example.get(label_column) in selected_labels\n]\nreturn valid_examples[: min(self.k, len(valid_examples))]\n@classmethod\ndef from_examples(\ncls,\nexamples: List,\nk: int = 4,\n) -&gt; FixedExampleSelector:\n\"\"\"Create pass-through example selector using example list\n        Returns:\n            The FixedExampleSelector instantiated\n        \"\"\"\nreturn cls(examples=examples, k=k)\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.examples","title":"<code>examples: List[dict]</code>  <code>instance-attribute</code>","text":"<p>A list of the examples that the prompt template expects.</p>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.k","title":"<code>k: int = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of examples to select</p>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.Config","title":"<code>Config</code>","text":"<p>Configuration for this pydantic object.</p> Source code in <code>autolabel/src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>class Config:\n\"\"\"Configuration for this pydantic object.\"\"\"\nextra = Extra.forbid\narbitrary_types_allowed = True\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.from_examples","title":"<code>from_examples(examples, k=4)</code>  <code>classmethod</code>","text":"<p>Create pass-through example selector using example list</p> <p>Returns:</p> Type Description <code>FixedExampleSelector</code> <p>The FixedExampleSelector instantiated</p> Source code in <code>autolabel/src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>@classmethod\ndef from_examples(\ncls,\nexamples: List,\nk: int = 4,\n) -&gt; FixedExampleSelector:\n\"\"\"Create pass-through example selector using example list\n    Returns:\n        The FixedExampleSelector instantiated\n    \"\"\"\nreturn cls(examples=examples, k=k)\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.fixed_example_selector.FixedExampleSelector.select_examples","title":"<code>select_examples(input_variables, **kwargs)</code>","text":"<p>Select which examples to use based on the input lengths.</p> Source code in <code>autolabel/src/autolabel/few_shot/fixed_example_selector.py</code> <pre><code>def select_examples(\nself,\ninput_variables: Dict[str, str],\n**kwargs,\n) -&gt; List[dict]:\n\"\"\"Select which examples to use based on the input lengths.\"\"\"\nlabel_column = kwargs.get(\"label_column\")\nselected_labels = kwargs.get(\"selected_labels\")\nif not selected_labels:\nreturn self.examples[: self.k]\nif not label_column:\nprint(\"No label column provided, returning all examples\")\nreturn self.examples[: self.k]\n# get the examples where label matches the selected labels\nvalid_examples = [\nexample\nfor example in self.examples\nif example.get(label_column) in selected_labels\n]\nreturn valid_examples[: min(self.k, len(valid_examples))]\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper","title":"<code>VectorStoreWrapper</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>class VectorStoreWrapper(VectorStore):\ndef __init__(\nself,\nembedding_function: Optional[Embeddings] = None,\ncorpus_embeddings: Optional[Tensor] = None,\ntexts: Optional[List[str]] = None,\nmetadatas: Optional[List[Dict[str, str]]] = None,\ncache: bool = True,\n) -&gt; None:\nself._embedding_function = embedding_function\nself._corpus_embeddings = corpus_embeddings\nself._texts = texts\nself._metadatas = metadatas\nif cache:\nself._db_engine = create_db_engine()\nwith self._db_engine.connect() as conn:\nquery = f\"CREATE TABLE IF NOT EXISTS {EMBEDDINGS_TABLE} (embedding_function TEXT, text TEXT, embedding BLOB)\"\nconn.execute(sql_text(query))\nconn.commit()\nelse:\nself._db_engine = None\ndef _get_embeddings(self, texts: Iterable[str]) -&gt; List[List[float]]:\n\"\"\"Get embeddings from the database. If not found, compute them and add them to the database.\n        If no database is used, compute the embeddings and return them.\n        Args:\n            texts (Iterable[str]): Iterable of texts to embed.\n        Returns:\n            List[List[float]]: List of embeddings.\n        \"\"\"\nif self._db_engine:\nwith self._db_engine.connect() as conn:\nembeddings = []\nuncached_texts = []\nuncached_texts_indices = []\nfor idx, text in enumerate(texts):\nquery = sql_text(\nf\"SELECT embedding FROM {EMBEDDINGS_TABLE} WHERE embedding_function = :x AND text = :y\",\n)\nparams = {\n\"x\": (\nself._embedding_function.model\nif self._embedding_function.__class__.__name__\nnot in [\"HuggingFaceEmbeddings\", \"VertexAIEmbeddings\"]\nelse self._embedding_function.model_name\n),\n\"y\": text,\n}\nresult = conn.execute(query, params).fetchone()\nif result:\nembeddings.append(pickle.loads(result[0]))\nelse:\nembeddings.append(None)\nuncached_texts.append(text)\nuncached_texts_indices.append(idx)\nuncached_embeddings = self._embedding_function.embed_documents(\nuncached_texts\n)\nself._add_embeddings_to_cache(uncached_texts, uncached_embeddings)\nfor idx, embedding in zip(uncached_texts_indices, uncached_embeddings):\nembeddings[idx] = embedding\nreturn embeddings\nelse:\nreturn self._embedding_function.embed_documents(list(texts))\ndef _add_embeddings_to_cache(\nself, texts: Iterable[str], embeddings: List[List[float]]\n) -&gt; None:\n\"\"\"Save embeddings to the database. If self._db_engine is None, do nothing.\n        Args:\n            texts (Iterable[str]): Iterable of texts.\n            embeddings (List[List[float]]): List of embeddings.\n        \"\"\"\nif self._db_engine:\nwith self._db_engine.connect() as conn:\nfor text, embedding in zip(texts, embeddings):\nquery = sql_text(\nf\"INSERT INTO {EMBEDDINGS_TABLE} (embedding_function, text, embedding) VALUES (:x, :y, :z)\"\n)\nparams = {\n\"x\": (\nself._embedding_function.model\nif self._embedding_function.__class__.__name__\nnot in [\"HuggingFaceEmbeddings\", \"VertexAIEmbeddings\"]\nelse self._embedding_function.model_name\n),\n\"y\": text,\n\"z\": pickle.dumps(embedding),\n}\nconn.execute(query, params)\nconn.commit()\ndef add_texts(\nself,\ntexts: Iterable[str],\nmetadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; List[str]:\n\"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n        Args:\n            texts (Iterable[str]): Texts to add to the vectorstore.\n            metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\nif self._embedding_function is not None:\nembeddings = self._get_embeddings(texts)\nself._corpus_embeddings = torch.tensor(embeddings)\nself._texts = texts\nself._metadatas = metadatas\nreturn metadatas\ndef similarity_search(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\ndocs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\nreturn [doc for doc, _ in docs_and_scores]\ndef similarity_search_with_score(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to the query\n                text with distance in float.\n        \"\"\"\nquery_embeddings = torch.tensor([self._get_embeddings([query])[0]])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in result_ids]]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\nreturn _results_to_docs_and_scores(results)\ndef label_diversity_similarity_search(\nself,\nquery: str,\nlabel_key: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return per label.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\ndocs_and_scores = self.label_diversity_similarity_search_with_score(\nquery, label_key, k, filter=filter\n)\nreturn [doc for doc, _ in docs_and_scores]\ndef label_diversity_similarity_search_with_score(\nself,\nquery: str,\nlabel_key: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n        Args:\n            query (str): Query text to search for.\n            k (int): Number of results to return. Defaults to 4.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to the query\n                text with distance in float.\n        \"\"\"\nquery_embeddings = torch.tensor([self._get_embeddings([query])[0]])\ndata = []\ndata = zip(self._corpus_embeddings, self._texts, self._metadatas)\nsorted_data = sorted(data, key=lambda item: item[2].get(label_key))\ndocuments = []\nscores = []\nmetadatas = []\nfor label, label_examples in groupby(\nsorted_data, key=lambda item: item[2].get(label_key)\n):\nlabel_examples_list = list(label_examples)\nlabel_embeddings = list(\nmap(lambda label_example: label_example[0], label_examples_list)\n)\nlabel_texts = list(\nmap(lambda label_example: label_example[1], label_examples_list)\n)\nlabel_metadatas = list(\nmap(lambda label_example: label_example[2], label_examples_list)\n)\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=label_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\ndocuments.extend([label_texts[index] for index in result_ids])\nmetadatas.extend([label_metadatas[index] for index in result_ids])\nscores.extend([result[\"score\"] for result in result_ids_and_scores[0]])\nresults = {}\nresults[\"documents\"] = [documents]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [metadatas]\nreturn _results_to_docs_and_scores(results)\ndef max_marginal_relevance_search_by_vector(\nself,\nquery: str,\nk: int = 4,\nfetch_k: int = 20,\nlambda_mult: float = 0.5,\n**kwargs: Any,\n) -&gt; List[Document]:\nquery_embedding = self._get_embeddings([query])[0]\nquery_embeddings = torch.tensor([query_embedding])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=fetch_k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nfetched_embeddings = torch.index_select(\ninput=self._corpus_embeddings, dim=0, index=torch.tensor(result_ids)\n).tolist()\nmmr_selected = maximal_marginal_relevance(\nnp.array([query_embedding], dtype=np.float32),\nfetched_embeddings,\nk=k,\nlambda_mult=lambda_mult,\n)\nselected_result_ids = [result_ids[i] for i in mmr_selected]\nselected_scores = [scores[i] for i in mmr_selected]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in selected_result_ids]]\nresults[\"distances\"] = [selected_scores]\nresults[\"metadatas\"] = [\n[self._metadatas[index] for index in selected_result_ids]\n]\nreturn _results_to_docs_and_scores(results)\ndef max_marginal_relevance_search(\nself,\nquery: str,\nk: int = 4,\nfetch_k: int = 20,\nlambda_mult: float = 0.5,\n**kwargs: Any,\n) -&gt; List[Document]:\ndocs_and_scores = self.max_marginal_relevance_search_by_vector(\nquery, k, fetch_k, lambda_mult=lambda_mult\n)\nreturn [doc for doc, _ in docs_and_scores]\n@classmethod\ndef from_texts(\ncls: Type[VectorStoreWrapper],\ntexts: List[str],\nembedding: Optional[Embeddings] = None,\nmetadatas: Optional[List[dict]] = None,\ncache: bool = True,\n**kwargs: Any,\n) -&gt; VectorStoreWrapper:\n\"\"\"Create a vectorstore from raw text.\n        The data will be ephemeral in-memory.\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            cache (bool): Whether to cache the embeddings. Defaults to True.\n        Returns:\n            vector_store: Vectorstore with seedset embeddings\n        \"\"\"\nvector_store = cls(\nembedding_function=embedding,\ncorpus_embeddings=None,\ntexts=None,\ncache=cache,\n**kwargs,\n)\nvector_store.add_texts(texts=texts, metadatas=metadatas)\nreturn vector_store\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.add_texts","title":"<code>add_texts(texts, metadatas=None)</code>","text":"<p>Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection. Args:     texts (Iterable[str]): Texts to add to the vectorstore.     metadatas (Optional[List[dict]], optional): Optional list of metadatas. Returns:     List[str]: List of IDs of the added texts.</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def add_texts(\nself,\ntexts: Iterable[str],\nmetadatas: Optional[List[Dict[str, str]]] = None,\n) -&gt; List[str]:\n\"\"\"Run texts through the embeddings and add to the vectorstore. Currently, the vectorstore is reinitialized each time, because we do not require a persistent vector store for example selection.\n    Args:\n        texts (Iterable[str]): Texts to add to the vectorstore.\n        metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n    Returns:\n        List[str]: List of IDs of the added texts.\n    \"\"\"\nif self._embedding_function is not None:\nembeddings = self._get_embeddings(texts)\nself._corpus_embeddings = torch.tensor(embeddings)\nself._texts = texts\nself._metadatas = metadatas\nreturn metadatas\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.from_texts","title":"<code>from_texts(texts, embedding=None, metadatas=None, cache=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a vectorstore from raw text. The data will be ephemeral in-memory. Args:     texts (List[str]): List of texts to add to the collection.     embedding (Optional[Embeddings]): Embedding function. Defaults to None.     metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.     cache (bool): Whether to cache the embeddings. Defaults to True. Returns:     vector_store: Vectorstore with seedset embeddings</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>@classmethod\ndef from_texts(\ncls: Type[VectorStoreWrapper],\ntexts: List[str],\nembedding: Optional[Embeddings] = None,\nmetadatas: Optional[List[dict]] = None,\ncache: bool = True,\n**kwargs: Any,\n) -&gt; VectorStoreWrapper:\n\"\"\"Create a vectorstore from raw text.\n    The data will be ephemeral in-memory.\n    Args:\n        texts (List[str]): List of texts to add to the collection.\n        embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n        metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n        cache (bool): Whether to cache the embeddings. Defaults to True.\n    Returns:\n        vector_store: Vectorstore with seedset embeddings\n    \"\"\"\nvector_store = cls(\nembedding_function=embedding,\ncorpus_embeddings=None,\ntexts=None,\ncache=cache,\n**kwargs,\n)\nvector_store.add_texts(texts=texts, metadatas=metadatas)\nreturn vector_store\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.label_diversity_similarity_search","title":"<code>label_diversity_similarity_search(query, label_key, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search. Args:     query (str): Query text to search for.     k (int): Number of results to return per label.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Document]: List of documents most similar to the query text.</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def label_diversity_similarity_search(\nself,\nquery: str,\nlabel_key: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return per label.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Document]: List of documents most similar to the query text.\n    \"\"\"\ndocs_and_scores = self.label_diversity_similarity_search_with_score(\nquery, label_key, k, filter=filter\n)\nreturn [doc for doc, _ in docs_and_scores]\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.label_diversity_similarity_search_with_score","title":"<code>label_diversity_similarity_search_with_score(query, label_key, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search and retrieve distances. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Tuple[Document, float]]: List of documents most similar to the query         text with distance in float.</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def label_diversity_similarity_search_with_score(\nself,\nquery: str,\nlabel_key: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Tuple[Document, float]]: List of documents most similar to the query\n            text with distance in float.\n    \"\"\"\nquery_embeddings = torch.tensor([self._get_embeddings([query])[0]])\ndata = []\ndata = zip(self._corpus_embeddings, self._texts, self._metadatas)\nsorted_data = sorted(data, key=lambda item: item[2].get(label_key))\ndocuments = []\nscores = []\nmetadatas = []\nfor label, label_examples in groupby(\nsorted_data, key=lambda item: item[2].get(label_key)\n):\nlabel_examples_list = list(label_examples)\nlabel_embeddings = list(\nmap(lambda label_example: label_example[0], label_examples_list)\n)\nlabel_texts = list(\nmap(lambda label_example: label_example[1], label_examples_list)\n)\nlabel_metadatas = list(\nmap(lambda label_example: label_example[2], label_examples_list)\n)\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=label_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\ndocuments.extend([label_texts[index] for index in result_ids])\nmetadatas.extend([label_metadatas[index] for index in result_ids])\nscores.extend([result[\"score\"] for result in result_ids_and_scores[0]])\nresults = {}\nresults[\"documents\"] = [documents]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [metadatas]\nreturn _results_to_docs_and_scores(results)\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search","title":"<code>similarity_search(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Document]: List of documents most similar to the query text.</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Document]:\n\"\"\"Run semantic similarity search.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Document]: List of documents most similar to the query text.\n    \"\"\"\ndocs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\nreturn [doc for doc, _ in docs_and_scores]\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.VectorStoreWrapper.similarity_search_with_score","title":"<code>similarity_search_with_score(query, k=4, filter=None, **kwargs)</code>","text":"<p>Run semantic similarity search and retrieve distances. Args:     query (str): Query text to search for.     k (int): Number of results to return. Defaults to 4.     filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None. Returns:     List[Tuple[Document, float]]: List of documents most similar to the query         text with distance in float.</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def similarity_search_with_score(\nself,\nquery: str,\nk: int = 4,\nfilter: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n\"\"\"Run semantic similarity search and retrieve distances.\n    Args:\n        query (str): Query text to search for.\n        k (int): Number of results to return. Defaults to 4.\n        filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n    Returns:\n        List[Tuple[Document, float]]: List of documents most similar to the query\n            text with distance in float.\n    \"\"\"\nquery_embeddings = torch.tensor([self._get_embeddings([query])[0]])\nresult_ids_and_scores = semantic_search(\ncorpus_embeddings=self._corpus_embeddings,\nquery_embeddings=query_embeddings,\ntop_k=k,\n)\nresult_ids = [result[\"corpus_id\"] for result in result_ids_and_scores[0]]\nscores = [result[\"score\"] for result in result_ids_and_scores[0]]\nresults = {}\nresults[\"documents\"] = [[self._texts[index] for index in result_ids]]\nresults[\"distances\"] = [scores]\nresults[\"metadatas\"] = [[self._metadatas[index] for index in result_ids]]\nreturn _results_to_docs_and_scores(results)\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.cos_sim","title":"<code>cos_sim(a, b)</code>","text":"<p>Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j. Returns:     cos_sim: Matrix with res(i)(j) = cos_sim(a[i], b[j])</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def cos_sim(a: Tensor, b: Tensor) -&gt; Tensor:\n\"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    Returns:\n        cos_sim: Matrix with res(i)(j) = cos_sim(a[i], b[j])\n    \"\"\"\nif not isinstance(a, torch.Tensor):\na = torch.tensor(a)\nif not isinstance(b, torch.Tensor):\nb = torch.tensor(b)\nif len(a.shape) == 1:\na = a.unsqueeze(0)\nif len(b.shape) == 1:\nb = b.unsqueeze(0)\na_norm = torch.nn.functional.normalize(a, p=2, dim=1)\nb_norm = torch.nn.functional.normalize(b, p=2, dim=1)\nreturn torch.mm(a_norm, b_norm.transpose(0, 1))\n</code></pre>"},{"location":"autolabel/reference/example_select/#src.autolabel.few_shot.vector_store.semantic_search","title":"<code>semantic_search(query_embeddings, corpus_embeddings, query_chunk_size=100, corpus_chunk_size=500000, top_k=10, score_function=cos_sim)</code>","text":"<p>Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers</p> Source code in <code>autolabel/src/autolabel/few_shot/vector_store.py</code> <pre><code>def semantic_search(\nquery_embeddings: Tensor,\ncorpus_embeddings: Tensor,\nquery_chunk_size: int = 100,\ncorpus_chunk_size: int = 500000,\ntop_k: int = 10,\nscore_function: Callable[[Tensor, Tensor], Tensor] = cos_sim,\n):\n\"\"\"\n    Semantic similarity search based on cosine similarity score. Implementation from this project: https://github.com/UKPLab/sentence-transformers\n    \"\"\"\nif isinstance(query_embeddings, (np.ndarray, np.generic)):\nquery_embeddings = torch.from_numpy(query_embeddings)\nelif isinstance(query_embeddings, list):\nquery_embeddings = torch.stack(query_embeddings)\nif len(query_embeddings.shape) == 1:\nquery_embeddings = query_embeddings.unsqueeze(0)\nif isinstance(corpus_embeddings, (np.ndarray, np.generic)):\ncorpus_embeddings = torch.from_numpy(corpus_embeddings)\nelif isinstance(corpus_embeddings, list):\ncorpus_embeddings = torch.stack(corpus_embeddings)\n# Check that corpus and queries are on the same device\nif corpus_embeddings.device != query_embeddings.device:\nquery_embeddings = query_embeddings.to(corpus_embeddings.device)\nqueries_result_list = [[] for _ in range(len(query_embeddings))]\nfor query_start_idx in range(0, len(query_embeddings), query_chunk_size):\n# Iterate over chunks of the corpus\nfor corpus_start_idx in range(0, len(corpus_embeddings), corpus_chunk_size):\n# Compute cosine similarities\ncos_scores = score_function(\nquery_embeddings[query_start_idx : query_start_idx + query_chunk_size],\ncorpus_embeddings[\ncorpus_start_idx : corpus_start_idx + corpus_chunk_size\n],\n)\n# Get top-k scores\ncos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(\ncos_scores,\nmin(top_k, len(cos_scores[0])),\ndim=1,\nlargest=True,\nsorted=False,\n)\ncos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\ncos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\nfor query_itr in range(len(cos_scores)):\nfor sub_corpus_id, score in zip(\ncos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]\n):\ncorpus_id = corpus_start_idx + sub_corpus_id\nquery_id = query_start_idx + query_itr\nif len(queries_result_list[query_id]) &lt; top_k:\nheapq.heappush(\nqueries_result_list[query_id], (score, corpus_id)\n)  # heaqp tracks the quantity of the first element in the tuple\nelse:\nheapq.heappushpop(\nqueries_result_list[query_id], (score, corpus_id)\n)\n# change the data format and sort\nfor query_id in range(len(queries_result_list)):\nfor doc_itr in range(len(queries_result_list[query_id])):\nscore, corpus_id = queries_result_list[query_id][doc_itr]\nqueries_result_list[query_id][doc_itr] = {\n\"corpus_id\": corpus_id,\n\"score\": score,\n}\nqueries_result_list[query_id] = sorted(\nqueries_result_list[query_id], key=lambda x: x[\"score\"], reverse=True\n)\nreturn queries_result_list\n</code></pre>"},{"location":"autolabel/reference/labeler/","title":"AutoLabeler","text":"Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>class LabelingAgent:\nCOST_KEY = \"Cost in $\"\nCONFIDENCE_MAX_CONTEXT_LENGTH = 3400\ndef __init__(\nself,\nconfig: Union[AutolabelConfig, str, dict],\ncache: Optional[bool] = True,\nexample_selector: Optional[BaseExampleSelector] = None,\nconsole_output: Optional[bool] = True,\ngeneration_cache: Optional[BaseCache] = SQLAlchemyGenerationCache(),\ntransform_cache: Optional[BaseCache] = SQLAlchemyTransformCache(),\nconfidence_cache: Optional[BaseCache] = SQLAlchemyConfidenceCache(),\nconfidence_tokenizer: Optional[AutoTokenizer] = None,\n) -&gt; None:\nself.generation_cache = generation_cache\nself.transform_cache = transform_cache\nself.confidence_cache = confidence_cache\nif not cache:\nlogger.warning(\nf\"cache parameter is deprecated and will be removed soon. Please use generation_cache, transform_cache and confidence_cache instead.\"\n)\nself.generation_cache = None\nself.transform_cache = None\nself.confidence_cache = None\nif self.generation_cache is not None:\nself.generation_cache.initialize()\nif self.transform_cache is not None:\nself.transform_cache.initialize()\nif self.confidence_cache is not None:\nself.confidence_cache.initialize()\nself.console = Console(quiet=not console_output)\nself.config = (\nconfig if isinstance(config, AutolabelConfig) else AutolabelConfig(config)\n)\nself.task = TaskFactory.from_config(self.config)\nself.llm: BaseModel = ModelFactory.from_config(\nself.config, cache=self.generation_cache\n)\nif self.config.confidence_chunk_column():\nif not confidence_tokenizer:\nself.confidence_tokenizer = AutoTokenizer.from_pretrained(\n\"google/flan-t5-xxl\"\n)\nelse:\nself.confidence_tokenizer = confidence_tokenizer\nscore_type = \"logprob_average\"\nif self.config.task_type() == TaskType.ATTRIBUTE_EXTRACTION:\nscore_type = \"logprob_average_per_key\"\nself.confidence = ConfidenceCalculator(\nscore_type=score_type,\nllm=self.llm,\ncache=self.confidence_cache,\n)\nself.example_selector = example_selector\nif in_notebook():\nimport nest_asyncio\nnest_asyncio.apply()\ndef run(\nself,\ndataset: AutolabelDataset,\noutput_name: Optional[str] = None,\nmax_items: Optional[int] = None,\nstart_index: int = 0,\nadditional_metrics: Optional[List[BaseMetric]] = [],\nskip_eval: Optional[bool] = False,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\nreturn asyncio.run(\nself.arun(\ndataset=dataset,\noutput_name=output_name,\nmax_items=max_items,\nstart_index=start_index,\nadditional_metrics=additional_metrics,\nskip_eval=skip_eval,\n)\n)\nasync def arun(\nself,\ndataset: AutolabelDataset,\noutput_name: Optional[str] = None,\nmax_items: Optional[int] = None,\nstart_index: int = 0,\nadditional_metrics: Optional[List[BaseMetric]] = [],\nskip_eval: Optional[bool] = False,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n\"\"\"Labels data in a given dataset. Output written to new CSV file.\n        Args:\n            dataset: path to CSV dataset to be annotated\n            max_items: maximum items in dataset to be annotated\n            output_name: custom name of output CSV file\n            start_index: skips annotating [0, start_index)\n        \"\"\"\ndataset = dataset.get_slice(max_items=max_items, start_index=start_index)\nllm_labels = []\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nif self.example_selector is None:\nif (\nself.config.label_selection()\nand self.config.few_shot_algorithm() != \"fixed\"\n):\n# TODO: Add support for other few shot algorithms specially semantic similarity\nraise ValueError(\n\"Error: Only 'fixed' few shot example selector is supported for label selection.\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config,\n[safe_serialize_to_string(example) for example in seed_examples],\ndataset.df.keys().tolist(),\ncache=self.generation_cache is not None,\n)\nif self.config.label_selection():\nif self.config.task_type() != TaskType.CLASSIFICATION:\nself.console.print(\n\"Warning: label_selection only supported for classification tasks!\"\n)\nelse:\nself.label_selector = LabelSelector(\nconfig=self.config,\nembedding_func=PROVIDER_TO_MODEL.get(\nself.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n)(model=self.config.embedding_model_name()),\n)\ncurrent_index = 0\ncost = 0.0\npostfix_dict = {}\nindices = range(current_index, len(dataset.inputs))\nselected_labels = self.config.labels_list()\nfor current_index in track_with_stats(\nindices,\npostfix_dict,\ntotal=len(dataset.inputs) - current_index,\nconsole=self.console,\n):\nchunk = dataset.inputs[current_index]\nexamples = []\nif (\nself.config.label_selection()\nand self.config.task_type() == TaskType.CLASSIFICATION\n):\n# get every column except the one we want to label\ntoEmbed = chunk.copy()\nif self.config.label_column() and self.config.label_column() in toEmbed:\ndel toEmbed[self.config.label_column()]\n# convert this to a string\ntoEmbed = json.dumps(toEmbed)\nselected_labels = self.label_selector.select_labels(toEmbed)\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(chunk),\nselected_labels=selected_labels,\nlabel_column=self.config.label_column(),\n)\nelse:\nexamples = []\nelse:\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(chunk),\n)\n# Construct Prompt to pass to LLM\nfinal_prompt = self.task.construct_prompt(\nchunk,\nexamples,\nselected_labels=selected_labels,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nresponse = await self.llm.label([final_prompt])\nfor i, generations, error, latency in zip(\nrange(len(response.generations)),\nresponse.generations,\nresponse.errors,\nresponse.latencies,\n):\ninput_tokens = self.llm.get_num_tokens(final_prompt)\nif error is not None:\nannotation = LLMAnnotation(\nsuccessfully_labeled=False,\nlabel=self.task.NULL_LABEL_TOKEN,\nraw_response=\"\",\ncurr_sample=pickle.dumps(chunk),\nprompt=final_prompt,\nconfidence_score=0,\nerror=error,\ninput_tokens=input_tokens,\ncost=0,\nlatency=0,\n)\nelse:\nannotations = []\nfor generation in generations:\nannotation = self.task.parse_llm_response(\ngeneration, chunk, final_prompt\n)\nannotation.confidence_prompt = (\nself.task.construct_confidence_prompt(chunk, examples)\n)\nannotation.input_tokens = input_tokens\nannotation.output_tokens = self.llm.get_num_tokens(\nannotation.raw_response\n)\nannotation.cost = sum(response.costs)\nannotation.latency = latency\nif self.config.confidence():\ntry:\nannotation.confidence_score = (\nawait self.get_confidence_score(\nannotation, chunk, examples\n)\n)\nexcept Exception as e:\nlogger.exception(\nf\"Error calculating confidence score: {e}\"\n)\nlogger.warning(\nf\"Could not calculate confidence score for annotation: {annotation}\"\n)\nif (\nself.config.task_type()\n== TaskType.ATTRIBUTE_EXTRACTION\n):\nannotation.confidence_score = {}\nelse:\nannotation.confidence_score = 0\nannotations.append(annotation)\nannotation = self.majority_annotation(annotations)\nllm_labels.append(annotation)\ncost += sum(response.costs)\npostfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n# Evaluate the task every eval_every examples\nif not skip_eval and (current_index + 1) % 100 == 0:\nif dataset.gt_labels:\neval_result = self.task.eval(\nllm_labels,\n(\ndataset.gt_labels[: len(llm_labels)]\nif isinstance(dataset.gt_labels, list)\nelse {\nk: v[: len(llm_labels)]\nfor k, v in dataset.gt_labels.items()\n}\n),\nadditional_metrics=additional_metrics,\n)\nfor m in eval_result:\n# This is a row wise metric\nif isinstance(m.value, list):\ncontinue\nelif m.show_running:\npostfix_dict[m.name] = (\nf\"{m.value:.4f}\"\nif isinstance(m.value, float)\nelse m.value\n)\neval_result = None\ntable = {}\n# if true labels are provided, evaluate accuracy of predictions\nif not skip_eval and dataset.gt_labels:\neval_result = self.task.eval(\nllm_labels,\n(\ndataset.gt_labels[: len(llm_labels)]\nif isinstance(dataset.gt_labels, list)\nelse {k: v[: len(llm_labels)] for k, v in dataset.gt_labels.items()}\n),\nadditional_metrics=additional_metrics,\n)\n# TODO: serialize and write to file\nfor m in eval_result:\nif isinstance(m.value, list):\ncontinue\nelif m.show_running:\ntable[m.name] = m.value\nelse:\nself.console.print(f\"{m.name}:\\n{m.value}\")\n# print cost\nself.console.print(f\"Actual Cost: {maybe_round(cost)}\")\nprint_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\ndataset.process_labels(llm_labels, eval_result)\n# Only save to csv if output_name is provided or dataset is a string\nif not output_name and isinstance(dataset, str):\noutput_name = (\ndataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n)\nif output_name:\ndataset.save(output_file_name=output_name)\nreturn dataset\ndef plan(\nself,\ndataset: AutolabelDataset,\nmax_items: Optional[int] = None,\nstart_index: int = 0,\n) -&gt; None:\n\"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n        Args:\n            dataset: path to a CSV dataset\n        \"\"\"\ndataset = dataset.get_slice(max_items=max_items, start_index=start_index)\nif (\nself.config.confidence()\nand \"REFUEL_API_KEY\" not in os.environ\nand not self.llm.returns_token_probs()\n):\nraise ValueError(\n\"REFUEL_API_KEY environment variable must be set to compute confidence scores. You can request an API key at https://refuel-ai.typeform.com/llm-access.\"\n)\nprompt_list = []\ntotal_cost = 0\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config,\n[safe_serialize_to_string(example) for example in seed_examples],\ndataset.df.keys().tolist(),\ncache=self.generation_cache is not None,\n)\nif self.config.label_selection():\nif self.config.task_type() != TaskType.CLASSIFICATION:\nself.console.print(\n\"Warning: label_selection only supported for classification tasks!\"\n)\nelse:\nself.label_selector = LabelSelector(\nconfig=self.config,\nembedding_func=PROVIDER_TO_MODEL.get(\nself.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n)(model=self.config.embedding_model_name()),\n)\ninput_limit = min(len(dataset.inputs), 100) if max_items is None else max_items  # type: ignore\nfor input_i in track(\ndataset.inputs[:input_limit],\ndescription=\"Generating Prompts...\",\nconsole=self.console,\n):\n# TODO: Check if this needs to use the example selector\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(input_i)\n)\nelse:\nexamples = []\nif (\nself.config.label_selection()\nand self.config.task_type() == TaskType.CLASSIFICATION\n):\nselected_labels = self.label_selector.select_labels(input_i[\"example\"])\nfinal_prompt = self.task.construct_prompt(\ninput_i,\nexamples,\nselected_labels=selected_labels,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nelse:\nfinal_prompt = self.task.construct_prompt(\ninput_i,\nexamples,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nprompt_list.append(final_prompt)\n# Calculate the number of tokens\ncurr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\ntotal_cost += curr_cost\ntotal_cost = total_cost * (len(dataset.inputs) / input_limit)\ntable = {\n\"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n\"Number of Examples\": len(dataset.inputs),\n\"Average cost per example\": f\"${maybe_round(total_cost / len(dataset.inputs))}\",\n}\ntable = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\nprint_table(\ntable, show_header=False, console=self.console, styles=COST_TABLE_STYLES\n)\nself.console.rule(\"Prompt Example\")\nself.console.print(f\"{prompt_list[0]}\", markup=False)\nself.console.rule()\nasync def async_run_transform(\nself, transform: BaseTransform, dataset: AutolabelDataset\n):\ntransform_outputs = [\ntransform.apply(input_dict) for input_dict in dataset.inputs\n]\noutputs = await gather_async_tasks_with_progress(\ntransform_outputs,\ndescription=f\"Running transform {transform.name()}...\",\nconsole=self.console,\n)\noutput_df = pd.DataFrame.from_records(outputs)\nfinal_df = pd.concat([dataset.df, output_df], axis=1)\ndataset = AutolabelDataset(final_df, self.config)\nreturn dataset\ndef transform(self, dataset: AutolabelDataset):\ntransforms = []\nfor transform_dict in self.config.transforms():\ntransforms.append(\nTransformFactory.from_dict(transform_dict, cache=self.transform_cache)\n)\nfor transform in transforms:\ndataset = asyncio.run(self.async_run_transform(transform, dataset))\nreturn dataset\nasync def get_confidence_score(\nself, annotation: LLMAnnotation, chunk: Dict, examples: List[Dict]\n) -&gt; Union[float, dict]:\nfull_confidence_input = annotation.confidence_prompt + annotation.raw_response\nif (\nself.llm.returns_token_probs()\nor not self.config.confidence_chunk_column()\nor self.get_num_tokens(full_confidence_input)\n&lt; self.CONFIDENCE_MAX_CONTEXT_LENGTH\n):\nreturn await self.confidence.calculate(model_generation=annotation)\nkey_to_chunk = self.config.confidence_chunk_column()\nif not key_to_chunk:\nraise ValueError(\n\"confidence_chunk_column must be set in the config to use confidence_chunk_size\"\n)\nif key_to_chunk == AUTO_CONFIDENCE_CHUNKING_COLUMN:\n# If the confidence_chunk_column is set to auto,\n# we choose the column with the most tokens as the chunking column.\nmax_tokens = -1\nexample_template_keys = get_format_variables(self.config.example_template())\nfor key in example_template_keys:\nnum_tokens = self.get_num_tokens(chunk[key])\nif num_tokens &gt; max_tokens:\nmax_tokens = num_tokens\nkey_to_chunk = key\nempty_chunk = chunk.copy()\nempty_chunk[key_to_chunk] = \"\"\nempty_prompt = self.task.construct_confidence_prompt(empty_chunk, examples)\nnum_tokens_empty_prompt = self.get_num_tokens(empty_prompt)\nnum_tokens_per_chunk = (\nself.config.confidence_chunk_size() - num_tokens_empty_prompt\n)\nconfidence_chunks = self.chunk_string(chunk[key_to_chunk], num_tokens_per_chunk)\nconfidence_scores = []\nfor confidence_chunk in confidence_chunks:\nnew_chunk = chunk.copy()\nnew_chunk[key_to_chunk] = confidence_chunk\nnew_prompt = self.task.construct_confidence_prompt(new_chunk, examples)\nannotation_dict = annotation.dict()\nannotation_dict[\"confidence_prompt\"] = new_prompt\nconfidence_scores.append(\nawait self.confidence.calculate(\nmodel_generation=LLMAnnotation(**annotation_dict),\n)\n)\nmerge_function = MERGE_FUNCTION[self.config.confidence_merge_function()]\nif isinstance(confidence_scores[0], dict):\nmerged_confidence = {}\nfor key in confidence_scores[0].keys():\nmerged_confidence[key] = merge_function(\n[conf[key] for conf in confidence_scores]\n)\nreturn merged_confidence\nelse:\nmerged_confidence = merge_function(confidence_scores)\nreturn merged_confidence\ndef majority_annotation(\nself, annotation_list: List[LLMAnnotation]\n) -&gt; LLMAnnotation:\nlabels = [a.label for a in annotation_list]\ncounts = {}\nfor ind, label in enumerate(labels):\n# Needed for named entity recognition which outputs lists instead of strings\nlabel = str(label)\nif label not in counts:\ncounts[label] = (1, ind)\nelse:\ncounts[label] = (counts[label][0] + 1, counts[label][1])\nmax_label = max(counts, key=lambda x: counts[x][0])\nreturn annotation_list[counts[max_label][1]]\ndef generate_explanations(\nself,\nseed_examples: Union[str, List[Dict]],\ninclude_label: bool = True,\n) -&gt; List[Dict]:\nreturn asyncio.run(\nself.agenerate_explanations(\nseed_examples=seed_examples, include_label=include_label\n)\n)\nasync def agenerate_explanations(\nself,\nseed_examples: Union[str, List[Dict]],\ninclude_label: bool = True,\n) -&gt; List[Dict]:\n\"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\nout_file = None\nif isinstance(seed_examples, str):\nout_file = seed_examples\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\nexplanation_column = self.config.explanation_column()\nif not explanation_column:\nraise ValueError(\n\"The explanation column needs to be specified in the dataset config.\"\n)\nfor seed_example in track(\nseed_examples,\ndescription=\"Generating explanations\",\nconsole=self.console,\n):\nexplanation_prompt = self.task.get_explanation_prompt(\nseed_example, include_label=include_label\n)\nif self.task.image_col is not None:\nexplanation_prompt = json.dumps(\n{\n\"text\": explanation_prompt,\n\"image_url\": seed_example[self.task.image_col],\n}\n)\nexplanation = await self.llm.label([explanation_prompt])\nexplanation = explanation.generations[0][0].text\nseed_example[explanation_column] = str(explanation) if explanation else \"\"\nif out_file:\ndf = pd.DataFrame.from_records(seed_examples)\ndf.to_csv(out_file, index=False)\nreturn seed_examples\ndef generate_synthetic_dataset(self) -&gt; AutolabelDataset:\ncolumns = get_format_variables(self.config.example_template())\ndf = pd.DataFrame(columns=columns)\nfor label in track(\nself.config.labels_list(),\ndescription=\"Generating dataset\",\nconsole=self.console,\n):\nprompt = self.task.get_generate_dataset_prompt(label)\nresult = self.llm.label([prompt])\nif result.errors[0] is not None:\nself.console.print(\nf\"Error generating rows for label {label}: {result.errors[0]}\"\n)\nelse:\nresponse = result.generations[0][0].text.strip()\nresponse = io.StringIO(response)\nlabel_df = pd.read_csv(response, sep=self.config.delimiter())\nlabel_df[self.config.label_column()] = label\ndf = pd.concat([df, label_df], axis=0, ignore_index=True)\nreturn AutolabelDataset(df, self.config)\ndef clear_cache(self, use_ttl: bool = True):\n\"\"\"\n        Clears the generation and transformation cache from autolabel.\n        Args:\n            use_ttl: If true, only clears the cache if the ttl has expired.\n        \"\"\"\nself.generation_cache.clear(use_ttl=use_ttl)\nself.transform_cache.clear(use_ttl=use_ttl)\ndef get_num_tokens(self, inp: str) -&gt; int:\n\"\"\"Returns the number of tokens in the prompt\"\"\"\nreturn len(self.confidence_tokenizer.encode(str(inp)))\ndef chunk_string(self, inp: str, chunk_size: int) -&gt; List[str]:\n\"\"\"Chunks the input string into chunks of size chunk_size\"\"\"\ntokens = self.confidence_tokenizer.encode(inp)\nchunks = [tokens[i : i + chunk_size] for i in range(0, len(tokens), chunk_size)]\nreturn [self.confidence_tokenizer.decode(chunk) for chunk in chunks]\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.agenerate_explanations","title":"<code>agenerate_explanations(seed_examples, include_label=True)</code>  <code>async</code>","text":"<p>Use LLM to generate explanations for why examples are labeled the way that they are.</p> Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>async def agenerate_explanations(\nself,\nseed_examples: Union[str, List[Dict]],\ninclude_label: bool = True,\n) -&gt; List[Dict]:\n\"\"\"Use LLM to generate explanations for why examples are labeled the way that they are.\"\"\"\nout_file = None\nif isinstance(seed_examples, str):\nout_file = seed_examples\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\nexplanation_column = self.config.explanation_column()\nif not explanation_column:\nraise ValueError(\n\"The explanation column needs to be specified in the dataset config.\"\n)\nfor seed_example in track(\nseed_examples,\ndescription=\"Generating explanations\",\nconsole=self.console,\n):\nexplanation_prompt = self.task.get_explanation_prompt(\nseed_example, include_label=include_label\n)\nif self.task.image_col is not None:\nexplanation_prompt = json.dumps(\n{\n\"text\": explanation_prompt,\n\"image_url\": seed_example[self.task.image_col],\n}\n)\nexplanation = await self.llm.label([explanation_prompt])\nexplanation = explanation.generations[0][0].text\nseed_example[explanation_column] = str(explanation) if explanation else \"\"\nif out_file:\ndf = pd.DataFrame.from_records(seed_examples)\ndf.to_csv(out_file, index=False)\nreturn seed_examples\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.arun","title":"<code>arun(dataset, output_name=None, max_items=None, start_index=0, additional_metrics=[], skip_eval=False)</code>  <code>async</code>","text":"<p>Labels data in a given dataset. Output written to new CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AutolabelDataset</code> <p>path to CSV dataset to be annotated</p> required <code>max_items</code> <code>Optional[int]</code> <p>maximum items in dataset to be annotated</p> <code>None</code> <code>output_name</code> <code>Optional[str]</code> <p>custom name of output CSV file</p> <code>None</code> <code>start_index</code> <code>int</code> <p>skips annotating [0, start_index)</p> <code>0</code> Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>async def arun(\nself,\ndataset: AutolabelDataset,\noutput_name: Optional[str] = None,\nmax_items: Optional[int] = None,\nstart_index: int = 0,\nadditional_metrics: Optional[List[BaseMetric]] = [],\nskip_eval: Optional[bool] = False,\n) -&gt; Tuple[pd.Series, pd.DataFrame, List[MetricResult]]:\n\"\"\"Labels data in a given dataset. Output written to new CSV file.\n    Args:\n        dataset: path to CSV dataset to be annotated\n        max_items: maximum items in dataset to be annotated\n        output_name: custom name of output CSV file\n        start_index: skips annotating [0, start_index)\n    \"\"\"\ndataset = dataset.get_slice(max_items=max_items, start_index=start_index)\nllm_labels = []\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nif self.example_selector is None:\nif (\nself.config.label_selection()\nand self.config.few_shot_algorithm() != \"fixed\"\n):\n# TODO: Add support for other few shot algorithms specially semantic similarity\nraise ValueError(\n\"Error: Only 'fixed' few shot example selector is supported for label selection.\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config,\n[safe_serialize_to_string(example) for example in seed_examples],\ndataset.df.keys().tolist(),\ncache=self.generation_cache is not None,\n)\nif self.config.label_selection():\nif self.config.task_type() != TaskType.CLASSIFICATION:\nself.console.print(\n\"Warning: label_selection only supported for classification tasks!\"\n)\nelse:\nself.label_selector = LabelSelector(\nconfig=self.config,\nembedding_func=PROVIDER_TO_MODEL.get(\nself.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n)(model=self.config.embedding_model_name()),\n)\ncurrent_index = 0\ncost = 0.0\npostfix_dict = {}\nindices = range(current_index, len(dataset.inputs))\nselected_labels = self.config.labels_list()\nfor current_index in track_with_stats(\nindices,\npostfix_dict,\ntotal=len(dataset.inputs) - current_index,\nconsole=self.console,\n):\nchunk = dataset.inputs[current_index]\nexamples = []\nif (\nself.config.label_selection()\nand self.config.task_type() == TaskType.CLASSIFICATION\n):\n# get every column except the one we want to label\ntoEmbed = chunk.copy()\nif self.config.label_column() and self.config.label_column() in toEmbed:\ndel toEmbed[self.config.label_column()]\n# convert this to a string\ntoEmbed = json.dumps(toEmbed)\nselected_labels = self.label_selector.select_labels(toEmbed)\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(chunk),\nselected_labels=selected_labels,\nlabel_column=self.config.label_column(),\n)\nelse:\nexamples = []\nelse:\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(chunk),\n)\n# Construct Prompt to pass to LLM\nfinal_prompt = self.task.construct_prompt(\nchunk,\nexamples,\nselected_labels=selected_labels,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nresponse = await self.llm.label([final_prompt])\nfor i, generations, error, latency in zip(\nrange(len(response.generations)),\nresponse.generations,\nresponse.errors,\nresponse.latencies,\n):\ninput_tokens = self.llm.get_num_tokens(final_prompt)\nif error is not None:\nannotation = LLMAnnotation(\nsuccessfully_labeled=False,\nlabel=self.task.NULL_LABEL_TOKEN,\nraw_response=\"\",\ncurr_sample=pickle.dumps(chunk),\nprompt=final_prompt,\nconfidence_score=0,\nerror=error,\ninput_tokens=input_tokens,\ncost=0,\nlatency=0,\n)\nelse:\nannotations = []\nfor generation in generations:\nannotation = self.task.parse_llm_response(\ngeneration, chunk, final_prompt\n)\nannotation.confidence_prompt = (\nself.task.construct_confidence_prompt(chunk, examples)\n)\nannotation.input_tokens = input_tokens\nannotation.output_tokens = self.llm.get_num_tokens(\nannotation.raw_response\n)\nannotation.cost = sum(response.costs)\nannotation.latency = latency\nif self.config.confidence():\ntry:\nannotation.confidence_score = (\nawait self.get_confidence_score(\nannotation, chunk, examples\n)\n)\nexcept Exception as e:\nlogger.exception(\nf\"Error calculating confidence score: {e}\"\n)\nlogger.warning(\nf\"Could not calculate confidence score for annotation: {annotation}\"\n)\nif (\nself.config.task_type()\n== TaskType.ATTRIBUTE_EXTRACTION\n):\nannotation.confidence_score = {}\nelse:\nannotation.confidence_score = 0\nannotations.append(annotation)\nannotation = self.majority_annotation(annotations)\nllm_labels.append(annotation)\ncost += sum(response.costs)\npostfix_dict[self.COST_KEY] = f\"{cost:.2f}\"\n# Evaluate the task every eval_every examples\nif not skip_eval and (current_index + 1) % 100 == 0:\nif dataset.gt_labels:\neval_result = self.task.eval(\nllm_labels,\n(\ndataset.gt_labels[: len(llm_labels)]\nif isinstance(dataset.gt_labels, list)\nelse {\nk: v[: len(llm_labels)]\nfor k, v in dataset.gt_labels.items()\n}\n),\nadditional_metrics=additional_metrics,\n)\nfor m in eval_result:\n# This is a row wise metric\nif isinstance(m.value, list):\ncontinue\nelif m.show_running:\npostfix_dict[m.name] = (\nf\"{m.value:.4f}\"\nif isinstance(m.value, float)\nelse m.value\n)\neval_result = None\ntable = {}\n# if true labels are provided, evaluate accuracy of predictions\nif not skip_eval and dataset.gt_labels:\neval_result = self.task.eval(\nllm_labels,\n(\ndataset.gt_labels[: len(llm_labels)]\nif isinstance(dataset.gt_labels, list)\nelse {k: v[: len(llm_labels)] for k, v in dataset.gt_labels.items()}\n),\nadditional_metrics=additional_metrics,\n)\n# TODO: serialize and write to file\nfor m in eval_result:\nif isinstance(m.value, list):\ncontinue\nelif m.show_running:\ntable[m.name] = m.value\nelse:\nself.console.print(f\"{m.name}:\\n{m.value}\")\n# print cost\nself.console.print(f\"Actual Cost: {maybe_round(cost)}\")\nprint_table(table, console=self.console, default_style=METRIC_TABLE_STYLE)\ndataset.process_labels(llm_labels, eval_result)\n# Only save to csv if output_name is provided or dataset is a string\nif not output_name and isinstance(dataset, str):\noutput_name = (\ndataset.rsplit(\".\", 1)[0] + \"_labeled.\" + dataset.rsplit(\".\", 1)[1]\n)\nif output_name:\ndataset.save(output_file_name=output_name)\nreturn dataset\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.chunk_string","title":"<code>chunk_string(inp, chunk_size)</code>","text":"<p>Chunks the input string into chunks of size chunk_size</p> Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>def chunk_string(self, inp: str, chunk_size: int) -&gt; List[str]:\n\"\"\"Chunks the input string into chunks of size chunk_size\"\"\"\ntokens = self.confidence_tokenizer.encode(inp)\nchunks = [tokens[i : i + chunk_size] for i in range(0, len(tokens), chunk_size)]\nreturn [self.confidence_tokenizer.decode(chunk) for chunk in chunks]\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.clear_cache","title":"<code>clear_cache(use_ttl=True)</code>","text":"<p>Clears the generation and transformation cache from autolabel. Args:     use_ttl: If true, only clears the cache if the ttl has expired.</p> Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>def clear_cache(self, use_ttl: bool = True):\n\"\"\"\n    Clears the generation and transformation cache from autolabel.\n    Args:\n        use_ttl: If true, only clears the cache if the ttl has expired.\n    \"\"\"\nself.generation_cache.clear(use_ttl=use_ttl)\nself.transform_cache.clear(use_ttl=use_ttl)\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.get_num_tokens","title":"<code>get_num_tokens(inp)</code>","text":"<p>Returns the number of tokens in the prompt</p> Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>def get_num_tokens(self, inp: str) -&gt; int:\n\"\"\"Returns the number of tokens in the prompt\"\"\"\nreturn len(self.confidence_tokenizer.encode(str(inp)))\n</code></pre>"},{"location":"autolabel/reference/labeler/#src.autolabel.labeler.LabelingAgent.plan","title":"<code>plan(dataset, max_items=None, start_index=0)</code>","text":"<p>Calculates and prints the cost of calling autolabel.run() on a given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AutolabelDataset</code> <p>path to a CSV dataset</p> required Source code in <code>autolabel/src/autolabel/labeler.py</code> <pre><code>def plan(\nself,\ndataset: AutolabelDataset,\nmax_items: Optional[int] = None,\nstart_index: int = 0,\n) -&gt; None:\n\"\"\"Calculates and prints the cost of calling autolabel.run() on a given dataset\n    Args:\n        dataset: path to a CSV dataset\n    \"\"\"\ndataset = dataset.get_slice(max_items=max_items, start_index=start_index)\nif (\nself.config.confidence()\nand \"REFUEL_API_KEY\" not in os.environ\nand not self.llm.returns_token_probs()\n):\nraise ValueError(\n\"REFUEL_API_KEY environment variable must be set to compute confidence scores. You can request an API key at https://refuel-ai.typeform.com/llm-access.\"\n)\nprompt_list = []\ntotal_cost = 0\n# Get the seed examples from the dataset config\nseed_examples = self.config.few_shot_example_set()\n# If this dataset config is a string, read the corrresponding csv file\nif isinstance(seed_examples, str):\nseed_loader = AutolabelDataset(seed_examples, self.config)\nseed_examples = seed_loader.inputs\n# Check explanations are present in data if explanation_column is passed in\nif (\nself.config.explanation_column()\nand len(seed_examples) &gt; 0\nand self.config.explanation_column() not in list(seed_examples[0].keys())\n):\nraise ValueError(\nf\"Explanation column {self.config.explanation_column()} not found in dataset.\\nMake sure that explanations were generated using labeler.generate_explanations(seed_file).\"\n)\nself.example_selector = ExampleSelectorFactory.initialize_selector(\nself.config,\n[safe_serialize_to_string(example) for example in seed_examples],\ndataset.df.keys().tolist(),\ncache=self.generation_cache is not None,\n)\nif self.config.label_selection():\nif self.config.task_type() != TaskType.CLASSIFICATION:\nself.console.print(\n\"Warning: label_selection only supported for classification tasks!\"\n)\nelse:\nself.label_selector = LabelSelector(\nconfig=self.config,\nembedding_func=PROVIDER_TO_MODEL.get(\nself.config.embedding_provider(), DEFAULT_EMBEDDING_PROVIDER\n)(model=self.config.embedding_model_name()),\n)\ninput_limit = min(len(dataset.inputs), 100) if max_items is None else max_items  # type: ignore\nfor input_i in track(\ndataset.inputs[:input_limit],\ndescription=\"Generating Prompts...\",\nconsole=self.console,\n):\n# TODO: Check if this needs to use the example selector\nif self.example_selector:\nexamples = self.example_selector.select_examples(\nsafe_serialize_to_string(input_i)\n)\nelse:\nexamples = []\nif (\nself.config.label_selection()\nand self.config.task_type() == TaskType.CLASSIFICATION\n):\nselected_labels = self.label_selector.select_labels(input_i[\"example\"])\nfinal_prompt = self.task.construct_prompt(\ninput_i,\nexamples,\nselected_labels=selected_labels,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nelse:\nfinal_prompt = self.task.construct_prompt(\ninput_i,\nexamples,\nmax_input_tokens=self.llm.DEFAULT_CONTEXT_LENGTH,\nget_num_tokens=self.llm.get_num_tokens,\n)\nprompt_list.append(final_prompt)\n# Calculate the number of tokens\ncurr_cost = self.llm.get_cost(prompt=final_prompt, label=\"\")\ntotal_cost += curr_cost\ntotal_cost = total_cost * (len(dataset.inputs) / input_limit)\ntable = {\n\"Total Estimated Cost\": f\"${maybe_round(total_cost)}\",\n\"Number of Examples\": len(dataset.inputs),\n\"Average cost per example\": f\"${maybe_round(total_cost / len(dataset.inputs))}\",\n}\ntable = {\"parameter\": list(table.keys()), \"value\": list(table.values())}\nprint_table(\ntable, show_header=False, console=self.console, styles=COST_TABLE_STYLES\n)\nself.console.rule(\"Prompt Example\")\nself.console.print(f\"{prompt_list[0]}\", markup=False)\nself.console.rule()\n</code></pre>"},{"location":"autolabel/reference/models/","title":"Models","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>class BaseModel(ABC):\nTTL_MS = 60 * 60 * 24 * 7 * 1000  # 1 week\nDEFAULT_CONTEXT_LENGTH = None\ndef __init__(self, config: AutolabelConfig, cache: BaseCache) -&gt; None:\nself.config = config\nself.cache = cache\nself.model_params = config.model_params()\n# Specific classes that implement this interface should run initialization steps here\n# E.g. initializing the LLM model with required parameters from ModelConfig\nasync def label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n\"\"\"Label a list of prompts.\"\"\"\nexisting_prompts = {}\nmissing_prompt_idxs = list(range(len(prompts)))\nmissing_prompts = prompts\ncosts = []\nerrors = [None for i in range(len(prompts))]\nlatencies = [0 for i in range(len(prompts))]\nif self.cache:\n(\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n) = self.get_cached_prompts(prompts)\n# label missing prompts\nif len(missing_prompts) &gt; 0:\nif hasattr(self, \"_alabel\"):\nnew_results = await self._alabel(missing_prompts)\nelse:\nnew_results = self._label(missing_prompts)\nfor ind, prompt in enumerate(missing_prompts):\ncosts.append(\nself.get_cost(prompt, label=new_results.generations[ind][0].text)\n)\n# Set the existing prompts to the new results\nfor i, result, error, latency in zip(\nmissing_prompt_idxs,\nnew_results.generations,\nnew_results.errors,\nnew_results.latencies,\n):\nexisting_prompts[i] = result\nerrors[i] = error\nlatencies[i] = latency\nif self.cache:\nself.update_cache(missing_prompt_idxs, new_results, prompts)\ngenerations = [existing_prompts[i] for i in range(len(prompts))]\nreturn RefuelLLMResult(\ngenerations=generations, costs=costs, errors=errors, latencies=latencies\n)\nasync def _alabel_individually(self, prompts: List[str]) -&gt; RefuelLLMResult:\n\"\"\"Label each prompt individually. Should be used only after trying as a batch first.\n        Args:\n            prompts (List[str]): List of prompts to label\n        Returns:\n            LLMResult: LLMResult object with generations\n            List[LabelingError]: List of errors encountered while labeling\n        \"\"\"\ngenerations = []\nerrors = []\nlatencies = []\nfor prompt in prompts:\ntry:\nstart_time = time()\nresponse = await self.llm.agenerate([prompt])\ngenerations.append(response.generations[0])\nerrors.append(None)\nlatencies.append(time() - start_time)\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}\")\ngenerations.append([Generation(text=\"\")])\nerrors.append(\nLabelingError(\nerror_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n)\n)\nlatencies.append(0)\nreturn RefuelLLMResult(\ngenerations=generations, errors=errors, latencies=latencies\n)\ndef _label_individually(self, prompts: List[str]) -&gt; RefuelLLMResult:\n\"\"\"Label each prompt individually. Should be used only after trying as a batch first.\n        Args:\n            prompts (List[str]): List of prompts to label\n        Returns:\n            LLMResult: LLMResult object with generations\n            List[LabelingError]: List of errors encountered while labeling\n        \"\"\"\ngenerations = []\nerrors = []\nlatencies = []\nfor prompt in prompts:\ntry:\nstart_time = time()\nresponse = self.llm.generate([prompt])\ngenerations.append(response.generations[0])\nerrors.append(None)\nlatencies.append(time() - start_time)\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}\")\ngenerations.append([Generation(text=\"\")])\nerrors.append(\nLabelingError(\nerror_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n)\n)\nlatencies.append(0)\nreturn RefuelLLMResult(\ngenerations=generations, errors=errors, latencies=latencies\n)\n@abstractmethod\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n# TODO: change return type to do parsing in the Model class\npass\n@abstractmethod\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\npass\ndef get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n\"\"\"Get prompts that are already cached.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nmissing_prompts = []\nmissing_prompt_idxs = []\nexisting_prompts = {}\nfor i, prompt in enumerate(prompts):\ncache_entry = GenerationCacheEntry(\nprompt=prompt,\nmodel_name=self.model_name,\nmodel_params=model_params_string,\n)\ncache_val = self.cache.lookup(cache_entry)\nif cache_val:\nexisting_prompts[i] = cache_val\nelse:\nmissing_prompts.append(prompt)\nmissing_prompt_idxs.append(i)\nreturn (\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n)\ndef update_cache(self, missing_prompt_idxs, new_results, prompts):\n\"\"\"Update the cache with new results.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nfor i, result, error in zip(\nmissing_prompt_idxs, new_results.generations, new_results.errors\n):\n# If there was an error, don't cache the result\nif error is not None:\ncontinue\ncache_entry = GenerationCacheEntry(\nprompt=prompts[i],\nmodel_name=self.model_name,\nmodel_params=model_params_string,\ngenerations=result,\nttl_ms=self.TTL_MS,\n)\nself.cache.update(cache_entry)\n@abstractmethod\ndef returns_token_probs(self) -&gt; bool:\n\"\"\"Whether the LLM supports returning logprobs of generated tokens\n        Returns:\n            bool: whether the LLM returns supports returning logprobs of generated tokens\n        \"\"\"\npass\n@abstractmethod\ndef get_num_tokens(self, prompt: str) -&gt; int:\n\"\"\"\n        Get the number of tokens in the prompt\"\"\"\npass\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/models/anthropic.py</code> <pre><code>class AnthropicLLM(BaseModel):\nDEFAULT_MODEL = \"claude-instant-v1\"\nDEFAULT_PARAMS = {\n\"max_tokens_to_sample\": 1000,\n\"temperature\": 0.0,\n}\n# Reference: https://cdn2.assets-servd.host/anthropic-website/production/images/apr-pricing-tokens.pdf\nCOST_PER_PROMPT_TOKEN = {\n# $11.02 per million tokens\n\"claude-v1\": (11.02 / 1000000),\n\"claude-instant-v1\": (1.63 / 1000000),\n}\nCOST_PER_COMPLETION_TOKEN = {\n# $32.68 per million tokens\n\"claude-v1\": (32.68 / 1000000),\n\"claude-instant-v1\": (5.51 / 1000000),\n}\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\ntry:\nfrom langchain.chat_models import ChatAnthropic\nfrom anthropic._tokenizers import sync_get_tokenizer\nexcept ImportError:\nraise ImportError(\n\"anthropic is required to use the anthropic LLM. Please install it with the following command: pip install 'refuel-autolabel[anthropic]'\"\n)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\n# initialize LLM\nself.llm = ChatAnthropic(model=self.model_name, **self.model_params)\nself.tokenizer = sync_get_tokenizer()\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\ntry:\nstart_time = time()\nresult = self.llm.generate(prompts)\nend_time = time()\nreturn RefuelLLMResult(\ngenerations=result.generations,\nerrors=[None] * len(result.generations),\nlatencies=[end_time - start_time] * len(result.generations),\n)\nexcept Exception as e:\nreturn self._label_individually(prompts)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nnum_prompt_toks = len(self.tokenizer.encode(prompt).ids)\nif label:\nnum_label_toks = len(self.tokenizer.encode(label).ids)\nelse:\n# get an upper bound\nnum_label_toks = self.model_params[\"max_tokens_to_sample\"]\ncost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\ncost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\nreturn (num_prompt_toks * cost_per_prompt_token) + (\nnum_label_toks * cost_per_completion_token\n)\ndef returns_token_probs(self) -&gt; bool:\nreturn False\ndef get_num_tokens(self, prompt: str) -&gt; int:\nreturn len(self.tokenizer.encode(prompt).ids)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/models/hf_pipeline.py</code> <pre><code>class HFPipelineLLM(BaseModel):\nDEFAULT_MODEL = \"google/flan-t5-xxl\"\nDEFAULT_PARAMS = {\"temperature\": 0.0, \"quantize\": 8}\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\nfrom langchain.llms import HuggingFacePipeline\ntry:\nfrom transformers import (\nAutoConfig,\nAutoModelForSeq2SeqLM,\nAutoModelForCausalLM,\nAutoTokenizer,\npipeline,\n)\nfrom transformers.models.auto.modeling_auto import (\nMODEL_FOR_CAUSAL_LM_MAPPING,\nMODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n)\nexcept ImportError:\nraise ValueError(\n\"Could not import transformers python package. \"\n\"Please it install it with `pip install transformers`.\"\n)\ntry:\nimport torch\nexcept ImportError:\nraise ValueError(\n\"Could not import torch package. \"\n\"Please it install it with `pip install torch`.\"\n)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\nif config.logit_bias() != 0:\nself.model_params = {\n**self._generate_sequence_bias(),\n**self.model_params,\n}\n# initialize HF pipeline\nself.tokenizer = AutoTokenizer.from_pretrained(\nself.model_name, use_fast=False, add_prefix_space=True\n)\nquantize_bits = self.model_params[\"quantize\"]\nmodel_config = AutoConfig.from_pretrained(self.model_name)\nif isinstance(model_config, tuple(MODEL_FOR_CAUSAL_LM_MAPPING)):\nAutoModel = AutoModelForCausalLM\nelif isinstance(model_config, tuple(MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING)):\nAutoModel = AutoModelForSeq2SeqLM\nelse:\nraise ValueError(\n\"model_name is neither a causal LM nor a seq2seq LM. Please check the model_name.\"\n)\nif not torch.cuda.is_available():\nmodel = AutoModel.from_pretrained(self.model_name)\nelif quantize_bits == 8:\nmodel = AutoModel.from_pretrained(\nself.model_name, load_in_8bit=True, device_map=\"auto\"\n)\nelif quantize_bits == \"16\":\nmodel = AutoModel.from_pretrained(\nself.model_name, torch_dtype=torch.float16, device_map=\"auto\"\n)\nelse:\nmodel = AutoModel.from_pretrained(self.model_name, device_map=\"auto\")\nmodel_kwargs = dict(self.model_params)  # make a copy of the model params\nmodel_kwargs.pop(\"quantize\", None)  # remove quantize from the model params\npipe = pipeline(\n\"text2text-generation\",\nmodel=model,\ntokenizer=self.tokenizer,\n**model_kwargs,\n)\n# initialize LLM\nself.llm = HuggingFacePipeline(pipeline=pipe, model_kwargs=model_kwargs)\ndef _generate_sequence_bias(self) -&gt; Dict:\n\"\"\"Generates sequence bias dict to add to the config for the labels specified\n        Returns:\n            Dict: sequence bias, max new tokens, and num beams\n        \"\"\"\nif len(self.config.labels_list()) == 0:\nlogger.warning(\n\"No labels specified in the config. Skipping logit bias generation.\"\n)\nreturn {}\ntry:\nfrom transformers import AutoTokenizer\nexcept ImportError:\nraise ValueError(\n\"Could not import transformers python package. \"\n\"Please it install it with `pip install transformers`.\"\n)\nsequence_bias = {tuple([self.tokenizer.eos_token_id]): self.config.logit_bias()}\nmax_new_tokens = 0\nfor label in self.config.labels_list():\ntokens = tuple(\nself.tokenizer([label], add_special_tokens=False).input_ids[0]\n)\nfor token in tokens:\nsequence_bias[tuple([token])] = self.config.logit_bias()\nmax_new_tokens = max(max_new_tokens, len(tokens))\nreturn {\n\"sequence_bias\": sequence_bias,\n\"max_new_tokens\": max_new_tokens,\n}\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\ntry:\nstart_time = time()\nresult = self.llm.generate(prompts)\nend_time = time()\nreturn RefuelLLMResult(\ngenerations=result.generations,\nerrors=[None] * len(result.generations),\nlatencies=[end_time - start_time] * len(result.generations),\n)\nexcept Exception as e:\nreturn self._label_individually(prompts)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\n# Model inference for this model is being run locally\n# Revisit this in the future when we support HF inference endpoints\nreturn 0.0\ndef returns_token_probs(self) -&gt; bool:\nreturn False\ndef get_num_tokens(self, prompt: str) -&gt; int:\nreturn len(self.tokenizer.encode(prompt))\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/models/openai.py</code> <pre><code>class OpenAILLM(BaseModel):\nCHAT_ENGINE_MODELS = [\n\"gpt-3.5-turbo\",\n\"gpt-3.5-turbo-0301\",\n\"gpt-3.5-turbo-0613\",\n\"gpt-3.5-turbo-16k\",\n\"gpt-3.5-turbo-16k-0613\",\n\"gpt-4\",\n\"gpt-4-0314\",\n\"gpt-4-32k-0314\",\n\"gpt-4-0613\",\n\"gpt-4-32k\",\n\"gpt-4-32k-0613\",\n\"gpt-4-1106-preview\",\n\"gpt-4-0125-preview\",\n]\nMODELS_WITH_TOKEN_PROBS = [\n\"text-curie-001\",\n\"text-davinci-003\",\n\"gpt-3.5-turbo\",\n\"gpt-3.5-turbo-0301\",\n\"gpt-3.5-turbo-0613\",\n\"gpt-3.5-turbo-16k\",\n\"gpt-3.5-turbo-16k-0613\",\n\"gpt-4\",\n\"gpt-4-0314\",\n\"gpt-4-32k-0314\",\n\"gpt-4-0613\",\n\"gpt-4-32k\",\n\"gpt-4-32k-0613\",\n\"gpt-4-1106-preview\",\n\"gpt-4-0125-preview\",\n]\nJSON_MODE_MODELS = [\n\"gpt-3.5-turbo-0125\",\n\"gpt-3.5-turbo\",\n\"gpt-4-0125-preview\",\n\"gpt-4-1106-preview\",\n\"gpt-4-turbo-preview\",\n]\n# Default parameters for OpenAILLM\nDEFAULT_MODEL = \"gpt-3.5-turbo\"\nDEFAULT_PARAMS_COMPLETION_ENGINE = {\n\"max_tokens\": 1000,\n\"temperature\": 0.0,\n\"model_kwargs\": {\"logprobs\": 1},\n\"request_timeout\": 30,\n}\nDEFAULT_PARAMS_CHAT_ENGINE = {\n\"max_tokens\": 1000,\n\"temperature\": 0.0,\n\"request_timeout\": 30,\n}\nDEFAULT_QUERY_PARAMS_CHAT_ENGINE = {\"logprobs\": True, \"top_logprobs\": 1}\n# Reference: https://openai.com/pricing\nCOST_PER_PROMPT_TOKEN = {\n\"text-davinci-003\": 0.02 / 1000,\n\"text-curie-001\": 0.002 / 1000,\n\"gpt-3.5-turbo\": 0.0015 / 1000,\n\"gpt-3.5-turbo-0301\": 0.0015 / 1000,\n\"gpt-3.5-turbo-0613\": 0.0015 / 1000,\n\"gpt-3.5-turbo-16k\": 0.003 / 1000,\n\"gpt-3.5-turbo-16k-0613\": 0.003 / 1000,\n\"gpt-4\": 0.03 / 1000,\n\"gpt-4-0613\": 0.03 / 1000,\n\"gpt-4-32k\": 0.06 / 1000,\n\"gpt-4-32k-0613\": 0.06 / 1000,\n\"gpt-4-0314\": 0.03 / 1000,\n\"gpt-4-32k-0314\": 0.06 / 1000,\n\"gpt-4-1106-preview\": 0.01 / 1000,\n\"gpt-4-0125-preview\": 0.01 / 1000,\n}\nCOST_PER_COMPLETION_TOKEN = {\n\"text-davinci-003\": 0.02 / 1000,\n\"text-curie-001\": 0.002 / 1000,\n\"gpt-3.5-turbo\": 0.002 / 1000,\n\"gpt-3.5-turbo-0301\": 0.002 / 1000,\n\"gpt-3.5-turbo-0613\": 0.002 / 1000,\n\"gpt-3.5-turbo-16k\": 0.004 / 1000,\n\"gpt-3.5-turbo-16k-0613\": 0.004 / 1000,\n\"gpt-4\": 0.06 / 1000,\n\"gpt-4-0613\": 0.06 / 1000,\n\"gpt-4-32k\": 0.12 / 1000,\n\"gpt-4-32k-0613\": 0.12 / 1000,\n\"gpt-4-0314\": 0.06 / 1000,\n\"gpt-4-32k-0314\": 0.12 / 1000,\n\"gpt-4-1106-preview\": 0.03 / 1000,\n\"gpt-4-0125-preview\": 0.03 / 1000,\n}\n@cached_property\ndef _engine(self) -&gt; str:\nif self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\nreturn \"chat\"\nelse:\nreturn \"completion\"\ndef __init__(self, config: AutolabelConfig, cache: BaseCache = None) -&gt; None:\nsuper().__init__(config, cache)\ntry:\nimport tiktoken\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nexcept ImportError:\nraise ImportError(\n\"openai is required to use the OpenAILLM. Please install it with the following command: pip install 'refuel-autolabel[openai]'\"\n)\nself.tiktoken = tiktoken\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\nif os.getenv(\"OPENAI_API_KEY\") is None:\nraise ValueError(\"OPENAI_API_KEY environment variable not set\")\n# populate model params and initialize the LLM\nmodel_params = config.model_params()\nif config.logit_bias() != 0:\nmodel_params = {\n**model_params,\n**self._generate_logit_bias(),\n}\nif self._engine == \"chat\":\nself.model_params = {**self.DEFAULT_PARAMS_CHAT_ENGINE, **model_params}\nself.query_params = self.DEFAULT_QUERY_PARAMS_CHAT_ENGINE\nself.llm = ChatOpenAI(\nmodel_name=self.model_name, verbose=False, **self.model_params\n)\nif config.json_mode():\nif self.model_name not in self.JSON_MODE_MODELS:\nlogger.warning(\nf\"json_mode is not supported for model {self.model_name}. Disabling json_mode.\"\n)\nelse:\nself.query_params[\"response_format\"] = {\"type\": \"json_object\"}\nelse:\nself.model_params = {\n**self.DEFAULT_PARAMS_COMPLETION_ENGINE,\n**model_params,\n}\nself.llm = OpenAI(\nmodel_name=self.model_name, verbose=False, **self.model_params\n)\ndef _generate_logit_bias(self) -&gt; None:\n\"\"\"Generates logit bias for the labels specified in the config\n        Returns:\n            Dict: logit bias and max tokens\n        \"\"\"\nif len(self.config.labels_list()) == 0:\nlogger.warning(\n\"No labels specified in the config. Skipping logit bias generation.\"\n)\nreturn {}\nencoding = self.tiktoken.encoding_for_model(self.model_name)\nlogit_bias = {}\nmax_tokens = 0\nfor label in self.config.labels_list():\nif label not in logit_bias:\ntokens = encoding.encode(label)\nfor token in tokens:\nlogit_bias[token] = self.config.logit_bias()\nmax_tokens = max(max_tokens, len(tokens))\nlogit_bias[encoding.eot_token] = self.config.logit_bias()\nreturn {\"logit_bias\": logit_bias, \"max_tokens\": max_tokens}\ndef _chat_backward_compatibility(\nself, generations: List[LLMResult]\n) -&gt; List[LLMResult]:\nfor generation_options in generations:\nfor curr_generation in generation_options:\ngeneration_info = curr_generation.generation_info\nnew_logprobs = {\"top_logprobs\": []}\nfor curr_token in generation_info[\"logprobs\"][\"content\"]:\nnew_logprobs[\"top_logprobs\"].append(\n{curr_token[\"token\"]: curr_token[\"logprob\"]}\n)\ncurr_generation.generation_info[\"logprobs\"] = new_logprobs\nreturn generations\nasync def _alabel(self, prompts: List[str]) -&gt; RefuelLLMResult:\ntry:\nstart_time = time()\nif self._engine == \"chat\":\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\nresult = await self.llm.agenerate(prompts, **self.query_params)\ngenerations = self._chat_backward_compatibility(result.generations)\nelse:\nresult = await self.llm.agenerate(prompts)\ngenerations = result.generations\nend_time = time()\nreturn RefuelLLMResult(\ngenerations=generations,\nerrors=[None] * len(generations),\nlatencies=[end_time - start_time] * len(generations),\n)\nexcept Exception as e:\nreturn await self._alabel_individually(prompts)\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\ntry:\nstart_time = time()\nif self._engine == \"chat\":\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\nresult = self.llm.generate(prompts, **self.query_params)\ngenerations = self._chat_backward_compatibility(result.generations)\nelse:\nresult = self.llm.generate(prompts)\ngenerations = result.generations\nend_time = time()\nreturn RefuelLLMResult(\ngenerations=generations,\nerrors=[None] * len(generations),\nlatencies=[end_time - start_time] * len(generations),\n)\nexcept Exception as e:\nreturn self._label_individually(prompts)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nencoding = self.tiktoken.encoding_for_model(self.model_name)\nnum_prompt_toks = len(encoding.encode(prompt))\nif label:\nnum_label_toks = len(encoding.encode(label))\nelse:\n# get an upper bound\nnum_label_toks = self.model_params[\"max_tokens\"]\ncost_per_prompt_token = self.COST_PER_PROMPT_TOKEN[self.model_name]\ncost_per_completion_token = self.COST_PER_COMPLETION_TOKEN[self.model_name]\nreturn (num_prompt_toks * cost_per_prompt_token) + (\nnum_label_toks * cost_per_completion_token\n)\ndef returns_token_probs(self) -&gt; bool:\nreturn (\nself.model_name is not None\nand self.model_name in self.MODELS_WITH_TOKEN_PROBS\n)\ndef get_num_tokens(self, prompt: str) -&gt; int:\nencoding = self.tiktoken.encoding_for_model(self.model_name)\nreturn len(encoding.encode(prompt))\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/models/palm.py</code> <pre><code>class PaLMLLM(BaseModel):\nSEP_REPLACEMENT_TOKEN = \"@@\"\nCHAT_ENGINE_MODELS = [\"chat-bison@001\"]\nDEFAULT_MODEL = \"text-bison@001\"\n# Reference: https://developers.generativeai.google/guide/concepts#model_parameters for \"A token is approximately 4 characters\"\nDEFAULT_PARAMS = {\"temperature\": 0, \"max_output_tokens\": 1000}\n# Reference: https://cloud.google.com/vertex-ai/pricing\nCOST_PER_CHARACTER = {\n\"text-bison@001\": 0.001 / 1000,\n\"chat-bison@001\": 0.0005 / 1000,\n\"textembedding-gecko@001\": 0.0001 / 1000,\n}\n@cached_property\ndef _engine(self) -&gt; str:\nif self.model_name is not None and self.model_name in self.CHAT_ENGINE_MODELS:\nreturn \"chat\"\nelse:\nreturn \"completion\"\ndef __init__(\nself,\nconfig: AutolabelConfig,\ncache: BaseCache = None,\n) -&gt; None:\nsuper().__init__(config, cache)\ntry:\nfrom langchain.chat_models import ChatVertexAI\nfrom langchain.llms import VertexAI\nimport tiktoken\nexcept ImportError:\nraise ImportError(\n\"palm is required to use the Palm LLM. Please install it with the following command: pip install 'refuel-autolabel[google]'\"\n)\n# populate model name\nself.model_name = config.model_name() or self.DEFAULT_MODEL\n# populate model params and initialize the LLM\nmodel_params = config.model_params()\nself.model_params = {\n**self.DEFAULT_PARAMS,\n**model_params,\n}\nif self._engine == \"chat\":\nself.llm = ChatVertexAI(model_name=self.model_name, **self.model_params)\nelse:\nself.llm = VertexAI(model_name=self.model_name, **self.model_params)\nself.tiktoken = tiktoken\n@retry(\nreraise=True,\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=1, min=2, max=10),\nbefore_sleep=before_sleep_log(logger, logging.WARNING),\n)\ndef _label_with_retry(self, prompts: List[str]) -&gt; LLMResult:\nstart_time = time()\nresponse = self.llm.generate(prompts)\nreturn response, time() - start_time\ndef _label_individually(self, prompts: List[str]) -&gt; RefuelLLMResult:\n\"\"\"Label each prompt individually. Should be used only after trying as a batch first.\n        Args:\n            prompts (List[str]): List of prompts to label\n        Returns:\n            RefuelLLMResult: RefuelLLMResult object\n        \"\"\"\ngenerations = []\nerrors = []\nlatencies = []\nfor i, prompt in enumerate(prompts):\ntry:\nresponse, latency = self._label_with_retry([prompt])\nfor generation in response.generations[0]:\ngeneration.text = generation.text.replace(\nself.SEP_REPLACEMENT_TOKEN, \"\\n\"\n)\ngenerations.append(response.generations[0])\nerrors.append(None)\nlatencies.append(latency)\nexcept Exception as e:\nprint(f\"Error generating from LLM: {e}, returning empty generation\")\ngenerations.append([Generation(text=\"\")])\nerrors.append(\nLabelingError(\nerror_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n)\n)\nlatencies.append(0)\nreturn RefuelLLMResult(\ngenerations=generations, errors=errors, latencies=latencies\n)\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\nfor prompt in prompts:\nif self.SEP_REPLACEMENT_TOKEN in prompt:\nlogger.warning(\nf\"\"\"Current prompt contains {self.SEP_REPLACEMENT_TOKEN}                                 which is currently used as a separator token by refuel\n                                llm. It is highly recommended to avoid having any\n                                occurences of this substring in the prompt.\n                            \"\"\"\n)\nprompts = [\nprompt.replace(\"\\n\", self.SEP_REPLACEMENT_TOKEN) for prompt in prompts\n]\nif self._engine == \"chat\":\n# Need to convert list[prompts] -&gt; list[messages]\n# Currently the entire prompt is stuck into the \"human message\"\n# We might consider breaking this up into human vs system message in future\nprompts = [[HumanMessage(content=prompt)] for prompt in prompts]\ntry:\nstart_time = time()\nresult = self._label_with_retry(prompts)\nend_time = time()\nfor generations in result.generations:\nfor generation in generations:\ngeneration.text = generation.text.replace(\nself.SEP_REPLACEMENT_TOKEN, \"\\n\"\n)\nreturn RefuelLLMResult(\ngenerations=result.generations,\nerrors=[None] * len(result.generations),\nlatencies=[end_time - start_time] * len(result.generations),\n)\nexcept Exception as e:\nreturn self._label_individually(prompts)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nif self.model_name is None:\nreturn 0.0\ncost_per_char = self.COST_PER_CHARACTER.get(self.model_name, 0.0)\nreturn cost_per_char * len(prompt) + cost_per_char * (\nlen(label) if label else 4 * self.model_params[\"max_output_tokens\"]\n)\ndef returns_token_probs(self) -&gt; bool:\nreturn False\ndef get_num_tokens(self, prompt: str) -&gt; int:\n# TODO(dhruva): Replace with actual tokenizer once that is available\nencoding = self.tiktoken.encoding_for_model(\"gpt2\")\nreturn len(encoding.encode(prompt))\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/models/refuel.py</code> <pre><code>class RefuelLLM(BaseModel):\nDEFAULT_TOKENIZATION_MODEL = \"NousResearch/Llama-2-13b-chat-hf\"\nDEFAULT_CONTEXT_LENGTH = 3250\nDEFAULT_CONNECT_TIMEOUT = 10\nDEFAULT_READ_TIMEOUT = 120\nDEFAULT_PARAMS = {\n\"max_new_tokens\": 128,\n}\ndef __init__(\nself,\nconfig: AutolabelConfig,\ncache: BaseCache = None,\n) -&gt; None:\nsuper().__init__(config, cache)\ntry:\nfrom transformers import AutoTokenizer\nexcept Exception as e:\nraise Exception(\n\"Unable to import transformers. Please install transformers to use RefuelLLM\"\n)\n# populate model name\n# This is unused today, but in the future could\n# be used to decide which refuel model is queried\nself.model_name = config.model_name()\nmodel_params = config.model_params()\nself.model_params = {**self.DEFAULT_PARAMS, **model_params}\nself.tokenizer = AutoTokenizer.from_pretrained(self.DEFAULT_TOKENIZATION_MODEL)\n# initialize runtime\nself.BASE_API = f\"https://llm.refuel.ai/models/{self.model_name}/generate\"\nself.REFUEL_API_ENV = \"REFUEL_API_KEY\"\nif self.REFUEL_API_ENV in os.environ and os.environ[self.REFUEL_API_ENV]:\nself.REFUEL_API_KEY = os.environ[self.REFUEL_API_ENV]\nelse:\nraise ValueError(\nf\"Did not find {self.REFUEL_API_ENV}, please add an environment variable\"\nf\" `{self.REFUEL_API_ENV}` which contains it\"\n)\n@retry(\nreraise=True,\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=1, min=2, max=10),\nbefore_sleep=before_sleep_log(logger, logging.WARNING),\nretry=retry_if_not_exception_type(UnretryableError),\n)\ndef _label_with_retry(self, prompt: str) -&gt; Tuple[requests.Response, float]:\npayload = {\n\"input\": prompt,\n\"params\": {**self.model_params},\n\"confidence\": self.config.confidence(),\n}\nheaders = {\"refuel_api_key\": self.REFUEL_API_KEY}\nstart_time = time()\nresponse = requests.post(\nself.BASE_API,\njson=payload,\nheaders=headers,\ntimeout=(self.DEFAULT_CONNECT_TIMEOUT, self.DEFAULT_READ_TIMEOUT),\n)\nend_time = time()\n# raise Exception if status != 200\nif response.status_code != 200:\nif response.status_code in UNRETRYABLE_ERROR_CODES:\n# This is a bad request, and we should not retry\nraise UnretryableError(\nf\"NonRetryable Error: Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n)\nlogger.warning(\nf\"Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n)\nresponse.raise_for_status()\nreturn response, end_time - start_time\n@retry(\nreraise=True,\nstop=stop_after_attempt(5),\nwait=wait_exponential(multiplier=1, min=2, max=10),\nbefore_sleep=before_sleep_log(logger, logging.WARNING),\nretry=retry_if_not_exception_type(UnretryableError),\n)\nasync def _alabel_with_retry(self, prompt: str) -&gt; Tuple[requests.Response, float]:\npayload = {\n\"input\": prompt,\n\"params\": {**self.model_params},\n\"confidence\": self.config.confidence(),\n}\nheaders = {\"refuel_api_key\": self.REFUEL_API_KEY}\nasync with httpx.AsyncClient() as client:\ntimeout = httpx.Timeout(\nself.DEFAULT_CONNECT_TIMEOUT, read=self.DEFAULT_READ_TIMEOUT\n)\nstart_time = time()\nresponse = await client.post(\nself.BASE_API, json=payload, headers=headers, timeout=timeout\n)\nend_time = time()\n# raise Exception if status != 200\nif response.status_code != 200:\nif response.status_code in UNRETRYABLE_ERROR_CODES:\n# This is a bad request, and we should not retry\nraise UnretryableError(\nf\"NonRetryable Error: Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n)\nlogger.warning(\nf\"Received status code {response.status_code} from Refuel API. Response: {response.text}\"\n)\nresponse.raise_for_status()\nreturn response, end_time - start_time\ndef _label(self, prompts: List[str]) -&gt; RefuelLLMResult:\ngenerations = []\nerrors = []\nlatencies = []\nfor prompt in prompts:\ntry:\nresponse, latency = self._label_with_retry(prompt)\nresponse = json.loads(response.json())\ngenerations.append(\n[\nGeneration(\ntext=response[\"generated_text\"],\ngeneration_info=(\n{\"logprobs\": {\"top_logprobs\": response[\"logprobs\"]}}\nif self.config.confidence()\nelse None\n),\n)\n]\n)\nerrors.append(None)\nlatencies.append(latency)\nexcept Exception as e:\n# This signifies an error in generating the response using RefuelLLm\nlogger.error(\nf\"Unable to generate prediction: {e}\",\n)\ngenerations.append([Generation(text=\"\")])\nerrors.append(\nLabelingError(\nerror_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n)\n)\nlatencies.append(0)\nreturn RefuelLLMResult(\ngenerations=generations, errors=errors, latencies=latencies\n)\nasync def _alabel(self, prompts: List[str]) -&gt; RefuelLLMResult:\ngenerations = []\nerrors = []\nlatencies = []\ntry:\nrequests = [self._alabel_with_retry(prompt) for prompt in prompts]\nresponses = await asyncio.gather(*requests)\nfor response, latency in responses:\nresponse = json.loads(response.json())\ngenerations.append(\n[\nGeneration(\ntext=response[\"generated_text\"],\ngeneration_info=(\n{\"logprobs\": {\"top_logprobs\": response[\"logprobs\"]}}\nif self.config.confidence()\nelse None\n),\n)\n]\n)\nerrors.append(None)\nlatencies.append(latency)\nexcept Exception as e:\n# This signifies an error in generating the response using RefuelLLm\nlogger.error(\nf\"Unable to generate prediction: {e}\",\n)\ngenerations.append([Generation(text=\"\")])\nerrors.append(\nLabelingError(\nerror_type=ErrorType.LLM_PROVIDER_ERROR, error_message=str(e)\n)\n)\nlatencies.append(0)\nreturn RefuelLLMResult(\ngenerations=generations, errors=errors, latencies=latencies\n)\ndef get_cost(self, prompt: str, label: Optional[str] = \"\") -&gt; float:\nreturn 0\ndef returns_token_probs(self) -&gt; bool:\nreturn True\ndef get_num_tokens(self, prompt: str) -&gt; int:\nreturn len(self.tokenizer.encode(prompt))\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.base.BaseModel.get_cached_prompts","title":"<code>get_cached_prompts(prompts)</code>","text":"<p>Get prompts that are already cached.</p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>def get_cached_prompts(self, prompts: List[str]) -&gt; Optional[str]:\n\"\"\"Get prompts that are already cached.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nmissing_prompts = []\nmissing_prompt_idxs = []\nexisting_prompts = {}\nfor i, prompt in enumerate(prompts):\ncache_entry = GenerationCacheEntry(\nprompt=prompt,\nmodel_name=self.model_name,\nmodel_params=model_params_string,\n)\ncache_val = self.cache.lookup(cache_entry)\nif cache_val:\nexisting_prompts[i] = cache_val\nelse:\nmissing_prompts.append(prompt)\nmissing_prompt_idxs.append(i)\nreturn (\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n)\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.base.BaseModel.get_num_tokens","title":"<code>get_num_tokens(prompt)</code>  <code>abstractmethod</code>","text":"<p>Get the number of tokens in the prompt</p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>@abstractmethod\ndef get_num_tokens(self, prompt: str) -&gt; int:\n\"\"\"\n    Get the number of tokens in the prompt\"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.base.BaseModel.label","title":"<code>label(prompts)</code>  <code>async</code>","text":"<p>Label a list of prompts.</p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>async def label(self, prompts: List[str]) -&gt; RefuelLLMResult:\n\"\"\"Label a list of prompts.\"\"\"\nexisting_prompts = {}\nmissing_prompt_idxs = list(range(len(prompts)))\nmissing_prompts = prompts\ncosts = []\nerrors = [None for i in range(len(prompts))]\nlatencies = [0 for i in range(len(prompts))]\nif self.cache:\n(\nexisting_prompts,\nmissing_prompt_idxs,\nmissing_prompts,\n) = self.get_cached_prompts(prompts)\n# label missing prompts\nif len(missing_prompts) &gt; 0:\nif hasattr(self, \"_alabel\"):\nnew_results = await self._alabel(missing_prompts)\nelse:\nnew_results = self._label(missing_prompts)\nfor ind, prompt in enumerate(missing_prompts):\ncosts.append(\nself.get_cost(prompt, label=new_results.generations[ind][0].text)\n)\n# Set the existing prompts to the new results\nfor i, result, error, latency in zip(\nmissing_prompt_idxs,\nnew_results.generations,\nnew_results.errors,\nnew_results.latencies,\n):\nexisting_prompts[i] = result\nerrors[i] = error\nlatencies[i] = latency\nif self.cache:\nself.update_cache(missing_prompt_idxs, new_results, prompts)\ngenerations = [existing_prompts[i] for i in range(len(prompts))]\nreturn RefuelLLMResult(\ngenerations=generations, costs=costs, errors=errors, latencies=latencies\n)\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.base.BaseModel.returns_token_probs","title":"<code>returns_token_probs()</code>  <code>abstractmethod</code>","text":"<p>Whether the LLM supports returning logprobs of generated tokens</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether the LLM returns supports returning logprobs of generated tokens</p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>@abstractmethod\ndef returns_token_probs(self) -&gt; bool:\n\"\"\"Whether the LLM supports returning logprobs of generated tokens\n    Returns:\n        bool: whether the LLM returns supports returning logprobs of generated tokens\n    \"\"\"\npass\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.base.BaseModel.update_cache","title":"<code>update_cache(missing_prompt_idxs, new_results, prompts)</code>","text":"<p>Update the cache with new results.</p> Source code in <code>autolabel/src/autolabel/models/base.py</code> <pre><code>def update_cache(self, missing_prompt_idxs, new_results, prompts):\n\"\"\"Update the cache with new results.\"\"\"\nmodel_params_string = str(\nsorted([(k, v) for k, v in self.model_params.items()])\n)\nfor i, result, error in zip(\nmissing_prompt_idxs, new_results.generations, new_results.errors\n):\n# If there was an error, don't cache the result\nif error is not None:\ncontinue\ncache_entry = GenerationCacheEntry(\nprompt=prompts[i],\nmodel_name=self.model_name,\nmodel_params=model_params_string,\ngenerations=result,\nttl_ms=self.TTL_MS,\n)\nself.cache.update(cache_entry)\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.</p> Source code in <code>autolabel/src/autolabel/models/__init__.py</code> <pre><code>class ModelFactory:\n\"\"\"The ModelFactory class is used to create a BaseModel object from the given AutoLabelConfig configuration.\"\"\"\n@staticmethod\ndef from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n\"\"\"\n        Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n        Args:\n            config: AutolabelConfig object containing project settings\n            cache: cache allows for saving results in between labeling runs for future use\n        Returns:\n            model: a fully configured BaseModel object\n        \"\"\"\nprovider = ModelProvider(config.provider())\ntry:\nmodel_cls = MODEL_REGISTRY[provider]\nmodel_obj = model_cls(config=config, cache=cache)\n# The below ensures that users should based off of the BaseModel\n# when creating/registering custom models.\nassert isinstance(\nmodel_obj, BaseModel\n), f\"{model_obj} should inherit from autolabel.models.BaseModel\"\nexcept KeyError as e:\n# We should never get here as the config should have already\n# been validated by the pydantic model.\nlogger.error(\nf\"{config.provider()} is not in the list of supported providers: \\\n{list(ModelProvider.__members__.keys())}\"\n)\nraise e\nreturn model_obj\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.ModelFactory.from_config","title":"<code>from_config(config, cache=None)</code>  <code>staticmethod</code>","text":"<p>Returns a BaseModel object configured with the settings found in the provided AutolabelConfig. Args:     config: AutolabelConfig object containing project settings     cache: cache allows for saving results in between labeling runs for future use Returns:     model: a fully configured BaseModel object</p> Source code in <code>autolabel/src/autolabel/models/__init__.py</code> <pre><code>@staticmethod\ndef from_config(config: AutolabelConfig, cache: BaseCache = None) -&gt; BaseModel:\n\"\"\"\n    Returns a BaseModel object configured with the settings found in the provided AutolabelConfig.\n    Args:\n        config: AutolabelConfig object containing project settings\n        cache: cache allows for saving results in between labeling runs for future use\n    Returns:\n        model: a fully configured BaseModel object\n    \"\"\"\nprovider = ModelProvider(config.provider())\ntry:\nmodel_cls = MODEL_REGISTRY[provider]\nmodel_obj = model_cls(config=config, cache=cache)\n# The below ensures that users should based off of the BaseModel\n# when creating/registering custom models.\nassert isinstance(\nmodel_obj, BaseModel\n), f\"{model_obj} should inherit from autolabel.models.BaseModel\"\nexcept KeyError as e:\n# We should never get here as the config should have already\n# been validated by the pydantic model.\nlogger.error(\nf\"{config.provider()} is not in the list of supported providers: \\\n{list(ModelProvider.__members__.keys())}\"\n)\nraise e\nreturn model_obj\n</code></pre>"},{"location":"autolabel/reference/models/#src.autolabel.models.register_model","title":"<code>register_model(name, model_cls)</code>","text":"<p>Register Model class</p> Source code in <code>autolabel/src/autolabel/models/__init__.py</code> <pre><code>def register_model(name, model_cls):\n\"\"\"Register Model class\"\"\"\nMODEL_REGISTRY[name] = model_cls\n</code></pre>"},{"location":"autolabel/reference/schema/","title":"Schema","text":""},{"location":"autolabel/reference/schema/#src.autolabel.schema.AggregationFunction","title":"<code>AggregationFunction</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported aggregation functions</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class AggregationFunction(str, Enum):\n\"\"\"Enum of supported aggregation functions\"\"\"\nMAX = \"max\"\nMEAN = \"mean\"\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ConfidenceCacheEntry","title":"<code>ConfidenceCacheEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class ConfidenceCacheEntry(BaseModel):\nprompt: Optional[str] = \"\"\nraw_response: Optional[str] = \"\"\nlogprobs: Optional[list] = None\nscore_type: Optional[str] = \"logprob_average\"\ncreation_time_ms: Optional[int] = -1\nttl_ms: Optional[int] = -1\nclass Config:\norm_mode = True\ndef get_id(self) -&gt; str:\n\"\"\"\n        Generates a unique ID for the given confidence cache configuration\n        \"\"\"\nreturn calculate_md5([self.prompt, self.raw_response, self.score_type])\ndef get_serialized_output(self) -&gt; str:\n\"\"\"\n        Returns the serialized cache entry output\n        \"\"\"\nreturn json.dumps(self.logprobs)\ndef deserialize_output(self, output: str) -&gt; Dict[str, float]:\n\"\"\"\n        Deserializes the cache entry output\n        \"\"\"\nreturn json.loads(output)\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.deserialize_output","title":"<code>deserialize_output(output)</code>","text":"<p>Deserializes the cache entry output</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def deserialize_output(self, output: str) -&gt; Dict[str, float]:\n\"\"\"\n    Deserializes the cache entry output\n    \"\"\"\nreturn json.loads(output)\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.get_id","title":"<code>get_id()</code>","text":"<p>Generates a unique ID for the given confidence cache configuration</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def get_id(self) -&gt; str:\n\"\"\"\n    Generates a unique ID for the given confidence cache configuration\n    \"\"\"\nreturn calculate_md5([self.prompt, self.raw_response, self.score_type])\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ConfidenceCacheEntry.get_serialized_output","title":"<code>get_serialized_output()</code>","text":"<p>Returns the serialized cache entry output</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def get_serialized_output(self) -&gt; str:\n\"\"\"\n    Returns the serialized cache entry output\n    \"\"\"\nreturn json.dumps(self.logprobs)\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ErrorType","title":"<code>ErrorType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported error types</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class ErrorType(str, Enum):\n\"\"\"Enum of supported error types\"\"\"\nLLM_PROVIDER_ERROR = \"llm_provider_error\"\nPARSING_ERROR = \"parsing_error\"\nOUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR = \"output_guidelines_not_followed_error\"\nEMPTY_RESPONSE_ERROR = \"empty_response_error\"\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.FewShotAlgorithm","title":"<code>FewShotAlgorithm</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class FewShotAlgorithm(str, Enum):\n\"\"\"Enum of supported algorithms for choosing which examples to provide the LLM in its instruction prompt\"\"\"\nFIXED = \"fixed\"\nSEMANTIC_SIMILARITY = \"semantic_similarity\"\nMAX_MARGINAL_RELEVANCE = \"max_marginal_relevance\"\nLABEL_DIVERSITY_RANDOM = \"label_diversity_random\"\nLABEL_DIVERSITY_SIMILARITY = \"label_diversity_similarity\"\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.GenerationCacheEntry","title":"<code>GenerationCacheEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class GenerationCacheEntry(BaseModel):\nmodel_name: str\nprompt: str\nmodel_params: str\ngenerations: Optional[List[Union[Generation, ChatGeneration]]] = None\ncreation_time_ms: Optional[int] = -1\nttl_ms: Optional[int] = -1\nclass Config:\norm_mode = True\ndef get_id(self) -&gt; str:\n\"\"\"\n        Generates a unique ID for the given generation cache configuration\n        \"\"\"\nreturn calculate_md5([self.model_name, self.model_params, self.prompt])\ndef get_serialized_output(self) -&gt; str:\n\"\"\"\n        Returns the serialized cache entry output\n        \"\"\"\nreturn json.dumps([gen.dict() for gen in self.generations])\ndef deserialize_output(\nself, output: str\n) -&gt; List[Union[Generation, ChatGeneration]]:\n\"\"\"\n        Deserializes the cache entry output\n        \"\"\"\ngenerations = [\nGeneration(**gen) if gen[\"type\"] == \"Generation\" else ChatGeneration(**gen)\nfor gen in json.loads(output)\n]\nreturn generations\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.GenerationCacheEntry.deserialize_output","title":"<code>deserialize_output(output)</code>","text":"<p>Deserializes the cache entry output</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def deserialize_output(\nself, output: str\n) -&gt; List[Union[Generation, ChatGeneration]]:\n\"\"\"\n    Deserializes the cache entry output\n    \"\"\"\ngenerations = [\nGeneration(**gen) if gen[\"type\"] == \"Generation\" else ChatGeneration(**gen)\nfor gen in json.loads(output)\n]\nreturn generations\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.GenerationCacheEntry.get_id","title":"<code>get_id()</code>","text":"<p>Generates a unique ID for the given generation cache configuration</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def get_id(self) -&gt; str:\n\"\"\"\n    Generates a unique ID for the given generation cache configuration\n    \"\"\"\nreturn calculate_md5([self.model_name, self.model_params, self.prompt])\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.GenerationCacheEntry.get_serialized_output","title":"<code>get_serialized_output()</code>","text":"<p>Returns the serialized cache entry output</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>def get_serialized_output(self) -&gt; str:\n\"\"\"\n    Returns the serialized cache entry output\n    \"\"\"\nreturn json.dumps([gen.dict() for gen in self.generations])\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.LLMAnnotation","title":"<code>LLMAnnotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class LLMAnnotation(BaseModel):\n\"\"\"Contains label information of a given data point, including the generated label, the prompt given to the LLM, and the LLMs response. Optionally includes a confidence_score if supported by the model\"\"\"\nsuccessfully_labeled: bool\nlabel: Any\ncurr_sample: Optional[bytes] = \"\"\nconfidence_score: Optional[float] = None\ngeneration_info: Optional[Dict[str, Any]] = None\nraw_response: Optional[str] = \"\"\nexplanation: Optional[str] = \"\"\nprompt: Optional[str] = \"\"\nconfidence_prompt: Optional[str] = \"\"\ninput_tokens: Optional[int] = None\noutput_tokens: Optional[int] = None\ncost: Optional[float] = None\nlatency: Optional[float] = None\nerror: Optional[LabelingError] = None\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.LabelingError","title":"<code>LabelingError</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains information about an error that occurred during the labeling process</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class LabelingError(BaseModel):\n\"\"\"Contains information about an error that occurred during the labeling process\"\"\"\nerror_type: ErrorType\nerror_message: str\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.MetricResult","title":"<code>MetricResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Contains performance metrics gathered from autolabeler runs</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class MetricResult(BaseModel):\n\"\"\"Contains performance metrics gathered from autolabeler runs\"\"\"\nname: str\nvalue: Any\nshow_running: Optional[bool] = True\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.MetricType","title":"<code>MetricType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class MetricType(str, Enum):\n\"\"\"Enum of supported performance metrics. Some metrics are always available (task agnostic), while others are only supported by certain types of tasks\"\"\"\n# Task agnostic\nSUPPORT = \"support\"\nCOMPLETION_RATE = \"completion_rate\"\n# Classification metrics\nACCURACY = \"accuracy\"\nCONFUSION_MATRIX = \"confusion_matrix\"\nLABEL_DISTRIBUTION = \"label_distribution\"\nF1 = \"f1\"\nF1_MICRO = \"f1_micro\"\nF1_MACRO = \"f1_macro\"\nF1_WEIGHTED = \"f1_weighted\"\nTEXT_PARTIAL_MATCH = \"text_partial_match\"\n# Token Classification metrics\nF1_EXACT = \"f1_exact\"\nF1_STRICT = \"f1_strict\"\nF1_PARTIAL = \"f1_partial\"\nF1_ENT_TYPE = \"f1_ent_type\"\n# Confidence metrics\nAUROC = \"auroc\"\nTHRESHOLD = \"threshold\"\n# Aggregate Metrics\nCLASSIFICATION_REPORT = \"classification_report\"\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all LLM providers currently supported by autolabeler</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class ModelProvider(str, Enum):\n\"\"\"Enum containing all LLM providers currently supported by autolabeler\"\"\"\nOPENAI = \"openai\"\nOPENAI_VISION = \"openai_vision\"\nANTHROPIC = \"anthropic\"\nHUGGINGFACE_PIPELINE = \"huggingface_pipeline\"\nHUGGINGFACE_PIPELINE_VISION = \"huggingface_pipeline_vision\"\nREFUEL = \"refuel\"\nGOOGLE = \"google\"\nCOHERE = \"cohere\"\nCUSTOM = \"custom\"\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.RefuelLLMResult","title":"<code>RefuelLLMResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>List of generated outputs. This is a List[List[]] because each input could have multiple candidate generations.</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class RefuelLLMResult(BaseModel):\n\"\"\"List of generated outputs. This is a List[List[]] because\n    each input could have multiple candidate generations.\"\"\"\ngenerations: List[List[Union[Generation, ChatGeneration]]]\n\"\"\"Errors encountered while running the labeling job\"\"\"\nerrors: List[Optional[LabelingError]]\n\"\"\"Costs incurred during the labeling job\"\"\"\ncosts: Optional[List[float]] = []\n\"\"\"Latencies incurred during the labeling job\"\"\"\nlatencies: Optional[List[float]] = []\n</code></pre>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.RefuelLLMResult.costs","title":"<code>costs: Optional[List[float]] = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Latencies incurred during the labeling job</p>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.RefuelLLMResult.errors","title":"<code>errors: List[Optional[LabelingError]]</code>  <code>instance-attribute</code>","text":"<p>Costs incurred during the labeling job</p>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.RefuelLLMResult.generations","title":"<code>generations: List[List[Union[Generation, ChatGeneration]]]</code>  <code>instance-attribute</code>","text":"<p>Errors encountered while running the labeling job</p>"},{"location":"autolabel/reference/schema/#src.autolabel.schema.TaskType","title":"<code>TaskType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum containing all the types of tasks that autolabeler currently supports</p> Source code in <code>autolabel/src/autolabel/schema.py</code> <pre><code>class TaskType(str, Enum):\n\"\"\"Enum containing all the types of tasks that autolabeler currently supports\"\"\"\nCLASSIFICATION = \"classification\"\nNAMED_ENTITY_RECOGNITION = \"named_entity_recognition\"\nQUESTION_ANSWERING = \"question_answering\"\nENTITY_MATCHING = \"entity_matching\"\nMULTILABEL_CLASSIFICATION = \"multilabel_classification\"\nATTRIBUTE_EXTRACTION = \"attribute_extraction\"\n</code></pre>"},{"location":"autolabel/reference/tasks/","title":"Tasks","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>autolabel/src/autolabel/tasks/base.py</code> <pre><code>class BaseTask(ABC):\nZERO_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nNow I want you to label the following example:\\n{current_example}\"\nFEW_SHOT_TEMPLATE = \"{task_guidelines}\\n\\n{output_guidelines}\\n\\nSome examples with their output answers are provided below:\\n\\n{seed_examples}\\n\\nNow I want you to label the following example:\\n{current_example}\"\nZERO_SHOT_TEMPLATE_REFUEL_LLM = \"\"\"\n    &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n{task_guidelines}\\n{output_guidelines}\n    &lt;&lt;/SYS&gt;&gt;\n{current_example}[/INST]\n    \"\"\"\nFEW_SHOT_TEMPLATE_REFUEL_LLM = \"\"\"\n    &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n{task_guidelines}\\n{output_guidelines}\\n{seed_examples}\n    &lt;&lt;/SYS&gt;&gt;\n{current_example}[/INST]\n    \"\"\"\n# Downstream classes should override these\nNULL_LABEL_TOKEN = \"NO_LABEL\"\nDEFAULT_TASK_GUIDELINES = \"\"\nDEFAULT_OUTPUT_GUIDELINES = \"\"\nDEFAULT_DATASET_GENERATION_GUIDELINES = \"\"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nself.config = config\nself.image_col = self.config.image_column()\n# Update the default prompt template with the prompt template from the config\nself.task_guidelines = (\nself.config.task_guidelines() or self.DEFAULT_TASK_GUIDELINES\n)\nself.output_guidelines = (\nself.config.output_guidelines() or self.DEFAULT_OUTPUT_GUIDELINES\n)\nself.dataset_generation_guidelines = (\nself.config.dataset_generation_guidelines()\nor self.DEFAULT_DATASET_GENERATION_GUIDELINES\n)\nself._prompt_schema_init()\ndef _prompt_schema_init(self) -&gt; None:\nself.use_refuel_prompt_schema = self.config.provider() == ModelProvider.REFUEL\nif self._is_few_shot_mode():\nself.example_template = (\nself.FEW_SHOT_TEMPLATE_REFUEL_LLM\nif self.use_refuel_prompt_schema\nelse self.FEW_SHOT_TEMPLATE\n)\nelse:\nself.example_template = (\nself.ZERO_SHOT_TEMPLATE_REFUEL_LLM\nif self.use_refuel_prompt_schema\nelse self.ZERO_SHOT_TEMPLATE\n)\nself.prompt_template = PromptTemplate(\ninput_variables=get_format_variables(self.example_template),\ntemplate=self.example_template,\n)\ndef _is_few_shot_mode(self) -&gt; bool:\nreturn self.config.few_shot_algorithm() in [x.value for x in FewShotAlgorithm]\n@abstractmethod\ndef construct_prompt(\nself,\ninput: str,\nexamples: List,\nprompt_template_override: PromptTemplate = None,\nrefuel_prompt_override: bool = False,\noutput_guidelines_override: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n**kwargs,\n) -&gt; str:\npass\ndef construct_confidence_prompt(self, input: str, examples: List, **kwargs) -&gt; str:\ncurr_template = (\nself.FEW_SHOT_TEMPLATE_REFUEL_LLM\nif self._is_few_shot_mode()\nelse self.ZERO_SHOT_TEMPLATE_REFUEL_LLM\n)\nprompt_template = PromptTemplate(\ninput_variables=get_format_variables(curr_template),\ntemplate=curr_template,\n)\nrefuel_prompt = self.construct_prompt(\ninput=input,\nexamples=examples,\nprompt_template_override=prompt_template,\nrefuel_prompt_override=True,\n**kwargs,\n)\nreturn refuel_prompt\ndef trim_prompt(\nself,\nprompt_template: str,\ntask_guidelines: str,\noutput_guidelines: str,\ncurrent_example: str,\nseed_examples: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n) -&gt; str:\ncomplete_prompt = prompt_template.format(\ntask_guidelines=task_guidelines,\noutput_guidelines=output_guidelines,\nseed_examples=seed_examples,\ncurrent_example=current_example,\n)\nif not max_input_tokens or not get_num_tokens:\nreturn complete_prompt\ntrimming_priority = [\nseed_examples,\ntask_guidelines,\noutput_guidelines,\ncurrent_example,\n]\ntrimmed_elements = {key: key for key in trimming_priority if key is not None}\nfor trimming_candidate in trimming_priority:\ncurrent_prompt_length = get_num_tokens(complete_prompt)\nif current_prompt_length &lt;= max_input_tokens:\nbreak\nif trimming_candidate is None:\ncontinue\nextra_tokens = current_prompt_length - max_input_tokens\ntrimming_candidate_tokens = get_num_tokens(trimming_candidate)\nmax_chars = (\nfloat(len(trimming_candidate))\n* (trimming_candidate_tokens - extra_tokens - 1)\n/ (trimming_candidate_tokens + 1)\n)\nfinal_candidate_chars = int(max(0, max_chars))\ntrimmed_elements[trimming_candidate] = trimming_candidate[\n:final_candidate_chars\n]\ncomplete_prompt = prompt_template.format(\ntask_guidelines=trimmed_elements[task_guidelines],\noutput_guidelines=trimmed_elements[output_guidelines],\nseed_examples=(\ntrimmed_elements[seed_examples]\nif seed_examples is not None\nelse None\n),\ncurrent_example=trimmed_elements[current_example],\n)\nreturn complete_prompt\n@abstractmethod\ndef eval(\nself,\nllm_labels: List,\ngt_labels: List,\nadditional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\npass\n@abstractmethod\ndef get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\nraise NotImplementedError(\n\"Explanation generation not implemented for this task\"\n)\n@abstractmethod\ndef get_generate_dataset_prompt(\nself, label: str, num_rows: int, guidelines: str = None\n) -&gt; str:\nraise NotImplementedError(\"Dataset generation not implemented for this task\")\ndef parse_llm_response(\nself,\nresponse: Union[Generation, ChatGeneration],\ncurr_sample: Dict,\nprompt: str,\n) -&gt; LLMAnnotation:\n# The last line of the response is the label\n# This is done to handle the case where the model generates an explanation before generating the label\nerror = None\nif self.config.chain_of_thought():\ntry:\nexplanation = response.text.strip().split(\"\\n\")[0].strip()\ncompletion_text = extract_valid_json_substring(\nresponse.text.strip().split(\"\\n\")[-1].strip()\n)\ncompletion_text = json.loads(completion_text)[\"label\"]\nexcept:\ncompletion_text = None\nelse:\ncompletion_text = response.text.strip().split(\"\\n\")[-1].strip()\nif len(response.text.strip()) == 0:\nsuccessfully_labeled = False\nllm_label = self.NULL_LABEL_TOKEN\nlogger.warning(\"LLM response is empty\")\nerror = LabelingError(\nerror_type=ErrorType.EMPTY_RESPONSE_ERROR,\nerror_message=\"Empty response from LLM\",\n)\nelif not completion_text:\nsuccessfully_labeled = False\nllm_label = self.NULL_LABEL_TOKEN\nlogger.warning(f\"Error parsing LLM response: {response.text}\")\nerror = LabelingError(\nerror_type=ErrorType.PARSING_ERROR,\nerror_message=f\"Error parsing LLM response: {response.text}\",\n)\nelse:\nllm_label = completion_text.strip()\nif self.config.task_type() in [\nTaskType.CLASSIFICATION,\nTaskType.ENTITY_MATCHING,\n]:\nif llm_label in self.config.labels_list():\nsuccessfully_labeled = True\nelse:\nlogger.warning(f\"LLM response is not in the labels list\")\nllm_label = self.NULL_LABEL_TOKEN\nsuccessfully_labeled = False\nerror = LabelingError(\nerror_type=ErrorType.OUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR,\nerror_message=f\"LLM response is not in the labels list: {llm_label}\",\n)\nelif self.config.task_type() == TaskType.MULTILABEL_CLASSIFICATION:\nllm_multi_labels = llm_label.split(self.config.label_separator())\nllm_multi_labels = list(\nfilter(\nlambda label: label in self.config.labels_list(),\nllm_multi_labels,\n)\n)\nllm_multi_labels = list(set(llm_multi_labels))\nif len(llm_multi_labels) == 0:\nllm_label = self.NULL_LABEL_TOKEN\nsuccessfully_labeled = False\nerror = LabelingError(\nerror_type=ErrorType.OUTPUT_GUIDELINES_NOT_FOLLOWED_ERROR,\nerror_message=f\"LLM response is not in the labels list: {llm_label}\",\n)\nelse:\nllm_label = self.config.label_separator().join(llm_multi_labels)\nsuccessfully_labeled = True\nelse:\nsuccessfully_labeled = True\nreturn LLMAnnotation(\nsuccessfully_labeled=successfully_labeled,\nlabel=llm_label,\ngeneration_info=response.generation_info,\nraw_response=response.text,\nprompt=prompt,\ncurr_sample=pickle.dumps(curr_sample),\nexplanation=explanation if self.config.chain_of_thought() else \"\",\nerror=error,\n)\n</code></pre> <p>               Bases: <code>BaseTask</code></p> Source code in <code>autolabel/src/autolabel/tasks/classification.py</code> <pre><code>class ClassificationTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer with just one element: \"the correct label\"'\n)\nDEFAULT_TASK_GUIDELINES = \"Your job is to correctly label the provided input example into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n\"\nLABEL_FORMAT_IN_EXPLANATION = (\n\" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n)\nEXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\nGENERATE_EXPLANATION_PROMPT = \"Your job is to provide an explanation for why a specific output might have been generated for a task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation with at most 2 sentences.{label_format}\\n{labeled_example}\\nExplanation: \"\nGENERATE_DATASET_TEMPLATE = \"{guidelines}\\n\\nThe inputs must be diverse, covering a wide range of scenarios. You will not generate duplicate inputs. These inputs should be organized in rows in csv format with the columns {columns}.\\n\\n{label_descriptions}\\n\\n{format_guidelines}\\n\\n{output_guidelines}\\n\\n```csv\"\nDEFAULT_DATASET_GENERATION_GUIDELINES = \"You are an expert at generating plausible inputs for a given task.\\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\"\nLABEL_DESCRIPTIONS_PROMPT = \"Each input should fall into one of these {num_labels} categories. These are the only categories that the inputs can belong to.\"\nGENERATE_DATASET_FORMAT_GUIDELINES = \"Your response should be in csv format with the following columns: {columns}.\\n\\nHere is a template you can follow for your output:\\n```csv\\n{columns}\\n{example_rows}\\n```\\n\\nMake sure to replace the placeholder variables with your own values.\"\nGENERATE_DATASET_OUTPUT_GUIDELINES = 'Now I want you to generate {num_rows} excerpts that follow the guidelines and all belong to the \"{label}\" category. They should not belong to any of the other categories.'\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\nself.metrics = [\nAccuracyMetric(),\nSupportMetric(),\nCompletionRateMetric(),\nClassificationReportMetric(),\n]\nif self.config.confidence():\nself.metrics.append(AUROCMetric())\nfor label in self.config.labels_list():\nif \"\\n\" in label:\nlogger.warning(\n\"Label contains newline character. This can have output guideline issues.\"\n)\ndef construct_prompt(\nself,\ninput: Dict,\nexamples: List,\nselected_labels: List[str] = None,\nprompt_template_override: PromptTemplate = None,\nrefuel_prompt_override: bool = False,\noutput_guidelines_override: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n**kwargs,\n) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare task guideline\nlabels_list = (\nself.config.labels_list() if not selected_labels else selected_labels\n)\nnum_labels = len(labels_list)\nis_refuel_llm = self.config.provider() == ModelProvider.REFUEL\nif is_refuel_llm:\nlabels = (\n\", \".join([f'\\\\\"{i}\\\\\"' for i in labels_list[:-1]])\n+ \" or \"\n+ f'\\\\\"{labels_list[-1]}\\\\\"'\n)\nelse:\nif self.config.label_descriptions():\nlabels = \"\"\nfor label, description in self.config.label_descriptions().items():\nlabels = labels + f\"{label} : {description}\\n\"\nelse:\nlabels = \"\\n\".join(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=labels\n)\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# check if all mapped keys in input are in the example template\ntry:\ncurrent_example = example_template.format(**input)\nexcept KeyError as e:\ncurrent_example = example_template.format_map(defaultdict(str, input))\nlogger.warn(\nf'\\n\\nKey {e} in the \"example_template\" in the given config'\nf\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\nf\"Input - {input}\\n\\n\"\n\"Continuing with the prompt as {current_example}\"\n)\n# populate the current example in the prompt\nprompt_template = (\nself.prompt_template\nif prompt_template_override is None\nelse prompt_template_override\n)\noutput_guidelines = (\nself.output_guidelines\nif output_guidelines_override is None\nelse output_guidelines_override\n)\nif self._is_few_shot_mode():\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nelse:\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nif self.image_col is not None:\nreturn json.dumps(\n{\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n)\nelse:\nreturn curr_text_prompt\ndef get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare labeled example\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=fmt_task_guidelines,\nlabel_format=(\nself.LABEL_FORMAT_IN_EXPLANATION\nif include_label\nelse self.EXCLUDE_LABEL_IN_EXPLANATION\n),\nlabeled_example=fmt_example,\n)\ndef get_generate_dataset_prompt(self, label: str) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_DATASET_TEMPLATE),\ntemplate=self.GENERATE_DATASET_TEMPLATE,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\nfmt_guidelines = self.dataset_generation_guidelines.format(\ntask_guidelines=fmt_task_guidelines\n)\n# prepare columns\ncolumns = get_format_variables(self.config.example_template())\ncolumns.remove(self.config.label_column())\n# prepare label descriptions\nfmt_label_descriptions = self.LABEL_DESCRIPTIONS_PROMPT.format(\nnum_labels=num_labels\n)\nfor i, l in enumerate(labels_list):\nfmt_label_descriptions += f\"\\n{i+1}. {l}{': ' + self.config.label_descriptions()[l] if self.config.label_descriptions() is not None and l in self.config.label_descriptions() else ''}\"\n# prepare format\nexample_rows = \"\\n\".join(\n[\",\".join([f'\"{column}_{i+1}\"' for column in columns]) for i in range(3)]\n)\nfmt_format_guidelines = self.GENERATE_DATASET_FORMAT_GUIDELINES.format(\ncolumns=\",\".join(columns), example_rows=example_rows\n)\n# prepare output guidelines\nfmt_output_guidelines = self.GENERATE_DATASET_OUTPUT_GUIDELINES.format(\nnum_rows=self.config.dataset_generation_num_rows(), label=label\n)\nreturn pt.format(\nguidelines=fmt_guidelines,\ncolumns=columns,\nlabel_descriptions=fmt_label_descriptions,\nformat_guidelines=fmt_format_guidelines,\noutput_guidelines=fmt_output_guidelines,\n)\ndef eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (List[BaseMetric], optional): The additional metrics to run. Defaults to [].\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre> <p>               Bases: <code>BaseTask</code></p> Source code in <code>autolabel/src/autolabel/tasks/entity_matching.py</code> <pre><code>class EntityMatchingTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer with one element: \"the correct option\"\\n'\n)\nDEFAULT_TASK_GUIDELINES = \"Your job is to tell if the two given entities are duplicates or not. You will return the answer from one of the choices. Choices:\\n{labels}\\n\"\nLABEL_FORMAT_IN_EXPLANATION = (\n\" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n)\nEXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\nGENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. Your job is to provide an explanation for why the output is correct for the task above.\\nThink step by step and generate an explanation.{label_format}\\n{labeled_example}\\nExplanation: \"\nGENERATE_DATASET_TEMPLATE = \"{guidelines}\\n\\nThe inputs must be diverse, covering a wide range of scenarios. You will not generate duplicate inputs. These inputs should be organized in rows in csv format with the columns {columns}.\\n\\n{label_descriptions}\\n\\n{format_guidelines}\\n\\n{output_guidelines}\\n\\n```csv\"\nDEFAULT_DATASET_GENERATION_GUIDELINES = \"You are an expert at generating plausible inputs for a given task.\\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\"\nLABEL_DESCRIPTIONS_PROMPT = \"Each input should fall into one of these {num_labels} categories. These are the only categories that the inputs can belong to.\"\nGENERATE_DATASET_FORMAT_GUIDELINES = \"Your response should be in csv format with the following columns: {columns}.\\n\\nHere is a template you can follow for your output:\\n```csv\\n{columns}\\n{example_rows}\\n```\\n\\nMake sure to replace the placeholder variables with your own values.\"\nGENERATE_DATASET_OUTPUT_GUIDELINES = 'Now I want you to generate {num_rows} excerpts that follow the guidelines and all belong to the \"{label}\" category. They should not belong to any of the other categories.'\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\nself.metrics = [\nAccuracyMetric(),\nSupportMetric(),\nCompletionRateMetric(),\nClassificationReportMetric(),\n]\nif self.config.confidence():\nself.metrics.append(AUROCMetric())\nfor label in self.config.labels_list():\nif \"\\n\" in label:\nlogger.warning(\n\"Label contains newline character. This can have output guideline issues.\"\n)\ndef construct_prompt(\nself,\ninput: Dict,\nexamples: List[Dict],\nprompt_template_override: PromptTemplate = None,\nrefuel_prompt_override: bool = False,\noutput_guidelines_override: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n**kwargs,\n) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format_map(\ndefaultdict(str, labels=\"\\n\".join(labels_list), num_labels=num_labels)\n)\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# check if all mapped keys in input are in the example template\ntry:\ncurrent_example = example_template.format(**input)\nexcept KeyError as e:\ncurrent_example = example_template.format_map(defaultdict(str, input))\nlogger.warn(\nf'\\n\\nKey {e} in the \"example_template\" in the given config'\nf\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\nf\"Input - {input}\\n\\n\"\n\"Continuing with the prompt as {current_example}\"\n)\n# populate the current example in the prompt\nprompt_template = (\nself.prompt_template\nif prompt_template_override is None\nelse prompt_template_override\n)\noutput_guidelines = (\nself.output_guidelines\nif output_guidelines_override is None\nelse output_guidelines_override\n)\nif self._is_few_shot_mode():\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nelse:\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nif self.image_col is not None:\nreturn json.dumps(\n{\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n)\nelse:\nreturn curr_text_prompt\ndef get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\n# prepare labeled example\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=fmt_task_guidelines,\nlabel_format=self.LABEL_FORMAT_IN_EXPLANATION\nif include_label\nelse self.EXCLUDE_LABEL_IN_EXPLANATION,\nlabeled_example=fmt_example,\n)\ndef get_generate_dataset_prompt(self, label: str) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_DATASET_TEMPLATE),\ntemplate=self.GENERATE_DATASET_TEMPLATE,\n)\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format(\nnum_labels=num_labels, labels=\"\\n\".join(labels_list)\n)\nfmt_guidelines = self.dataset_generation_guidelines.format(\ntask_guidelines=fmt_task_guidelines\n)\n# prepare columns\ncolumns = get_format_variables(self.config.example_template())\ncolumns.remove(self.config.label_column())\n# prepare label descriptions\nfmt_label_descriptions = self.LABEL_DESCRIPTIONS_PROMPT.format(\nnum_labels=num_labels\n)\nfor i, l in enumerate(labels_list):\nfmt_label_descriptions += f\"\\n{i+1}. {l}{': ' + self.config.label_descriptions()[l] if self.config.label_descriptions() is not None and l in self.config.label_descriptions() else ''}\"\n# prepare format\nexample_rows = \"\\n\".join(\n[\",\".join([f'\"{column}_{i+1}\"' for column in columns]) for i in range(3)]\n)\nfmt_format_guidelines = self.GENERATE_DATASET_FORMAT_GUIDELINES.format(\ncolumns=\",\".join(columns), example_rows=example_rows\n)\n# prepare output guidelines\nfmt_output_guidelines = self.GENERATE_DATASET_OUTPUT_GUIDELINES.format(\nnum_rows=self.config.dataset_generation_num_rows(), label=label\n)\nreturn pt.format(\nguidelines=fmt_guidelines,\ncolumns=columns,\nlabel_descriptions=fmt_label_descriptions,\nformat_guidelines=fmt_format_guidelines,\noutput_guidelines=fmt_output_guidelines,\n)\ndef eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (List[BaseMetric], optional): List of additional metrics to run. Defaults to [].\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre> <p>               Bases: <code>BaseTask</code></p> Source code in <code>autolabel/src/autolabel/tasks/question_answering.py</code> <pre><code>class QuestionAnsweringTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = (\n'You will return the answer one element: \"the correct label\"\\n'\n)\nREFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES = \"\"\nDEFAULT_TASK_GUIDELINES = \"Your job is to answer the following questions using the options provided for each question. Choose the best answer for the question.\\n\"\nNULL_LABEL_TOKEN = \"NO_LABEL\"\nLABEL_FORMAT_IN_EXPLANATION = (\n\" The last line of the explanation should be - So, the answer is &lt;label&gt;.\"\n)\nEXCLUDE_LABEL_IN_EXPLANATION = \" Do not repeat the output of the task - simply provide an explanation for the provided output. The provided label was generated by you in a previous step and your job now is to only provided an explanation for the output. Your job is not verify the output but instead explain why it might have been generated, even if it is incorrect. If you think the provided output is incorrect, give an explanation of why it might have been generated anyway but don't say that the output may be incorrect or incorrectly generated.'\"\nGENERATE_EXPLANATION_PROMPT = \"You are an expert at providing a well reasoned explanation for the output of a given task. \\n\\nBEGIN TASK DESCRIPTION\\n{task_guidelines}\\nEND TASK DESCRIPTION\\nYou will be given an input example and the corresponding output. You will be given a question and an answer. Your job is to provide an explanation for why the answer is correct for the task above.\\nThink step by step and generate an explanation.{label_format}\\n{labeled_example}\\nExplanation: \"\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nif config.provider() == ModelProvider.REFUEL:\nself.DEFAULT_OUTPUT_GUIDELINES = self.REFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES\nsuper().__init__(config)\nself.metrics = [\nAccuracyMetric(),\nSupportMetric(),\nCompletionRateMetric(),\nF1Metric(\ntype=F1Type.TEXT,\n),\n]\nif self.config.confidence():\nself.metrics.append(AUROCMetric())\ndef construct_prompt(\nself,\ninput: Dict,\nexamples: List[Dict],\nprompt_template_override: PromptTemplate = None,\nrefuel_prompt_override: bool = False,\noutput_guidelines_override: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n**kwargs,\n) -&gt; str:\n# Copy over the input so that we can modify it\ninput = input.copy()\n# prepare seed examples\nexample_template = self.config.example_template()\nlabel_column = self.config.label_column()\nfmt_examples = []\nfor eg in examples:\neg_copy = eg.copy()\n# If chain of thought is enabled\nif label_column and self.config.chain_of_thought():\neg_copy[label_column] = json.dumps({\"label\": eg[label_column]})\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# check if all mapped keys in input are in the example template\ntry:\ncurrent_example = example_template.format(**input)\nexcept KeyError as e:\ncurrent_example = example_template.format_map(defaultdict(str, input))\nlogger.warn(\nf'\\n\\nKey {e} in the \"example_template\" in the given config'\nf\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\nf\"Input - {input}\\n\\n\"\n\"Continuing with the prompt as {current_example}\"\n)\n# populate the current example in the prompt\nprompt_template = (\nself.prompt_template\nif prompt_template_override is None\nelse prompt_template_override\n)\noutput_guidelines = (\nself.output_guidelines\nif output_guidelines_override is None\nelse output_guidelines_override\n)\nif self._is_few_shot_mode():\ncurr_text_prompt = prompt_template.format(\ntask_guidelines=self.task_guidelines,\noutput_guidelines=output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\n)\nelse:\ncurr_text_prompt = prompt_template.format(\ntask_guidelines=self.task_guidelines,\noutput_guidelines=output_guidelines,\ncurrent_example=current_example,\n)\nif self.image_col is not None:\nreturn json.dumps(\n{\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n)\nelse:\nreturn curr_text_prompt\ndef construct_confidence_prompt(self, input: str, examples: List, **kwargs) -&gt; str:\noutput_guidelines_override = (\nself.config.output_guidelines() or self.REFUEL_LLM_DEFAULT_OUTPUT_GUIDELINES\n)\nrefuel_prompt = super().construct_confidence_prompt(\ninput,\nexamples,\noutput_guidelines_override=output_guidelines_override,\n**kwargs,\n)\nreturn refuel_prompt\ndef get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\npt = PromptTemplate(\ninput_variables=get_format_variables(self.GENERATE_EXPLANATION_PROMPT),\ntemplate=self.GENERATE_EXPLANATION_PROMPT,\n)\nexample_template = self.config.example_template()\nfmt_example = example_template.format_map(defaultdict(str, example))\nreturn pt.format(\ntask_guidelines=self.task_guidelines,\nlabel_format=self.LABEL_FORMAT_IN_EXPLANATION\nif include_label\nelse self.EXCLUDE_LABEL_IN_EXPLANATION,\nlabeled_example=fmt_example,\n)\ndef get_generate_dataset_prompt(\nself, label: str, num_rows: int, guidelines: str = None\n) -&gt; str:\nraise NotImplementedError(\"Dataset generation not implemented for this task\")\ndef eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n            additional_metrics (Optional[List[BaseMetric]], optional): _description_. Defaults to [].\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre> <p>               Bases: <code>BaseTask</code></p> Source code in <code>autolabel/src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>class NamedEntityRecognitionTask(BaseTask):\nDEFAULT_OUTPUT_GUIDELINES = \"You will return the answer in CSV format, with two columns seperated by the % character. First column is the extracted entity and second column is the category. Rows in the CSV are separated by new line character.\"\nDEFAULT_TASK_GUIDELINES = \"Your job is to extract named entities mentioned in text, and classify them into one of the following {num_labels} categories.\\nCategories:\\n{labels}\\n \"\nNULL_LABEL = {}\ndef __init__(self, config: AutolabelConfig) -&gt; None:\nsuper().__init__(config)\ndef _json_to_llm_format(self, input_label: str) -&gt; str:\n# `label` format: {\"entity type\": [list of entities of this type]}\ntry:\nlabels = json.loads(input_label)\nrows = []\nfor entity_type, detected_entites in labels.items():\nfor e in detected_entites:\nrow = \"%\".join([e, entity_type])\nrows.append(row)\nllm_formatted_label = \"\\n\".join(rows)\nreturn llm_formatted_label\nexcept json.JSONDecodeError as e:\nlogger.error(\nf\"Could not parse label: {input_label}. Few-shot examples might be formatted incorrectly\"\n)\nreturn input_label\ndef _llm_to_json_format(self, response: str):\nsplit_response = response.split(\"\\n\")\njson_output = {i: [] for i in self.config.labels_list()}\nfor row in split_response:\nparts = row.split(\"%\")\nif len(parts) != 2 or parts[1] not in json_output.keys():\nlogger.debug(f\"Malformed LLM response: {row}\")\ncontinue\nnamed_entity = parts[0]\ncategory = parts[1]\njson_output[category].append(named_entity)\nreturn json_output\ndef construct_prompt(\nself,\ninput: Dict,\nexamples: List,\nprompt_template_override: PromptTemplate = None,\nrefuel_prompt_override: bool = False,\noutput_guidelines_override: str = None,\nmax_input_tokens: int = None,\nget_num_tokens: Optional[Callable] = None,\n**kwargs,\n) -&gt; str:\n# prepare task guideline\nlabels_list = self.config.labels_list()\nnum_labels = len(labels_list)\nfmt_task_guidelines = self.task_guidelines.format_map(\ndefaultdict(str, labels=\"\\n\".join(labels_list), num_labels=num_labels)\n)\n# prepare seed examples\nlabel_column = self.config.label_column()\nexample_template = self.config.example_template()\nfmt_examples = []\nfor eg in examples:\neg_copy = deepcopy(eg)\nif label_column:\neg_copy[label_column] = self._json_to_llm_format(eg_copy[label_column])\nfmt_examples.append(example_template.format_map(defaultdict(str, eg_copy)))\n# populate the current example in the prompt\nif label_column:\ninput[label_column] = \"\"\n# populate the explanation column with empty string for current example\nexplanation_column = self.config.explanation_column()\nif explanation_column:\ninput[explanation_column] = \"\"\n# check if all mapped keys in input are in the example template\ntry:\ncurrent_example = example_template.format(**input)\nexcept KeyError as e:\ncurrent_example = example_template.format_map(defaultdict(str, input))\nlogger.warn(\nf'\\n\\nKey {e} in the \"example_template\" in the given config'\nf\"\\n\\n{example_template}\\n\\nis not present in the datsaset columns - {input.keys()}.\\n\\n\"\nf\"Input - {input}\\n\\n\"\n\"Continuing with the prompt as {current_example}\"\n)\n# populate the current example in the prompt\nprompt_template = (\nself.prompt_template\nif prompt_template_override is None\nelse prompt_template_override\n)\noutput_guidelines = (\nself.output_guidelines\nif output_guidelines_override is None\nelse output_guidelines_override\n)\nif self._is_few_shot_mode():\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\nseed_examples=\"\\n\\n\".join(fmt_examples),\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nelse:\ncurr_text_prompt = self.trim_prompt(\nprompt_template,\ntask_guidelines=fmt_task_guidelines,\noutput_guidelines=output_guidelines,\ncurrent_example=current_example,\nmax_input_tokens=max_input_tokens,\nget_num_tokens=get_num_tokens,\n)\nif self.image_col is not None:\nreturn json.dumps(\n{\"text\": curr_text_prompt, \"image_url\": input[self.image_col]}\n)\nelse:\nreturn curr_text_prompt\ndef get_explanation_prompt(self, example: Dict, include_label=True) -&gt; str:\nraise NotImplementedError(\n\"Explanation generation not implemented for this task\"\n)\ndef get_generate_dataset_prompt(\nself, label: str, num_rows: int, guidelines: str = None\n) -&gt; str:\nraise NotImplementedError(\"Dataset generation not implemented for this task\")\ndef add_text_spans(self, raw_output: dict, input: str) -&gt; list:\nprocessed_output = []\nfor entity_type in raw_output:\nfor curr_entity in raw_output[entity_type]:\nprocessed_output.append({\"type\": entity_type, \"text\": curr_entity})\n# create a frequency dict of each named entity in the input to determine text spans for repeated entities\nfrequency_count = {label[\"text\"]: 0 for label in processed_output}\nfor label in processed_output:\ntext = label[\"text\"]\nmatches = [i.start() for i in re.finditer(text, input)]\ncount = frequency_count[text]\n# if count of the named entity is greater than the number of matches, default to last found match\nif count &gt;= len(matches):\ncount = -1\n# if no occurence of named entity in input, default text span to start: -1, end: -1\nif len(matches) == 0:\nlabel[\"start\"] = -1\nlabel[\"end\"] = -1\nelse:\nlabel[\"start\"] = matches[count]\nlabel[\"end\"] = matches[count] + len(text)\nfrequency_count[text] += 1\nreturn processed_output\ndef parse_llm_response(\nself,\nresponse: Union[Generation, ChatGeneration],\ncurr_sample: Dict,\nprompt: str,\n) -&gt; LLMAnnotation:\noutput = {}\nsuccessfully_labeled = False\nerror = None\ntext_column = self.config.text_column()\ninput_str = curr_sample[text_column]\ntry:\ncompletion_text = response.text\noutput = self._llm_to_json_format(completion_text.strip())\nllm_label = self.add_text_spans(output, input_str)\nexcept Exception as e:\nlogger.error(f\"Error parsing LLM response: {response.text}, Error: {e}\")\nllm_label = self.NULL_LABEL\nerror = LabelingError(error_type=ErrorType.PARSING_ERROR, error_msg=str(e))\nsuccessfully_labeled = False if llm_label == self.NULL_LABEL else True\n# TODO: parse generation info correctly to fetch &amp; transform logprobs -&gt; score\nreturn LLMAnnotation(\ncurr_sample=input_str,\nsuccessfully_labeled=successfully_labeled,\nlabel=llm_label,\ngeneration_info=response.generation_info,\nraw_response=response.text,\nprompt=prompt,\nerror=error,\n)\ndef auroc_score_labels(\nself, gt_labels, llm_labels_with_conf\n) -&gt; Tuple[List[int], List[float]]:\nlabels = []\nconfidences = []\nfor index, pred_entities in enumerate(llm_labels_with_conf):\ngt_entities = gt_labels[index]\npred_conf = pred_entities[0][\"conf\"] if len(pred_entities) &gt; 0 else 0\nfor gt_entity in gt_entities:\nmatch_found = False\npred_index = 0\nwhile not match_found and pred_index &lt; len(pred_entities):\ncurr_match = True\nfor key in gt_entity:\nif gt_entity[key] != pred_entities[pred_index][key]:\ncurr_match = False\nif curr_match:\nmatch_found = True\npred_index += 1\nlabels.append(int(match_found))\nconfidences.append(pred_conf)\nreturn labels, confidences\ndef get_labels_predictions_with_threshold(self, gt_labels, llm_labels, threshold):\nanswered_gt_labels, answered_llm_preds = [], []\nfor index, l in enumerate(llm_labels):\nif l.successfully_labeled and (\nl.confidence_score is None or l.confidence_score &gt;= threshold\n):\nanswered_gt_labels.append(\n[{**entity, \"label\": entity[\"type\"]} for entity in gt_labels[index]]\n)\nanswered_llm_preds.append(\n[\n{\n**entity,\n\"label\": entity[\"type\"],\n\"conf\": l.confidence_score,\n}\nfor entity in l.label\n],\n)\nreturn answered_gt_labels, answered_llm_preds\ndef run_metrics(\nself,\nanswered_gt_labels,\nanswered_llm_preds,\nentity_types_set,\n) -&gt; List[MetricResult]:\neval_metrics = []\nevaluator = Evaluator(\nanswered_gt_labels, answered_llm_preds, tags=entity_types_set\n)\nresults, _ = evaluator.evaluate()\n# f1 score for exact match\neval_metrics.append(\nMetricResult(\nname=MetricType.F1_EXACT,\nvalue=results[\"exact\"][\"f1\"],\n)\n)\n# f1 score for strict match\neval_metrics.append(\nMetricResult(\nname=MetricType.F1_STRICT,\nvalue=results[\"strict\"][\"f1\"],\n)\n)\n# f1 score for partial match\neval_metrics.append(\nMetricResult(\nname=MetricType.F1_PARTIAL,\nvalue=results[\"partial\"][\"f1\"],\n)\n)\n# f1 score for entity type match\neval_metrics.append(\nMetricResult(\nname=MetricType.F1_ENT_TYPE,\nvalue=results[\"ent_type\"][\"f1\"],\n)\n)\n# accuracy\naccuracy = (\nresults.get(\"strict\").get(\"correct\")\n/ (results.get(\"strict\").get(\"possible\"))\nif results.get(\"strict\").get(\"possible\") &gt; 0\nelse 0.0\n)\neval_metrics.append(\nMetricResult(\nname=MetricType.ACCURACY,\nvalue=accuracy,\n)\n)\nif self.config.confidence():\nmatch, confidences = self.auroc_score_labels(\nanswered_gt_labels, answered_llm_preds\n)\nauroc = roc_auc_score(match, confidences)\neval_metrics.append(\nMetricResult(\nname=MetricType.AUROC,\nvalue=auroc,\n)\n)\nreturn eval_metrics\ndef eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n        Args:\n            llm_labels (List[LLMAnnotation]): _description_\n            gt_labels (List[str]): _description_\n        Returns:\n            List[MetricResult]: list of metrics and corresponding values\n        \"\"\"\ngt_labels = [\nself.add_text_spans(\njson.loads(gt_labels[index]), llm_labels[index].curr_sample.decode()\n)\nfor index in range(len(gt_labels))\n]\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, float(\"-inf\")\n)\nentity_types_set = list(\nset(\n[\ngt_entity.get(\"label\")\nfor gt_label in curr_gt_labels\nfor gt_entity in gt_label\n]\n)\n)\neval_metrics = []\neval_metrics.append(\nMetricResult(\nname=MetricType.SUPPORT,\nvalue=len(gt_labels),\n)\n)\neval_metrics.append(\nMetricResult(\nname=MetricType.COMPLETION_RATE,\nvalue=(\nlen(curr_llm_labels) / float(len(gt_labels))\nif len(gt_labels) &gt; 0\nelse 0.0\n),\n)\n)\ncurr_threshold_metrics = self.run_metrics(\ncurr_gt_labels,\ncurr_llm_labels,\nentity_types_set,\n)\neval_metrics.extend(curr_threshold_metrics)\nreturn eval_metrics\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.classification.ClassificationTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>List[BaseMetric]</code> <p>The additional metrics to run. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>autolabel/src/autolabel/tasks/classification.py</code> <pre><code>def eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (List[BaseMetric], optional): The additional metrics to run. Defaults to [].\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.entity_matching.EntityMatchingTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>List[BaseMetric]</code> <p>List of additional metrics to run. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>autolabel/src/autolabel/tasks/entity_matching.py</code> <pre><code>def eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: List[BaseMetric] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (List[BaseMetric], optional): List of additional metrics to run. Defaults to [].\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.question_answering.QuestionAnsweringTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <code>additional_metrics</code> <code>Optional[List[BaseMetric]]</code> <p>description. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>autolabel/src/autolabel/tasks/question_answering.py</code> <pre><code>def eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n        additional_metrics (Optional[List[BaseMetric]], optional): _description_. Defaults to [].\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\neval_metrics = []\nfor metric in self.metrics + additional_metrics:\neval_metrics.extend(metric.compute(llm_labels, gt_labels))\nreturn eval_metrics\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.named_entity_recognition.NamedEntityRecognitionTask.eval","title":"<code>eval(llm_labels, gt_labels, additional_metrics=[])</code>","text":"<p>Evaluate the LLM generated labels by comparing them against ground truth</p> <p>Parameters:</p> Name Type Description Default <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>description</p> required <code>gt_labels</code> <code>List[str]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List[MetricResult]: list of metrics and corresponding values</p> Source code in <code>autolabel/src/autolabel/tasks/named_entity_recognition.py</code> <pre><code>def eval(\nself,\nllm_labels: List[LLMAnnotation],\ngt_labels: List[str],\nadditional_metrics: Optional[List[BaseMetric]] = [],\n) -&gt; List[MetricResult]:\n\"\"\"Evaluate the LLM generated labels by comparing them against ground truth\n    Args:\n        llm_labels (List[LLMAnnotation]): _description_\n        gt_labels (List[str]): _description_\n    Returns:\n        List[MetricResult]: list of metrics and corresponding values\n    \"\"\"\ngt_labels = [\nself.add_text_spans(\njson.loads(gt_labels[index]), llm_labels[index].curr_sample.decode()\n)\nfor index in range(len(gt_labels))\n]\n(\ncurr_gt_labels,\ncurr_llm_labels,\n) = self.get_labels_predictions_with_threshold(\ngt_labels, llm_labels, float(\"-inf\")\n)\nentity_types_set = list(\nset(\n[\ngt_entity.get(\"label\")\nfor gt_label in curr_gt_labels\nfor gt_entity in gt_label\n]\n)\n)\neval_metrics = []\neval_metrics.append(\nMetricResult(\nname=MetricType.SUPPORT,\nvalue=len(gt_labels),\n)\n)\neval_metrics.append(\nMetricResult(\nname=MetricType.COMPLETION_RATE,\nvalue=(\nlen(curr_llm_labels) / float(len(gt_labels))\nif len(gt_labels) &gt; 0\nelse 0.0\n),\n)\n)\ncurr_threshold_metrics = self.run_metrics(\ncurr_gt_labels,\ncurr_llm_labels,\nentity_types_set,\n)\neval_metrics.extend(curr_threshold_metrics)\nreturn eval_metrics\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.utils.filter_unlabeled_examples","title":"<code>filter_unlabeled_examples(gt_labels, llm_labels)</code>","text":"<p>Filter out unlabeled examples from the ground truth and LLM generated labels. This is done by checking the ground truth labels which have nan values. The corresponding ground truth and LLM labels are removed from the filtered labels lists.</p> <p>Parameters:</p> Name Type Description Default <code>gt_labels</code> <code>List[str]</code> <p>ground truth labels</p> required <code>llm_labels</code> <code>List[LLMAnnotation]</code> <p>llm labels</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[LLMAnnotation]]</code> <p>filtered_gt_labels, filtered_llm_labels: filtered ground truth and LLM generated labels</p> Source code in <code>autolabel/src/autolabel/tasks/utils.py</code> <pre><code>def filter_unlabeled_examples(\ngt_labels: List[str], llm_labels: List[LLMAnnotation]\n) -&gt; Tuple[List[str], List[LLMAnnotation]]:\n\"\"\"Filter out unlabeled examples from the ground truth and LLM generated labels.\n    This is done by checking the ground truth labels which have nan values.\n    The corresponding ground truth and LLM labels are removed from the filtered labels lists.\n    Args:\n        gt_labels (List[str]): ground truth labels\n        llm_labels (List[LLMAnnotation]): llm labels\n    Returns:\n        filtered_gt_labels, filtered_llm_labels: filtered ground truth and LLM generated labels\n    \"\"\"\nfiltered_gt_labels = []\nfiltered_llm_labels = []\nfor gt_label, llm_label in zip(gt_labels, llm_labels):\nif gt_label != \"nan\":\nfiltered_gt_labels.append(gt_label)\nfiltered_llm_labels.append(llm_label)\nreturn filtered_gt_labels, filtered_llm_labels\n</code></pre>"},{"location":"autolabel/reference/tasks/#src.autolabel.tasks.utils.normalize_text","title":"<code>normalize_text(s)</code>","text":"<p>Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.</p> Source code in <code>autolabel/src/autolabel/tasks/utils.py</code> <pre><code>def normalize_text(s: str) -&gt; str:\n\"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\ndef remove_articles(text):\nregex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\nreturn re.sub(regex, \" \", text)\ndef white_space_fix(text):\nreturn \" \".join(text.split())\ndef remove_punc(text):\nexclude = set(string.punctuation)\nreturn \"\".join(ch for ch in text if ch not in exclude)\ndef lower(text):\nreturn text.lower()\nreturn white_space_fix(remove_articles(remove_punc(lower(s))))\n</code></pre>"},{"location":"integrations/aws/","title":"AWS Integration with Refuel","text":"<p>This document describes how you can set up an AWS integration with Refuel. This integration allows Refuel to read your data directly from S3 in order to:</p> <ul> <li>Display it in the Refuel web application</li> <li>Perform some necessary processing, including extracting metadata, performing OCR, etc.</li> </ul> <p>In the first case, a pre-signed URL is created for the data that expires after a short time, and in the second case, the data is promptly deleted from the Refuel backend after processing.</p>"},{"location":"integrations/aws/#setup","title":"Setup","text":""},{"location":"integrations/aws/#refuel-aws-account-id-and-external-id","title":"Refuel AWS Account ID and External ID","text":"<p>Email support@refuel.ai to receive the Refuel AWS Account ID and external ID that you can use in the later steps.</p> <p>This will be made available in the UI at a later point.</p>"},{"location":"integrations/aws/#configure-cors","title":"Configure CORS","text":"<p>CORS (Cross-origin resource sharing) is a browser mechanism that enables restricted access across domain boundaries that would otherwise be prohibited by default browser restrictions. The CORS header enables Refuel to send a pre-flight request to your cloud storage and enables your cloud storage to explicitly allow requests from Refuel. If the Refuel domains are included in the CORS header, Refuel will be able to request resources from your cloud storage.</p> <p>When configuring CORS for your cloud storage bucket, you will need to include the following Refuel origin: https://app.refuel.ai.</p> <ul> <li>In your AWS account, go to your S3 Management Console</li> <li>Click the bucket name in the list of buckets.</li> <li>Go to the Permissions tab.</li> <li>In the Cross-origin resource sharing (CORS) section, click Edit.</li> <li>Paste the following configuration in the text field.</li> </ul> <pre><code>[\n{\n\"AllowedHeaders\": [\"*\"],\n\"AllowedMethods\": [\"GET\"],\n\"AllowedOrigins\": [\"https://app.refuel.ai\"],\n\"ExposeHeaders\": []\n}\n]\n</code></pre> <ul> <li>Click Save changes.</li> <li>For more details on setting up CORS for your AWS S3 bucket, see these AWS docs.</li> </ul>"},{"location":"integrations/aws/#create-iam-policy-for-s3-access","title":"Create IAM Policy for S3 Access","text":"<ul> <li>Navigate to the IAM Console in AWS, click on Policies in the left navbar, and then hit the Create Policy button.</li> <li>Click on the JSON tab, copy the policy below (substitute your bucket name, add multiple buckets, restrict to specific keys in a bucket) and then hit Next.</li> </ul> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n\"Resource\": [\n\"arn:aws:s3:::YourBucketName/*\",\n\"arn:aws:s3:::YourBucketName\"\n]\n}\n]\n}\n</code></pre> <ul> <li>Click Next again, give the policy a name, such as RefuelS3ReadAccessPolicy and then hit Create policy.</li> </ul>"},{"location":"integrations/aws/#create-iam-role-for-s3-access","title":"Create IAM Role for S3 Access","text":"<ul> <li>In the IAM Console, click on Roles in the left navbar and click on Create role</li> <li>In Trusted entity type, click on AWS account and then click on Another AWS account.</li> <li>Paste in the Refuel AWS account ID from step 1.</li> <li>Click Require external ID and again paste in the external ID from step 1, and click Next.</li> <li>Search for, and add the policy you just created: RefuelS3ReadAcccessPolicy and hit Next.</li> <li>Give this role a name, let\u2019s say RefuelS3ReadAccessRole and click Create role.</li> </ul>"},{"location":"integrations/aws/#send-refuel-the-arn-of-the-newly-created-role","title":"Send Refuel the ARN of the newly created role","text":"<p>Email support@refuel.ai with the role ARN.</p> <p>At a later point, this will just be done in the Refuel web application.</p> <p>That\u2019s it \u2013 now start sending Refuel your data!</p> <p>When you send Refuel any pieces of content that sit in S3, all you need to send is the path in S3 (s3://YourBucketName/path/to/key), and Refuel will be able to securely access it.</p>"},{"location":"integrations/introduction/","title":"Cloud Integration with Refuel","text":"<p>Welcome to the guide on setting up a cloud integration with Refuel to securely access and utilize data in your cloud storage. This integration ensures a robust and secure data flow between your cloud storage and Refuel's App.</p>"},{"location":"integrations/introduction/#why-security-matters","title":"Why Security Matters","text":"<p>Ensuring the security of your data is paramount, especially when integrating with external services. To ensure a secure integration with Refuel, we follow these best practices:</p>"},{"location":"integrations/introduction/#principle-of-least-privilege","title":"Principle of Least Privilege:","text":"<p>Requiring the minimum level of access for our app to perform its functions. Avoiding excessive permissions that could compromise the security of your data.</p>"},{"location":"integrations/introduction/#secure-access-credentials","title":"Secure Access Credentials:","text":"<p>Protecting access credentials for both your cloud storage and Refuel. Safeguarding API keys, tokens, and other sensitive information to prevent unauthorized access.</p>"},{"location":"integrations/introduction/#encrypted-communication","title":"Encrypted Communication:","text":"<p>Implement encrypted communication (HTTPS) for all data transfers between your cloud storage and Refuel services. This ensures that data is transmitted securely over the network.</p>"},{"location":"integrations/introduction/#regular-audits-and-monitoring","title":"Regular Audits and Monitoring:","text":"<p>Periodically auditing access permissions and reviewing security logs. We have implemented continuous monitoring to detect and respond to any unusual activities.</p>"}]}